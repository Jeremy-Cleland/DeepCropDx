2025-03-02 20:34:29,642 - INFO - Starting experiment: efficientnet_b0
2025-03-02 20:34:29,642 - INFO - Command line arguments: Namespace(data_dir='data/raw', output_dir='models/efficientnet_b0_v1', model='efficientnet', img_size=224, batch_size=32, num_workers=16, epochs=30, lr=0.001, weight_decay=0.0001, pretrained=True, freeze_backbone=True, no_cuda=False, no_mps=False, use_mps=True, use_amp=False, memory_efficient=True, cache_dataset=True, mps_graph=True, mps_fallback=True, pin_memory=True, optimize_for_m_series=True, patience=30, keep_top_k=3, version=None, find_lr=False, experiment_name='efficientnet_b0', resnet_version=50)
2025-03-02 20:34:29,642 - INFO - Processing dataset...
2025-03-02 20:34:29,716 - INFO - Class distribution:
2025-03-02 20:34:29,716 - INFO -   Tomato_healthy: 1591 images
2025-03-02 20:34:29,716 - INFO -   Potato___Early_blight: 1000 images
2025-03-02 20:34:29,716 - INFO -   Tomato__Tomato_YellowLeaf__Curl_Virus: 3208 images
2025-03-02 20:34:29,716 - INFO -   Tomato_Early_blight: 1000 images
2025-03-02 20:34:29,716 - INFO -   Tomato__Target_Spot: 1404 images
2025-03-02 20:34:29,716 - INFO -   Potato___Late_blight: 1000 images
2025-03-02 20:34:29,716 - INFO -   Tomato_Leaf_Mold: 952 images
2025-03-02 20:34:29,716 - INFO -   Tomato_Spider_mites_Two_spotted_spider_mite: 1676 images
2025-03-02 20:34:29,716 - INFO -   Tomato_Septoria_leaf_spot: 1771 images
2025-03-02 20:34:29,716 - INFO -   Tomato__Tomato_mosaic_virus: 373 images
2025-03-02 20:34:29,716 - INFO -   Pepper__bell___Bacterial_spot: 997 images
2025-03-02 20:34:29,716 - INFO -   Tomato_Bacterial_spot: 2127 images
2025-03-02 20:34:29,716 - INFO -   Tomato_Late_blight: 1909 images
2025-03-02 20:34:29,716 - INFO -   Pepper__bell___healthy: 1478 images
2025-03-02 20:34:29,716 - INFO -   Potato___healthy: 152 images
2025-03-02 20:34:29,716 - INFO - Creating model: efficientnet with 15 classes
2025-03-02 20:34:29,838 - INFO - Model architecture:
EfficientNet(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
    )
    (2): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.025, mode=row)
      )
    )
    (3): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.125, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)
      )
    )
    (7): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)
      )
    )
    (8): Conv2dNormActivation(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=15, bias=True)
  )
)
2025-03-02 20:34:29,839 - INFO - Using class weights: [0.5015516  0.7979687  0.24874336 0.7979687  0.5683538  0.7979687
 0.8382024  0.47611496 0.4505752  2.1393263  0.8003698  0.3751616
 0.4180035  0.5398976  5.249794  ]
2025-03-02 20:34:29,839 - INFO - Training only 2 parameters (classifier)
2025-03-02 20:34:29,840 - INFO - Starting training for 30 epochs
2025-03-02 20:34:29,840 - INFO - Using Automatic Mixed Precision: False
2025-03-02 20:34:29,840 - INFO - Early stopping patience: 30
2025-03-02 20:34:29,840 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:34:29,840 - INFO - Starting training: efficientnet_b0
2025-03-02 20:34:29,840 - INFO - Total epochs: 30
2025-03-02 20:34:29,840 - INFO - Training batches per epoch: 452
2025-03-02 20:34:29,840 - INFO - Validation batches per epoch: 97
2025-03-02 20:34:29,840 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:34:29,840 - INFO - Training model: efficientnet_b0_v1
2025-03-02 20:34:29,840 - INFO - Epoch 1/30
2025-03-02 20:34:29,840 - INFO - ----------------------------------------
2025-03-02 20:34:58,732 - INFO - [TRAIN] Epoch: 1/30 | Batch: 0/452 (0.2%) | Loss: 2.7513 | Batch time: 0.45s
2025-03-02 20:34:59,749 - INFO - [TRAIN] Epoch: 1/30 | Batch: 45/452 (10.2%) | Loss: 2.4590 | Batch time: 0.02s
2025-03-02 20:35:00,768 - INFO - [TRAIN] Epoch: 1/30 | Batch: 90/452 (20.1%) | Loss: 1.6642 | Batch time: 0.02s
2025-03-02 20:35:01,790 - INFO - [TRAIN] Epoch: 1/30 | Batch: 135/452 (30.1%) | Loss: 1.4860 | Batch time: 0.02s
2025-03-02 20:35:02,819 - INFO - [TRAIN] Epoch: 1/30 | Batch: 180/452 (40.0%) | Loss: 1.3419 | Batch time: 0.02s
2025-03-02 20:35:03,837 - INFO - [TRAIN] Epoch: 1/30 | Batch: 225/452 (50.0%) | Loss: 1.7594 | Batch time: 0.02s
2025-03-02 20:35:04,857 - INFO - [TRAIN] Epoch: 1/30 | Batch: 270/452 (60.0%) | Loss: 1.2329 | Batch time: 0.02s
2025-03-02 20:35:05,876 - INFO - [TRAIN] Epoch: 1/30 | Batch: 315/452 (69.9%) | Loss: 1.0122 | Batch time: 0.02s
2025-03-02 20:35:06,903 - INFO - [TRAIN] Epoch: 1/30 | Batch: 360/452 (79.9%) | Loss: 1.1421 | Batch time: 0.02s
2025-03-02 20:35:07,933 - INFO - [TRAIN] Epoch: 1/30 | Batch: 405/452 (89.8%) | Loss: 1.5191 | Batch time: 0.02s
2025-03-02 20:35:08,936 - INFO - [TRAIN] Epoch: 1/30 | Batch: 450/452 (99.8%) | Loss: 1.1972 | Batch time: 0.02s
2025-03-02 20:35:09,221 - INFO - [TRAIN] Epoch: 1/30 | Batch: 451/452 (100.0%) | Loss: 2.0485 | Batch time: 0.29s
2025-03-02 20:35:38,116 - INFO - [VAL] Epoch: 1/30 | Batch: 0/97 (1.0%) | Loss: 0.7367 | Batch time: 0.13s
2025-03-02 20:35:38,272 - INFO - [VAL] Epoch: 1/30 | Batch: 9/97 (10.3%) | Loss: 0.8661 | Batch time: 0.02s
2025-03-02 20:35:38,428 - INFO - [VAL] Epoch: 1/30 | Batch: 18/97 (19.6%) | Loss: 0.8223 | Batch time: 0.02s
2025-03-02 20:35:38,585 - INFO - [VAL] Epoch: 1/30 | Batch: 27/97 (28.9%) | Loss: 0.6167 | Batch time: 0.02s
2025-03-02 20:35:38,742 - INFO - [VAL] Epoch: 1/30 | Batch: 36/97 (38.1%) | Loss: 0.6582 | Batch time: 0.02s
2025-03-02 20:35:38,900 - INFO - [VAL] Epoch: 1/30 | Batch: 45/97 (47.4%) | Loss: 0.7775 | Batch time: 0.02s
2025-03-02 20:35:39,057 - INFO - [VAL] Epoch: 1/30 | Batch: 54/97 (56.7%) | Loss: 1.1048 | Batch time: 0.02s
2025-03-02 20:35:39,217 - INFO - [VAL] Epoch: 1/30 | Batch: 63/97 (66.0%) | Loss: 0.6785 | Batch time: 0.02s
2025-03-02 20:35:39,373 - INFO - [VAL] Epoch: 1/30 | Batch: 72/97 (75.3%) | Loss: 0.8351 | Batch time: 0.02s
2025-03-02 20:35:39,529 - INFO - [VAL] Epoch: 1/30 | Batch: 81/97 (84.5%) | Loss: 0.7634 | Batch time: 0.02s
2025-03-02 20:35:39,685 - INFO - [VAL] Epoch: 1/30 | Batch: 90/97 (93.8%) | Loss: 0.7967 | Batch time: 0.02s
2025-03-02 20:35:39,996 - INFO - [VAL] Epoch: 1/30 | Batch: 96/97 (100.0%) | Loss: 0.8503 | Batch time: 0.22s
2025-03-02 20:35:40,151 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 1)
2025-03-02 20:35:40,151 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:35:40,151 - INFO - Epoch 1/30 completed in 70.31s
2025-03-02 20:35:40,151 - INFO - Training   - Loss: 1.4892, Accuracy: 0.5971, F1: 0.5978
2025-03-02 20:35:40,151 - INFO - Validation - Loss: 0.8333, Accuracy: 0.7532, F1: 0.7575
2025-03-02 20:35:40,151 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:35:40,151 - INFO - Epoch 2/30
2025-03-02 20:35:40,151 - INFO - ----------------------------------------
2025-03-02 20:35:40,613 - INFO - [TRAIN] Epoch: 2/30 | Batch: 0/452 (0.2%) | Loss: 0.9725 | Batch time: 0.26s
2025-03-02 20:35:41,655 - INFO - [TRAIN] Epoch: 2/30 | Batch: 45/452 (10.2%) | Loss: 0.8615 | Batch time: 0.02s
2025-03-02 20:35:42,707 - INFO - [TRAIN] Epoch: 2/30 | Batch: 90/452 (20.1%) | Loss: 1.1987 | Batch time: 0.02s
2025-03-02 20:35:43,731 - INFO - [TRAIN] Epoch: 2/30 | Batch: 135/452 (30.1%) | Loss: 0.7665 | Batch time: 0.02s
2025-03-02 20:35:44,755 - INFO - [TRAIN] Epoch: 2/30 | Batch: 180/452 (40.0%) | Loss: 1.0363 | Batch time: 0.02s
2025-03-02 20:35:45,773 - INFO - [TRAIN] Epoch: 2/30 | Batch: 225/452 (50.0%) | Loss: 1.0197 | Batch time: 0.02s
2025-03-02 20:35:46,877 - INFO - [TRAIN] Epoch: 2/30 | Batch: 270/452 (60.0%) | Loss: 1.0632 | Batch time: 0.02s
2025-03-02 20:35:47,936 - INFO - [TRAIN] Epoch: 2/30 | Batch: 315/452 (69.9%) | Loss: 0.8208 | Batch time: 0.03s
2025-03-02 20:35:49,059 - INFO - [TRAIN] Epoch: 2/30 | Batch: 360/452 (79.9%) | Loss: 1.0130 | Batch time: 0.02s
2025-03-02 20:35:50,097 - INFO - [TRAIN] Epoch: 2/30 | Batch: 405/452 (89.8%) | Loss: 0.7095 | Batch time: 0.02s
2025-03-02 20:35:51,110 - INFO - [TRAIN] Epoch: 2/30 | Batch: 450/452 (99.8%) | Loss: 0.9772 | Batch time: 0.02s
2025-03-02 20:35:51,130 - INFO - [TRAIN] Epoch: 2/30 | Batch: 451/452 (100.0%) | Loss: 1.0155 | Batch time: 0.02s
2025-03-02 20:35:51,218 - INFO - [VAL] Epoch: 2/30 | Batch: 0/97 (1.0%) | Loss: 0.6310 | Batch time: 0.02s
2025-03-02 20:35:51,383 - INFO - [VAL] Epoch: 2/30 | Batch: 9/97 (10.3%) | Loss: 0.7266 | Batch time: 0.02s
2025-03-02 20:35:51,541 - INFO - [VAL] Epoch: 2/30 | Batch: 18/97 (19.6%) | Loss: 0.7699 | Batch time: 0.02s
2025-03-02 20:35:51,698 - INFO - [VAL] Epoch: 2/30 | Batch: 27/97 (28.9%) | Loss: 0.3814 | Batch time: 0.02s
2025-03-02 20:35:51,855 - INFO - [VAL] Epoch: 2/30 | Batch: 36/97 (38.1%) | Loss: 0.4618 | Batch time: 0.02s
2025-03-02 20:35:52,020 - INFO - [VAL] Epoch: 2/30 | Batch: 45/97 (47.4%) | Loss: 0.5303 | Batch time: 0.02s
2025-03-02 20:35:52,178 - INFO - [VAL] Epoch: 2/30 | Batch: 54/97 (56.7%) | Loss: 1.0408 | Batch time: 0.02s
2025-03-02 20:35:52,336 - INFO - [VAL] Epoch: 2/30 | Batch: 63/97 (66.0%) | Loss: 0.4700 | Batch time: 0.02s
2025-03-02 20:35:52,493 - INFO - [VAL] Epoch: 2/30 | Batch: 72/97 (75.3%) | Loss: 0.6870 | Batch time: 0.02s
2025-03-02 20:35:52,648 - INFO - [VAL] Epoch: 2/30 | Batch: 81/97 (84.5%) | Loss: 0.6601 | Batch time: 0.02s
2025-03-02 20:35:52,804 - INFO - [VAL] Epoch: 2/30 | Batch: 90/97 (93.8%) | Loss: 0.5651 | Batch time: 0.02s
2025-03-02 20:35:52,903 - INFO - [VAL] Epoch: 2/30 | Batch: 96/97 (100.0%) | Loss: 0.8393 | Batch time: 0.01s
2025-03-02 20:35:53,024 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 2)
2025-03-02 20:35:53,024 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:35:53,024 - INFO - Epoch 2/30 completed in 12.87s
2025-03-02 20:35:53,024 - INFO - Training   - Loss: 1.0555, Accuracy: 0.6823, F1: 0.6846
2025-03-02 20:35:53,024 - INFO - Validation - Loss: 0.6859, Accuracy: 0.7852, F1: 0.7894
2025-03-02 20:35:53,024 - INFO - Validation F1 improved from 0.7575 to 0.7894
2025-03-02 20:35:53,025 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:35:53,025 - INFO - Epoch 3/30
2025-03-02 20:35:53,025 - INFO - ----------------------------------------
2025-03-02 20:35:53,253 - INFO - [TRAIN] Epoch: 3/30 | Batch: 0/452 (0.2%) | Loss: 1.0033 | Batch time: 0.05s
2025-03-02 20:35:54,386 - INFO - [TRAIN] Epoch: 3/30 | Batch: 45/452 (10.2%) | Loss: 0.9225 | Batch time: 0.02s
2025-03-02 20:35:55,437 - INFO - [TRAIN] Epoch: 3/30 | Batch: 90/452 (20.1%) | Loss: 1.1223 | Batch time: 0.02s
2025-03-02 20:35:56,527 - INFO - [TRAIN] Epoch: 3/30 | Batch: 135/452 (30.1%) | Loss: 0.9104 | Batch time: 0.02s
2025-03-02 20:35:57,636 - INFO - [TRAIN] Epoch: 3/30 | Batch: 180/452 (40.0%) | Loss: 1.2550 | Batch time: 0.02s
2025-03-02 20:35:58,758 - INFO - [TRAIN] Epoch: 3/30 | Batch: 225/452 (50.0%) | Loss: 1.4601 | Batch time: 0.02s
2025-03-02 20:35:59,867 - INFO - [TRAIN] Epoch: 3/30 | Batch: 270/452 (60.0%) | Loss: 0.8465 | Batch time: 0.02s
2025-03-02 20:36:00,999 - INFO - [TRAIN] Epoch: 3/30 | Batch: 315/452 (69.9%) | Loss: 1.1381 | Batch time: 0.02s
2025-03-02 20:36:02,129 - INFO - [TRAIN] Epoch: 3/30 | Batch: 360/452 (79.9%) | Loss: 0.9124 | Batch time: 0.02s
2025-03-02 20:36:03,257 - INFO - [TRAIN] Epoch: 3/30 | Batch: 405/452 (89.8%) | Loss: 0.8653 | Batch time: 0.03s
2025-03-02 20:36:04,321 - INFO - [TRAIN] Epoch: 3/30 | Batch: 450/452 (99.8%) | Loss: 0.9637 | Batch time: 0.02s
2025-03-02 20:36:04,336 - INFO - [TRAIN] Epoch: 3/30 | Batch: 451/452 (100.0%) | Loss: 0.9854 | Batch time: 0.01s
2025-03-02 20:36:04,425 - INFO - [VAL] Epoch: 3/30 | Batch: 0/97 (1.0%) | Loss: 0.5931 | Batch time: 0.02s
2025-03-02 20:36:04,602 - INFO - [VAL] Epoch: 3/30 | Batch: 9/97 (10.3%) | Loss: 0.7180 | Batch time: 0.02s
2025-03-02 20:36:04,769 - INFO - [VAL] Epoch: 3/30 | Batch: 18/97 (19.6%) | Loss: 0.6918 | Batch time: 0.02s
2025-03-02 20:36:04,936 - INFO - [VAL] Epoch: 3/30 | Batch: 27/97 (28.9%) | Loss: 0.3775 | Batch time: 0.02s
2025-03-02 20:36:05,102 - INFO - [VAL] Epoch: 3/30 | Batch: 36/97 (38.1%) | Loss: 0.4503 | Batch time: 0.02s
2025-03-02 20:36:05,270 - INFO - [VAL] Epoch: 3/30 | Batch: 45/97 (47.4%) | Loss: 0.5045 | Batch time: 0.02s
2025-03-02 20:36:05,437 - INFO - [VAL] Epoch: 3/30 | Batch: 54/97 (56.7%) | Loss: 0.9990 | Batch time: 0.02s
2025-03-02 20:36:05,606 - INFO - [VAL] Epoch: 3/30 | Batch: 63/97 (66.0%) | Loss: 0.3435 | Batch time: 0.02s
2025-03-02 20:36:05,771 - INFO - [VAL] Epoch: 3/30 | Batch: 72/97 (75.3%) | Loss: 0.5560 | Batch time: 0.02s
2025-03-02 20:36:05,934 - INFO - [VAL] Epoch: 3/30 | Batch: 81/97 (84.5%) | Loss: 0.5980 | Batch time: 0.02s
2025-03-02 20:36:06,096 - INFO - [VAL] Epoch: 3/30 | Batch: 90/97 (93.8%) | Loss: 0.5302 | Batch time: 0.02s
2025-03-02 20:36:06,201 - INFO - [VAL] Epoch: 3/30 | Batch: 96/97 (100.0%) | Loss: 0.7950 | Batch time: 0.01s
2025-03-02 20:36:06,329 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 3)
2025-03-02 20:36:06,329 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:06,329 - INFO - Epoch 3/30 completed in 13.30s
2025-03-02 20:36:06,329 - INFO - Training   - Loss: 0.9783, Accuracy: 0.6956, F1: 0.6987
2025-03-02 20:36:06,329 - INFO - Validation - Loss: 0.6318, Accuracy: 0.8065, F1: 0.8124
2025-03-02 20:36:06,329 - INFO - Validation F1 improved from 0.7894 to 0.8124
2025-03-02 20:36:06,329 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:06,329 - INFO - Epoch 4/30
2025-03-02 20:36:06,329 - INFO - ----------------------------------------
2025-03-02 20:36:06,592 - INFO - [TRAIN] Epoch: 4/30 | Batch: 0/452 (0.2%) | Loss: 0.6260 | Batch time: 0.03s
2025-03-02 20:36:07,836 - INFO - [TRAIN] Epoch: 4/30 | Batch: 45/452 (10.2%) | Loss: 1.0118 | Batch time: 0.03s
2025-03-02 20:36:08,970 - INFO - [TRAIN] Epoch: 4/30 | Batch: 90/452 (20.1%) | Loss: 0.9248 | Batch time: 0.03s
2025-03-02 20:36:10,115 - INFO - [TRAIN] Epoch: 4/30 | Batch: 135/452 (30.1%) | Loss: 0.7632 | Batch time: 0.02s
2025-03-02 20:36:11,272 - INFO - [TRAIN] Epoch: 4/30 | Batch: 180/452 (40.0%) | Loss: 1.2652 | Batch time: 0.03s
2025-03-02 20:36:12,415 - INFO - [TRAIN] Epoch: 4/30 | Batch: 225/452 (50.0%) | Loss: 0.9320 | Batch time: 0.02s
2025-03-02 20:36:13,564 - INFO - [TRAIN] Epoch: 4/30 | Batch: 270/452 (60.0%) | Loss: 1.0913 | Batch time: 0.03s
2025-03-02 20:36:14,760 - INFO - [TRAIN] Epoch: 4/30 | Batch: 315/452 (69.9%) | Loss: 1.1429 | Batch time: 0.03s
2025-03-02 20:36:15,911 - INFO - [TRAIN] Epoch: 4/30 | Batch: 360/452 (79.9%) | Loss: 0.6635 | Batch time: 0.03s
2025-03-02 20:36:17,089 - INFO - [TRAIN] Epoch: 4/30 | Batch: 405/452 (89.8%) | Loss: 0.7379 | Batch time: 0.03s
2025-03-02 20:36:18,194 - INFO - [TRAIN] Epoch: 4/30 | Batch: 450/452 (99.8%) | Loss: 0.9638 | Batch time: 0.02s
2025-03-02 20:36:18,209 - INFO - [TRAIN] Epoch: 4/30 | Batch: 451/452 (100.0%) | Loss: 0.8387 | Batch time: 0.01s
2025-03-02 20:36:18,303 - INFO - [VAL] Epoch: 4/30 | Batch: 0/97 (1.0%) | Loss: 0.5595 | Batch time: 0.02s
2025-03-02 20:36:18,483 - INFO - [VAL] Epoch: 4/30 | Batch: 9/97 (10.3%) | Loss: 0.8079 | Batch time: 0.02s
2025-03-02 20:36:18,656 - INFO - [VAL] Epoch: 4/30 | Batch: 18/97 (19.6%) | Loss: 0.5984 | Batch time: 0.02s
2025-03-02 20:36:18,829 - INFO - [VAL] Epoch: 4/30 | Batch: 27/97 (28.9%) | Loss: 0.3357 | Batch time: 0.02s
2025-03-02 20:36:19,000 - INFO - [VAL] Epoch: 4/30 | Batch: 36/97 (38.1%) | Loss: 0.4111 | Batch time: 0.02s
2025-03-02 20:36:19,173 - INFO - [VAL] Epoch: 4/30 | Batch: 45/97 (47.4%) | Loss: 0.4626 | Batch time: 0.02s
2025-03-02 20:36:19,346 - INFO - [VAL] Epoch: 4/30 | Batch: 54/97 (56.7%) | Loss: 1.0901 | Batch time: 0.02s
2025-03-02 20:36:19,520 - INFO - [VAL] Epoch: 4/30 | Batch: 63/97 (66.0%) | Loss: 0.3116 | Batch time: 0.02s
2025-03-02 20:36:19,691 - INFO - [VAL] Epoch: 4/30 | Batch: 72/97 (75.3%) | Loss: 0.4990 | Batch time: 0.02s
2025-03-02 20:36:19,861 - INFO - [VAL] Epoch: 4/30 | Batch: 81/97 (84.5%) | Loss: 0.6126 | Batch time: 0.02s
2025-03-02 20:36:20,030 - INFO - [VAL] Epoch: 4/30 | Batch: 90/97 (93.8%) | Loss: 0.4684 | Batch time: 0.02s
2025-03-02 20:36:20,139 - INFO - [VAL] Epoch: 4/30 | Batch: 96/97 (100.0%) | Loss: 0.7997 | Batch time: 0.01s
2025-03-02 20:36:20,275 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 4)
2025-03-02 20:36:20,275 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:20,275 - INFO - Epoch 4/30 completed in 13.95s
2025-03-02 20:36:20,275 - INFO - Training   - Loss: 0.9342, Accuracy: 0.7109, F1: 0.7141
2025-03-02 20:36:20,275 - INFO - Validation - Loss: 0.6046, Accuracy: 0.8078, F1: 0.8160
2025-03-02 20:36:20,275 - INFO - Validation F1 improved from 0.8124 to 0.8160
2025-03-02 20:36:20,275 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:20,275 - INFO - Epoch 5/30
2025-03-02 20:36:20,275 - INFO - ----------------------------------------
2025-03-02 20:36:20,534 - INFO - [TRAIN] Epoch: 5/30 | Batch: 0/452 (0.2%) | Loss: 0.5890 | Batch time: 0.05s
2025-03-02 20:36:21,761 - INFO - [TRAIN] Epoch: 5/30 | Batch: 45/452 (10.2%) | Loss: 0.7020 | Batch time: 0.02s
2025-03-02 20:36:22,911 - INFO - [TRAIN] Epoch: 5/30 | Batch: 90/452 (20.1%) | Loss: 1.0987 | Batch time: 0.02s
2025-03-02 20:36:24,090 - INFO - [TRAIN] Epoch: 5/30 | Batch: 135/452 (30.1%) | Loss: 1.3921 | Batch time: 0.02s
2025-03-02 20:36:25,237 - INFO - [TRAIN] Epoch: 5/30 | Batch: 180/452 (40.0%) | Loss: 0.6305 | Batch time: 0.03s
2025-03-02 20:36:26,405 - INFO - [TRAIN] Epoch: 5/30 | Batch: 225/452 (50.0%) | Loss: 0.7335 | Batch time: 0.03s
2025-03-02 20:36:27,575 - INFO - [TRAIN] Epoch: 5/30 | Batch: 270/452 (60.0%) | Loss: 1.0106 | Batch time: 0.03s
2025-03-02 20:36:28,757 - INFO - [TRAIN] Epoch: 5/30 | Batch: 315/452 (69.9%) | Loss: 1.2067 | Batch time: 0.03s
2025-03-02 20:36:29,932 - INFO - [TRAIN] Epoch: 5/30 | Batch: 360/452 (79.9%) | Loss: 1.2116 | Batch time: 0.03s
2025-03-02 20:36:31,108 - INFO - [TRAIN] Epoch: 5/30 | Batch: 405/452 (89.8%) | Loss: 1.0374 | Batch time: 0.03s
2025-03-02 20:36:32,232 - INFO - [TRAIN] Epoch: 5/30 | Batch: 450/452 (99.8%) | Loss: 1.1068 | Batch time: 0.02s
2025-03-02 20:36:32,247 - INFO - [TRAIN] Epoch: 5/30 | Batch: 451/452 (100.0%) | Loss: 1.0764 | Batch time: 0.01s
2025-03-02 20:36:32,330 - INFO - [VAL] Epoch: 5/30 | Batch: 0/97 (1.0%) | Loss: 0.4804 | Batch time: 0.02s
2025-03-02 20:36:32,512 - INFO - [VAL] Epoch: 5/30 | Batch: 9/97 (10.3%) | Loss: 0.5777 | Batch time: 0.02s
2025-03-02 20:36:32,680 - INFO - [VAL] Epoch: 5/30 | Batch: 18/97 (19.6%) | Loss: 0.5293 | Batch time: 0.02s
2025-03-02 20:36:32,846 - INFO - [VAL] Epoch: 5/30 | Batch: 27/97 (28.9%) | Loss: 0.3486 | Batch time: 0.02s
2025-03-02 20:36:33,018 - INFO - [VAL] Epoch: 5/30 | Batch: 36/97 (38.1%) | Loss: 0.2834 | Batch time: 0.02s
2025-03-02 20:36:33,185 - INFO - [VAL] Epoch: 5/30 | Batch: 45/97 (47.4%) | Loss: 0.4518 | Batch time: 0.02s
2025-03-02 20:36:33,355 - INFO - [VAL] Epoch: 5/30 | Batch: 54/97 (56.7%) | Loss: 0.8287 | Batch time: 0.02s
2025-03-02 20:36:33,523 - INFO - [VAL] Epoch: 5/30 | Batch: 63/97 (66.0%) | Loss: 0.2812 | Batch time: 0.02s
2025-03-02 20:36:33,690 - INFO - [VAL] Epoch: 5/30 | Batch: 72/97 (75.3%) | Loss: 0.4555 | Batch time: 0.02s
2025-03-02 20:36:33,856 - INFO - [VAL] Epoch: 5/30 | Batch: 81/97 (84.5%) | Loss: 0.5166 | Batch time: 0.02s
2025-03-02 20:36:34,020 - INFO - [VAL] Epoch: 5/30 | Batch: 90/97 (93.8%) | Loss: 0.4473 | Batch time: 0.02s
2025-03-02 20:36:34,126 - INFO - [VAL] Epoch: 5/30 | Batch: 96/97 (100.0%) | Loss: 0.6450 | Batch time: 0.01s
2025-03-02 20:36:34,258 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 5)
2025-03-02 20:36:34,258 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:34,258 - INFO - Epoch 5/30 completed in 13.98s
2025-03-02 20:36:34,258 - INFO - Training   - Loss: 0.9013, Accuracy: 0.7094, F1: 0.7124
2025-03-02 20:36:34,258 - INFO - Validation - Loss: 0.5417, Accuracy: 0.8343, F1: 0.8349
2025-03-02 20:36:34,258 - INFO - Validation F1 improved from 0.8160 to 0.8349
2025-03-02 20:36:34,258 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:34,324 - INFO - Checkpoint saved: checkpoint_epoch_5.pth (Epoch 5)
2025-03-02 20:36:34,325 - INFO - Epoch 6/30
2025-03-02 20:36:34,325 - INFO - ----------------------------------------
2025-03-02 20:36:34,574 - INFO - [TRAIN] Epoch: 6/30 | Batch: 0/452 (0.2%) | Loss: 1.0123 | Batch time: 0.04s
2025-03-02 20:36:35,735 - INFO - [TRAIN] Epoch: 6/30 | Batch: 45/452 (10.2%) | Loss: 0.6167 | Batch time: 0.02s
2025-03-02 20:36:36,813 - INFO - [TRAIN] Epoch: 6/30 | Batch: 90/452 (20.1%) | Loss: 0.9496 | Batch time: 0.02s
2025-03-02 20:36:37,934 - INFO - [TRAIN] Epoch: 6/30 | Batch: 135/452 (30.1%) | Loss: 0.6367 | Batch time: 0.02s
2025-03-02 20:36:39,085 - INFO - [TRAIN] Epoch: 6/30 | Batch: 180/452 (40.0%) | Loss: 0.9856 | Batch time: 0.03s
2025-03-02 20:36:40,242 - INFO - [TRAIN] Epoch: 6/30 | Batch: 225/452 (50.0%) | Loss: 0.9478 | Batch time: 0.02s
2025-03-02 20:36:41,396 - INFO - [TRAIN] Epoch: 6/30 | Batch: 270/452 (60.0%) | Loss: 0.5321 | Batch time: 0.02s
2025-03-02 20:36:42,564 - INFO - [TRAIN] Epoch: 6/30 | Batch: 315/452 (69.9%) | Loss: 0.5793 | Batch time: 0.03s
2025-03-02 20:36:43,871 - INFO - [TRAIN] Epoch: 6/30 | Batch: 360/452 (79.9%) | Loss: 1.0541 | Batch time: 0.03s
2025-03-02 20:36:45,023 - INFO - [TRAIN] Epoch: 6/30 | Batch: 405/452 (89.8%) | Loss: 0.9023 | Batch time: 0.03s
2025-03-02 20:36:46,102 - INFO - [TRAIN] Epoch: 6/30 | Batch: 450/452 (99.8%) | Loss: 0.6374 | Batch time: 0.02s
2025-03-02 20:36:46,117 - INFO - [TRAIN] Epoch: 6/30 | Batch: 451/452 (100.0%) | Loss: 0.8823 | Batch time: 0.01s
2025-03-02 20:36:46,199 - INFO - [VAL] Epoch: 6/30 | Batch: 0/97 (1.0%) | Loss: 0.4776 | Batch time: 0.02s
2025-03-02 20:36:46,376 - INFO - [VAL] Epoch: 6/30 | Batch: 9/97 (10.3%) | Loss: 0.5067 | Batch time: 0.02s
2025-03-02 20:36:46,539 - INFO - [VAL] Epoch: 6/30 | Batch: 18/97 (19.6%) | Loss: 0.4944 | Batch time: 0.02s
2025-03-02 20:36:46,704 - INFO - [VAL] Epoch: 6/30 | Batch: 27/97 (28.9%) | Loss: 0.2522 | Batch time: 0.02s
2025-03-02 20:36:46,868 - INFO - [VAL] Epoch: 6/30 | Batch: 36/97 (38.1%) | Loss: 0.3189 | Batch time: 0.02s
2025-03-02 20:36:47,033 - INFO - [VAL] Epoch: 6/30 | Batch: 45/97 (47.4%) | Loss: 0.3590 | Batch time: 0.02s
2025-03-02 20:36:47,199 - INFO - [VAL] Epoch: 6/30 | Batch: 54/97 (56.7%) | Loss: 0.8143 | Batch time: 0.02s
2025-03-02 20:36:47,364 - INFO - [VAL] Epoch: 6/30 | Batch: 63/97 (66.0%) | Loss: 0.2777 | Batch time: 0.02s
2025-03-02 20:36:47,528 - INFO - [VAL] Epoch: 6/30 | Batch: 72/97 (75.3%) | Loss: 0.4233 | Batch time: 0.02s
2025-03-02 20:36:47,692 - INFO - [VAL] Epoch: 6/30 | Batch: 81/97 (84.5%) | Loss: 0.5036 | Batch time: 0.02s
2025-03-02 20:36:47,854 - INFO - [VAL] Epoch: 6/30 | Batch: 90/97 (93.8%) | Loss: 0.4203 | Batch time: 0.02s
2025-03-02 20:36:47,958 - INFO - [VAL] Epoch: 6/30 | Batch: 96/97 (100.0%) | Loss: 0.5238 | Batch time: 0.01s
2025-03-02 20:36:48,092 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 6)
2025-03-02 20:36:48,092 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:48,092 - INFO - Epoch 6/30 completed in 13.77s
2025-03-02 20:36:48,092 - INFO - Training   - Loss: 0.9094, Accuracy: 0.7092, F1: 0.7130
2025-03-02 20:36:48,092 - INFO - Validation - Loss: 0.5044, Accuracy: 0.8356, F1: 0.8391
2025-03-02 20:36:48,092 - INFO - Validation F1 improved from 0.8349 to 0.8391
2025-03-02 20:36:48,092 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:36:48,092 - INFO - Epoch 7/30
2025-03-02 20:36:48,092 - INFO - ----------------------------------------
2025-03-02 20:36:48,310 - INFO - [TRAIN] Epoch: 7/30 | Batch: 0/452 (0.2%) | Loss: 0.4777 | Batch time: 0.03s
2025-03-02 20:36:49,451 - INFO - [TRAIN] Epoch: 7/30 | Batch: 45/452 (10.2%) | Loss: 0.8906 | Batch time: 0.02s
2025-03-02 20:36:50,584 - INFO - [TRAIN] Epoch: 7/30 | Batch: 90/452 (20.1%) | Loss: 1.0494 | Batch time: 0.03s
2025-03-02 20:36:51,722 - INFO - [TRAIN] Epoch: 7/30 | Batch: 135/452 (30.1%) | Loss: 0.8056 | Batch time: 0.02s
2025-03-02 20:36:52,867 - INFO - [TRAIN] Epoch: 7/30 | Batch: 180/452 (40.0%) | Loss: 0.7347 | Batch time: 0.02s
2025-03-02 20:36:54,056 - INFO - [TRAIN] Epoch: 7/30 | Batch: 225/452 (50.0%) | Loss: 1.0676 | Batch time: 0.03s
2025-03-02 20:36:55,444 - INFO - [TRAIN] Epoch: 7/30 | Batch: 270/452 (60.0%) | Loss: 1.0446 | Batch time: 0.03s
2025-03-02 20:36:56,622 - INFO - [TRAIN] Epoch: 7/30 | Batch: 315/452 (69.9%) | Loss: 1.0113 | Batch time: 0.03s
2025-03-02 20:36:57,805 - INFO - [TRAIN] Epoch: 7/30 | Batch: 360/452 (79.9%) | Loss: 0.6623 | Batch time: 0.03s
2025-03-02 20:36:58,966 - INFO - [TRAIN] Epoch: 7/30 | Batch: 405/452 (89.8%) | Loss: 0.9232 | Batch time: 0.03s
2025-03-02 20:37:00,057 - INFO - [TRAIN] Epoch: 7/30 | Batch: 450/452 (99.8%) | Loss: 0.5859 | Batch time: 0.02s
2025-03-02 20:37:00,075 - INFO - [TRAIN] Epoch: 7/30 | Batch: 451/452 (100.0%) | Loss: 0.4242 | Batch time: 0.02s
2025-03-02 20:37:00,162 - INFO - [VAL] Epoch: 7/30 | Batch: 0/97 (1.0%) | Loss: 0.5150 | Batch time: 0.02s
2025-03-02 20:37:00,335 - INFO - [VAL] Epoch: 7/30 | Batch: 9/97 (10.3%) | Loss: 0.7560 | Batch time: 0.02s
2025-03-02 20:37:00,497 - INFO - [VAL] Epoch: 7/30 | Batch: 18/97 (19.6%) | Loss: 0.5144 | Batch time: 0.02s
2025-03-02 20:37:00,658 - INFO - [VAL] Epoch: 7/30 | Batch: 27/97 (28.9%) | Loss: 0.3257 | Batch time: 0.02s
2025-03-02 20:37:00,821 - INFO - [VAL] Epoch: 7/30 | Batch: 36/97 (38.1%) | Loss: 0.3497 | Batch time: 0.02s
2025-03-02 20:37:00,984 - INFO - [VAL] Epoch: 7/30 | Batch: 45/97 (47.4%) | Loss: 0.2996 | Batch time: 0.02s
2025-03-02 20:37:01,149 - INFO - [VAL] Epoch: 7/30 | Batch: 54/97 (56.7%) | Loss: 0.9665 | Batch time: 0.02s
2025-03-02 20:37:01,313 - INFO - [VAL] Epoch: 7/30 | Batch: 63/97 (66.0%) | Loss: 0.3008 | Batch time: 0.02s
2025-03-02 20:37:01,476 - INFO - [VAL] Epoch: 7/30 | Batch: 72/97 (75.3%) | Loss: 0.3818 | Batch time: 0.02s
2025-03-02 20:37:01,638 - INFO - [VAL] Epoch: 7/30 | Batch: 81/97 (84.5%) | Loss: 0.5185 | Batch time: 0.02s
2025-03-02 20:37:01,801 - INFO - [VAL] Epoch: 7/30 | Batch: 90/97 (93.8%) | Loss: 0.3983 | Batch time: 0.02s
2025-03-02 20:37:01,904 - INFO - [VAL] Epoch: 7/30 | Batch: 96/97 (100.0%) | Loss: 0.7406 | Batch time: 0.01s
2025-03-02 20:37:01,907 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:01,907 - INFO - Epoch 7/30 completed in 13.82s
2025-03-02 20:37:01,907 - INFO - Training   - Loss: 0.8818, Accuracy: 0.7192, F1: 0.7225
2025-03-02 20:37:01,908 - INFO - Validation - Loss: 0.5222, Accuracy: 0.8211, F1: 0.8281
2025-03-02 20:37:01,908 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:01,908 - INFO - Epoch 8/30
2025-03-02 20:37:01,908 - INFO - ----------------------------------------
2025-03-02 20:37:02,125 - INFO - [TRAIN] Epoch: 8/30 | Batch: 0/452 (0.2%) | Loss: 1.1883 | Batch time: 0.03s
2025-03-02 20:37:03,323 - INFO - [TRAIN] Epoch: 8/30 | Batch: 45/452 (10.2%) | Loss: 1.1280 | Batch time: 0.02s
2025-03-02 20:37:04,474 - INFO - [TRAIN] Epoch: 8/30 | Batch: 90/452 (20.1%) | Loss: 0.6495 | Batch time: 0.02s
2025-03-02 20:37:05,632 - INFO - [TRAIN] Epoch: 8/30 | Batch: 135/452 (30.1%) | Loss: 0.6588 | Batch time: 0.02s
2025-03-02 20:37:06,807 - INFO - [TRAIN] Epoch: 8/30 | Batch: 180/452 (40.0%) | Loss: 0.6673 | Batch time: 0.03s
2025-03-02 20:37:08,007 - INFO - [TRAIN] Epoch: 8/30 | Batch: 225/452 (50.0%) | Loss: 1.0962 | Batch time: 0.03s
2025-03-02 20:37:09,201 - INFO - [TRAIN] Epoch: 8/30 | Batch: 270/452 (60.0%) | Loss: 0.6512 | Batch time: 0.03s
2025-03-02 20:37:10,403 - INFO - [TRAIN] Epoch: 8/30 | Batch: 315/452 (69.9%) | Loss: 0.8739 | Batch time: 0.03s
2025-03-02 20:37:11,621 - INFO - [TRAIN] Epoch: 8/30 | Batch: 360/452 (79.9%) | Loss: 1.1712 | Batch time: 0.03s
2025-03-02 20:37:12,843 - INFO - [TRAIN] Epoch: 8/30 | Batch: 405/452 (89.8%) | Loss: 0.8579 | Batch time: 0.03s
2025-03-02 20:37:13,938 - INFO - [TRAIN] Epoch: 8/30 | Batch: 450/452 (99.8%) | Loss: 0.7046 | Batch time: 0.02s
2025-03-02 20:37:13,953 - INFO - [TRAIN] Epoch: 8/30 | Batch: 451/452 (100.0%) | Loss: 0.8843 | Batch time: 0.02s
2025-03-02 20:37:14,034 - INFO - [VAL] Epoch: 8/30 | Batch: 0/97 (1.0%) | Loss: 0.5282 | Batch time: 0.02s
2025-03-02 20:37:14,206 - INFO - [VAL] Epoch: 8/30 | Batch: 9/97 (10.3%) | Loss: 0.6042 | Batch time: 0.02s
2025-03-02 20:37:14,371 - INFO - [VAL] Epoch: 8/30 | Batch: 18/97 (19.6%) | Loss: 0.5774 | Batch time: 0.02s
2025-03-02 20:37:14,536 - INFO - [VAL] Epoch: 8/30 | Batch: 27/97 (28.9%) | Loss: 0.2962 | Batch time: 0.02s
2025-03-02 20:37:14,703 - INFO - [VAL] Epoch: 8/30 | Batch: 36/97 (38.1%) | Loss: 0.3267 | Batch time: 0.02s
2025-03-02 20:37:14,869 - INFO - [VAL] Epoch: 8/30 | Batch: 45/97 (47.4%) | Loss: 0.3186 | Batch time: 0.02s
2025-03-02 20:37:15,036 - INFO - [VAL] Epoch: 8/30 | Batch: 54/97 (56.7%) | Loss: 0.8939 | Batch time: 0.02s
2025-03-02 20:37:15,203 - INFO - [VAL] Epoch: 8/30 | Batch: 63/97 (66.0%) | Loss: 0.2762 | Batch time: 0.02s
2025-03-02 20:37:15,367 - INFO - [VAL] Epoch: 8/30 | Batch: 72/97 (75.3%) | Loss: 0.4166 | Batch time: 0.02s
2025-03-02 20:37:15,532 - INFO - [VAL] Epoch: 8/30 | Batch: 81/97 (84.5%) | Loss: 0.5295 | Batch time: 0.02s
2025-03-02 20:37:15,698 - INFO - [VAL] Epoch: 8/30 | Batch: 90/97 (93.8%) | Loss: 0.3814 | Batch time: 0.02s
2025-03-02 20:37:15,804 - INFO - [VAL] Epoch: 8/30 | Batch: 96/97 (100.0%) | Loss: 0.7054 | Batch time: 0.01s
2025-03-02 20:37:15,808 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:15,808 - INFO - Epoch 8/30 completed in 13.90s
2025-03-02 20:37:15,808 - INFO - Training   - Loss: 0.8637, Accuracy: 0.7238, F1: 0.7270
2025-03-02 20:37:15,808 - INFO - Validation - Loss: 0.5182, Accuracy: 0.8291, F1: 0.8351
2025-03-02 20:37:15,808 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:15,808 - INFO - Epoch 9/30
2025-03-02 20:37:15,808 - INFO - ----------------------------------------
2025-03-02 20:37:16,025 - INFO - [TRAIN] Epoch: 9/30 | Batch: 0/452 (0.2%) | Loss: 0.6040 | Batch time: 0.03s
2025-03-02 20:37:17,332 - INFO - [TRAIN] Epoch: 9/30 | Batch: 45/452 (10.2%) | Loss: 0.6442 | Batch time: 0.03s
2025-03-02 20:37:18,600 - INFO - [TRAIN] Epoch: 9/30 | Batch: 90/452 (20.1%) | Loss: 1.0403 | Batch time: 0.03s
2025-03-02 20:37:19,876 - INFO - [TRAIN] Epoch: 9/30 | Batch: 135/452 (30.1%) | Loss: 0.9473 | Batch time: 0.03s
2025-03-02 20:37:21,164 - INFO - [TRAIN] Epoch: 9/30 | Batch: 180/452 (40.0%) | Loss: 0.8412 | Batch time: 0.03s
2025-03-02 20:37:22,436 - INFO - [TRAIN] Epoch: 9/30 | Batch: 225/452 (50.0%) | Loss: 0.8733 | Batch time: 0.03s
2025-03-02 20:37:23,722 - INFO - [TRAIN] Epoch: 9/30 | Batch: 270/452 (60.0%) | Loss: 1.0494 | Batch time: 0.03s
2025-03-02 20:37:25,061 - INFO - [TRAIN] Epoch: 9/30 | Batch: 315/452 (69.9%) | Loss: 0.7770 | Batch time: 0.03s
2025-03-02 20:37:26,379 - INFO - [TRAIN] Epoch: 9/30 | Batch: 360/452 (79.9%) | Loss: 1.1203 | Batch time: 0.03s
2025-03-02 20:37:27,683 - INFO - [TRAIN] Epoch: 9/30 | Batch: 405/452 (89.8%) | Loss: 0.7568 | Batch time: 0.03s
2025-03-02 20:37:28,817 - INFO - [TRAIN] Epoch: 9/30 | Batch: 450/452 (99.8%) | Loss: 0.9257 | Batch time: 0.02s
2025-03-02 20:37:28,833 - INFO - [TRAIN] Epoch: 9/30 | Batch: 451/452 (100.0%) | Loss: 0.4791 | Batch time: 0.02s
2025-03-02 20:37:28,919 - INFO - [VAL] Epoch: 9/30 | Batch: 0/97 (1.0%) | Loss: 0.5039 | Batch time: 0.02s
2025-03-02 20:37:29,091 - INFO - [VAL] Epoch: 9/30 | Batch: 9/97 (10.3%) | Loss: 0.5866 | Batch time: 0.02s
2025-03-02 20:37:29,254 - INFO - [VAL] Epoch: 9/30 | Batch: 18/97 (19.6%) | Loss: 0.4508 | Batch time: 0.02s
2025-03-02 20:37:29,418 - INFO - [VAL] Epoch: 9/30 | Batch: 27/97 (28.9%) | Loss: 0.2847 | Batch time: 0.02s
2025-03-02 20:37:29,582 - INFO - [VAL] Epoch: 9/30 | Batch: 36/97 (38.1%) | Loss: 0.3217 | Batch time: 0.02s
2025-03-02 20:37:29,749 - INFO - [VAL] Epoch: 9/30 | Batch: 45/97 (47.4%) | Loss: 0.3146 | Batch time: 0.02s
2025-03-02 20:37:29,914 - INFO - [VAL] Epoch: 9/30 | Batch: 54/97 (56.7%) | Loss: 0.8656 | Batch time: 0.02s
2025-03-02 20:37:30,080 - INFO - [VAL] Epoch: 9/30 | Batch: 63/97 (66.0%) | Loss: 0.2886 | Batch time: 0.02s
2025-03-02 20:37:30,247 - INFO - [VAL] Epoch: 9/30 | Batch: 72/97 (75.3%) | Loss: 0.3877 | Batch time: 0.02s
2025-03-02 20:37:30,413 - INFO - [VAL] Epoch: 9/30 | Batch: 81/97 (84.5%) | Loss: 0.4951 | Batch time: 0.02s
2025-03-02 20:37:30,578 - INFO - [VAL] Epoch: 9/30 | Batch: 90/97 (93.8%) | Loss: 0.3651 | Batch time: 0.02s
2025-03-02 20:37:30,685 - INFO - [VAL] Epoch: 9/30 | Batch: 96/97 (100.0%) | Loss: 0.6104 | Batch time: 0.01s
2025-03-02 20:37:30,816 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 9)
2025-03-02 20:37:30,816 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:30,816 - INFO - Epoch 9/30 completed in 15.01s
2025-03-02 20:37:30,816 - INFO - Training   - Loss: 0.8623, Accuracy: 0.7247, F1: 0.7276
2025-03-02 20:37:30,816 - INFO - Validation - Loss: 0.4921, Accuracy: 0.8379, F1: 0.8429
2025-03-02 20:37:30,816 - INFO - Validation F1 improved from 0.8391 to 0.8429
2025-03-02 20:37:30,816 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:30,816 - INFO - Epoch 10/30
2025-03-02 20:37:30,816 - INFO - ----------------------------------------
2025-03-02 20:37:31,024 - INFO - [TRAIN] Epoch: 10/30 | Batch: 0/452 (0.2%) | Loss: 0.8222 | Batch time: 0.04s
2025-03-02 20:37:32,422 - INFO - [TRAIN] Epoch: 10/30 | Batch: 45/452 (10.2%) | Loss: 0.5352 | Batch time: 0.03s
2025-03-02 20:37:33,744 - INFO - [TRAIN] Epoch: 10/30 | Batch: 90/452 (20.1%) | Loss: 0.9885 | Batch time: 0.03s
2025-03-02 20:37:35,110 - INFO - [TRAIN] Epoch: 10/30 | Batch: 135/452 (30.1%) | Loss: 0.6691 | Batch time: 0.03s
2025-03-02 20:37:36,461 - INFO - [TRAIN] Epoch: 10/30 | Batch: 180/452 (40.0%) | Loss: 1.1048 | Batch time: 0.03s
2025-03-02 20:37:37,855 - INFO - [TRAIN] Epoch: 10/30 | Batch: 225/452 (50.0%) | Loss: 0.8524 | Batch time: 0.03s
2025-03-02 20:37:39,229 - INFO - [TRAIN] Epoch: 10/30 | Batch: 270/452 (60.0%) | Loss: 0.7802 | Batch time: 0.03s
2025-03-02 20:37:40,608 - INFO - [TRAIN] Epoch: 10/30 | Batch: 315/452 (69.9%) | Loss: 0.7567 | Batch time: 0.03s
2025-03-02 20:37:42,028 - INFO - [TRAIN] Epoch: 10/30 | Batch: 360/452 (79.9%) | Loss: 1.0921 | Batch time: 0.03s
2025-03-02 20:37:43,421 - INFO - [TRAIN] Epoch: 10/30 | Batch: 405/452 (89.8%) | Loss: 0.9177 | Batch time: 0.03s
2025-03-02 20:37:44,595 - INFO - [TRAIN] Epoch: 10/30 | Batch: 450/452 (99.8%) | Loss: 0.5816 | Batch time: 0.02s
2025-03-02 20:37:44,611 - INFO - [TRAIN] Epoch: 10/30 | Batch: 451/452 (100.0%) | Loss: 1.4367 | Batch time: 0.02s
2025-03-02 20:37:44,699 - INFO - [VAL] Epoch: 10/30 | Batch: 0/97 (1.0%) | Loss: 0.4259 | Batch time: 0.02s
2025-03-02 20:37:44,877 - INFO - [VAL] Epoch: 10/30 | Batch: 9/97 (10.3%) | Loss: 0.5937 | Batch time: 0.02s
2025-03-02 20:37:45,042 - INFO - [VAL] Epoch: 10/30 | Batch: 18/97 (19.6%) | Loss: 0.4679 | Batch time: 0.02s
2025-03-02 20:37:45,208 - INFO - [VAL] Epoch: 10/30 | Batch: 27/97 (28.9%) | Loss: 0.2697 | Batch time: 0.02s
2025-03-02 20:37:45,374 - INFO - [VAL] Epoch: 10/30 | Batch: 36/97 (38.1%) | Loss: 0.3118 | Batch time: 0.02s
2025-03-02 20:37:45,543 - INFO - [VAL] Epoch: 10/30 | Batch: 45/97 (47.4%) | Loss: 0.2901 | Batch time: 0.02s
2025-03-02 20:37:45,713 - INFO - [VAL] Epoch: 10/30 | Batch: 54/97 (56.7%) | Loss: 0.8320 | Batch time: 0.02s
2025-03-02 20:37:45,884 - INFO - [VAL] Epoch: 10/30 | Batch: 63/97 (66.0%) | Loss: 0.2428 | Batch time: 0.02s
2025-03-02 20:37:46,052 - INFO - [VAL] Epoch: 10/30 | Batch: 72/97 (75.3%) | Loss: 0.3649 | Batch time: 0.02s
2025-03-02 20:37:46,219 - INFO - [VAL] Epoch: 10/30 | Batch: 81/97 (84.5%) | Loss: 0.4438 | Batch time: 0.02s
2025-03-02 20:37:46,385 - INFO - [VAL] Epoch: 10/30 | Batch: 90/97 (93.8%) | Loss: 0.3772 | Batch time: 0.02s
2025-03-02 20:37:46,491 - INFO - [VAL] Epoch: 10/30 | Batch: 96/97 (100.0%) | Loss: 0.5366 | Batch time: 0.01s
2025-03-02 20:37:46,618 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 10)
2025-03-02 20:37:46,618 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:46,618 - INFO - Epoch 10/30 completed in 15.80s
2025-03-02 20:37:46,618 - INFO - Training   - Loss: 0.8747, Accuracy: 0.7219, F1: 0.7253
2025-03-02 20:37:46,618 - INFO - Validation - Loss: 0.4687, Accuracy: 0.8479, F1: 0.8514
2025-03-02 20:37:46,618 - INFO - Validation F1 improved from 0.8429 to 0.8514
2025-03-02 20:37:46,618 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:37:46,684 - INFO - Checkpoint saved: checkpoint_epoch_10.pth (Epoch 10)
2025-03-02 20:37:46,684 - INFO - Epoch 11/30
2025-03-02 20:37:46,684 - INFO - ----------------------------------------
2025-03-02 20:37:46,902 - INFO - [TRAIN] Epoch: 11/30 | Batch: 0/452 (0.2%) | Loss: 0.6689 | Batch time: 0.04s
2025-03-02 20:37:48,454 - INFO - [TRAIN] Epoch: 11/30 | Batch: 45/452 (10.2%) | Loss: 0.6531 | Batch time: 0.03s
2025-03-02 20:37:49,878 - INFO - [TRAIN] Epoch: 11/30 | Batch: 90/452 (20.1%) | Loss: 0.8111 | Batch time: 0.03s
2025-03-02 20:37:51,280 - INFO - [TRAIN] Epoch: 11/30 | Batch: 135/452 (30.1%) | Loss: 0.8316 | Batch time: 0.03s
2025-03-02 20:37:52,721 - INFO - [TRAIN] Epoch: 11/30 | Batch: 180/452 (40.0%) | Loss: 0.9942 | Batch time: 0.03s
2025-03-02 20:37:54,138 - INFO - [TRAIN] Epoch: 11/30 | Batch: 225/452 (50.0%) | Loss: 0.5987 | Batch time: 0.03s
2025-03-02 20:37:55,824 - INFO - [TRAIN] Epoch: 11/30 | Batch: 270/452 (60.0%) | Loss: 0.7362 | Batch time: 0.03s
2025-03-02 20:37:57,265 - INFO - [TRAIN] Epoch: 11/30 | Batch: 315/452 (69.9%) | Loss: 1.0040 | Batch time: 0.03s
2025-03-02 20:37:58,696 - INFO - [TRAIN] Epoch: 11/30 | Batch: 360/452 (79.9%) | Loss: 0.9530 | Batch time: 0.03s
2025-03-02 20:38:00,112 - INFO - [TRAIN] Epoch: 11/30 | Batch: 405/452 (89.8%) | Loss: 1.2768 | Batch time: 0.03s
2025-03-02 20:38:01,299 - INFO - [TRAIN] Epoch: 11/30 | Batch: 450/452 (99.8%) | Loss: 0.8138 | Batch time: 0.02s
2025-03-02 20:38:01,315 - INFO - [TRAIN] Epoch: 11/30 | Batch: 451/452 (100.0%) | Loss: 0.6177 | Batch time: 0.02s
2025-03-02 20:38:01,404 - INFO - [VAL] Epoch: 11/30 | Batch: 0/97 (1.0%) | Loss: 0.5033 | Batch time: 0.02s
2025-03-02 20:38:01,577 - INFO - [VAL] Epoch: 11/30 | Batch: 9/97 (10.3%) | Loss: 0.6668 | Batch time: 0.02s
2025-03-02 20:38:01,743 - INFO - [VAL] Epoch: 11/30 | Batch: 18/97 (19.6%) | Loss: 0.4701 | Batch time: 0.02s
2025-03-02 20:38:01,910 - INFO - [VAL] Epoch: 11/30 | Batch: 27/97 (28.9%) | Loss: 0.3060 | Batch time: 0.02s
2025-03-02 20:38:02,079 - INFO - [VAL] Epoch: 11/30 | Batch: 36/97 (38.1%) | Loss: 0.3092 | Batch time: 0.02s
2025-03-02 20:38:02,257 - INFO - [VAL] Epoch: 11/30 | Batch: 45/97 (47.4%) | Loss: 0.3064 | Batch time: 0.02s
2025-03-02 20:38:02,426 - INFO - [VAL] Epoch: 11/30 | Batch: 54/97 (56.7%) | Loss: 0.9282 | Batch time: 0.02s
2025-03-02 20:38:02,597 - INFO - [VAL] Epoch: 11/30 | Batch: 63/97 (66.0%) | Loss: 0.2840 | Batch time: 0.02s
2025-03-02 20:38:02,765 - INFO - [VAL] Epoch: 11/30 | Batch: 72/97 (75.3%) | Loss: 0.4237 | Batch time: 0.02s
2025-03-02 20:38:02,930 - INFO - [VAL] Epoch: 11/30 | Batch: 81/97 (84.5%) | Loss: 0.4882 | Batch time: 0.02s
2025-03-02 20:38:03,098 - INFO - [VAL] Epoch: 11/30 | Batch: 90/97 (93.8%) | Loss: 0.3908 | Batch time: 0.02s
2025-03-02 20:38:03,205 - INFO - [VAL] Epoch: 11/30 | Batch: 96/97 (100.0%) | Loss: 0.7866 | Batch time: 0.01s
2025-03-02 20:38:03,209 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:03,209 - INFO - Epoch 11/30 completed in 16.53s
2025-03-02 20:38:03,209 - INFO - Training   - Loss: 0.8518, Accuracy: 0.7259, F1: 0.7288
2025-03-02 20:38:03,209 - INFO - Validation - Loss: 0.5145, Accuracy: 0.8311, F1: 0.8367
2025-03-02 20:38:03,209 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:03,209 - INFO - Epoch 12/30
2025-03-02 20:38:03,209 - INFO - ----------------------------------------
2025-03-02 20:38:03,429 - INFO - [TRAIN] Epoch: 12/30 | Batch: 0/452 (0.2%) | Loss: 1.0305 | Batch time: 0.04s
2025-03-02 20:38:05,085 - INFO - [TRAIN] Epoch: 12/30 | Batch: 45/452 (10.2%) | Loss: 1.0819 | Batch time: 0.03s
2025-03-02 20:38:06,563 - INFO - [TRAIN] Epoch: 12/30 | Batch: 90/452 (20.1%) | Loss: 1.2178 | Batch time: 0.03s
2025-03-02 20:38:08,043 - INFO - [TRAIN] Epoch: 12/30 | Batch: 135/452 (30.1%) | Loss: 0.8356 | Batch time: 0.03s
2025-03-02 20:38:09,538 - INFO - [TRAIN] Epoch: 12/30 | Batch: 180/452 (40.0%) | Loss: 0.7862 | Batch time: 0.03s
2025-03-02 20:38:11,047 - INFO - [TRAIN] Epoch: 12/30 | Batch: 225/452 (50.0%) | Loss: 1.1304 | Batch time: 0.03s
2025-03-02 20:38:12,579 - INFO - [TRAIN] Epoch: 12/30 | Batch: 270/452 (60.0%) | Loss: 0.8993 | Batch time: 0.03s
2025-03-02 20:38:14,116 - INFO - [TRAIN] Epoch: 12/30 | Batch: 315/452 (69.9%) | Loss: 0.7420 | Batch time: 0.04s
2025-03-02 20:38:15,711 - INFO - [TRAIN] Epoch: 12/30 | Batch: 360/452 (79.9%) | Loss: 0.9384 | Batch time: 0.03s
2025-03-02 20:38:17,246 - INFO - [TRAIN] Epoch: 12/30 | Batch: 405/452 (89.8%) | Loss: 0.5293 | Batch time: 0.03s
2025-03-02 20:38:18,476 - INFO - [TRAIN] Epoch: 12/30 | Batch: 450/452 (99.8%) | Loss: 0.9787 | Batch time: 0.02s
2025-03-02 20:38:18,492 - INFO - [TRAIN] Epoch: 12/30 | Batch: 451/452 (100.0%) | Loss: 0.8084 | Batch time: 0.02s
2025-03-02 20:38:18,585 - INFO - [VAL] Epoch: 12/30 | Batch: 0/97 (1.0%) | Loss: 0.5003 | Batch time: 0.02s
2025-03-02 20:38:18,760 - INFO - [VAL] Epoch: 12/30 | Batch: 9/97 (10.3%) | Loss: 0.5914 | Batch time: 0.02s
2025-03-02 20:38:18,924 - INFO - [VAL] Epoch: 12/30 | Batch: 18/97 (19.6%) | Loss: 0.4621 | Batch time: 0.02s
2025-03-02 20:38:19,090 - INFO - [VAL] Epoch: 12/30 | Batch: 27/97 (28.9%) | Loss: 0.2869 | Batch time: 0.02s
2025-03-02 20:38:19,260 - INFO - [VAL] Epoch: 12/30 | Batch: 36/97 (38.1%) | Loss: 0.2873 | Batch time: 0.02s
2025-03-02 20:38:19,430 - INFO - [VAL] Epoch: 12/30 | Batch: 45/97 (47.4%) | Loss: 0.3380 | Batch time: 0.02s
2025-03-02 20:38:19,601 - INFO - [VAL] Epoch: 12/30 | Batch: 54/97 (56.7%) | Loss: 0.9025 | Batch time: 0.02s
2025-03-02 20:38:19,773 - INFO - [VAL] Epoch: 12/30 | Batch: 63/97 (66.0%) | Loss: 0.2909 | Batch time: 0.02s
2025-03-02 20:38:19,944 - INFO - [VAL] Epoch: 12/30 | Batch: 72/97 (75.3%) | Loss: 0.4021 | Batch time: 0.02s
2025-03-02 20:38:20,119 - INFO - [VAL] Epoch: 12/30 | Batch: 81/97 (84.5%) | Loss: 0.5101 | Batch time: 0.02s
2025-03-02 20:38:20,289 - INFO - [VAL] Epoch: 12/30 | Batch: 90/97 (93.8%) | Loss: 0.3654 | Batch time: 0.02s
2025-03-02 20:38:20,398 - INFO - [VAL] Epoch: 12/30 | Batch: 96/97 (100.0%) | Loss: 0.6208 | Batch time: 0.01s
2025-03-02 20:38:20,401 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:20,401 - INFO - Epoch 12/30 completed in 17.19s
2025-03-02 20:38:20,401 - INFO - Training   - Loss: 0.8462, Accuracy: 0.7294, F1: 0.7316
2025-03-02 20:38:20,401 - INFO - Validation - Loss: 0.4896, Accuracy: 0.8417, F1: 0.8455
2025-03-02 20:38:20,401 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:20,401 - INFO - Epoch 13/30
2025-03-02 20:38:20,401 - INFO - ----------------------------------------
2025-03-02 20:38:20,643 - INFO - [TRAIN] Epoch: 13/30 | Batch: 0/452 (0.2%) | Loss: 0.7205 | Batch time: 0.04s
2025-03-02 20:38:22,320 - INFO - [TRAIN] Epoch: 13/30 | Batch: 45/452 (10.2%) | Loss: 0.7978 | Batch time: 0.04s
2025-03-02 20:38:23,838 - INFO - [TRAIN] Epoch: 13/30 | Batch: 90/452 (20.1%) | Loss: 1.0293 | Batch time: 0.03s
2025-03-02 20:38:25,393 - INFO - [TRAIN] Epoch: 13/30 | Batch: 135/452 (30.1%) | Loss: 1.2210 | Batch time: 0.03s
2025-03-02 20:38:26,926 - INFO - [TRAIN] Epoch: 13/30 | Batch: 180/452 (40.0%) | Loss: 1.0510 | Batch time: 0.03s
2025-03-02 20:38:28,477 - INFO - [TRAIN] Epoch: 13/30 | Batch: 225/452 (50.0%) | Loss: 0.5273 | Batch time: 0.03s
2025-03-02 20:38:30,024 - INFO - [TRAIN] Epoch: 13/30 | Batch: 270/452 (60.0%) | Loss: 1.0405 | Batch time: 0.03s
2025-03-02 20:38:31,577 - INFO - [TRAIN] Epoch: 13/30 | Batch: 315/452 (69.9%) | Loss: 1.0681 | Batch time: 0.03s
2025-03-02 20:38:33,133 - INFO - [TRAIN] Epoch: 13/30 | Batch: 360/452 (79.9%) | Loss: 0.5546 | Batch time: 0.03s
2025-03-02 20:38:34,701 - INFO - [TRAIN] Epoch: 13/30 | Batch: 405/452 (89.8%) | Loss: 0.8201 | Batch time: 0.04s
2025-03-02 20:38:35,946 - INFO - [TRAIN] Epoch: 13/30 | Batch: 450/452 (99.8%) | Loss: 0.8927 | Batch time: 0.02s
2025-03-02 20:38:35,964 - INFO - [TRAIN] Epoch: 13/30 | Batch: 451/452 (100.0%) | Loss: 0.8311 | Batch time: 0.02s
2025-03-02 20:38:36,060 - INFO - [VAL] Epoch: 13/30 | Batch: 0/97 (1.0%) | Loss: 0.3884 | Batch time: 0.02s
2025-03-02 20:38:36,231 - INFO - [VAL] Epoch: 13/30 | Batch: 9/97 (10.3%) | Loss: 0.5091 | Batch time: 0.02s
2025-03-02 20:38:36,397 - INFO - [VAL] Epoch: 13/30 | Batch: 18/97 (19.6%) | Loss: 0.4392 | Batch time: 0.02s
2025-03-02 20:38:36,563 - INFO - [VAL] Epoch: 13/30 | Batch: 27/97 (28.9%) | Loss: 0.2714 | Batch time: 0.02s
2025-03-02 20:38:36,731 - INFO - [VAL] Epoch: 13/30 | Batch: 36/97 (38.1%) | Loss: 0.2873 | Batch time: 0.02s
2025-03-02 20:38:36,902 - INFO - [VAL] Epoch: 13/30 | Batch: 45/97 (47.4%) | Loss: 0.3294 | Batch time: 0.02s
2025-03-02 20:38:37,074 - INFO - [VAL] Epoch: 13/30 | Batch: 54/97 (56.7%) | Loss: 0.7969 | Batch time: 0.02s
2025-03-02 20:38:37,247 - INFO - [VAL] Epoch: 13/30 | Batch: 63/97 (66.0%) | Loss: 0.2639 | Batch time: 0.02s
2025-03-02 20:38:37,420 - INFO - [VAL] Epoch: 13/30 | Batch: 72/97 (75.3%) | Loss: 0.4188 | Batch time: 0.02s
2025-03-02 20:38:37,592 - INFO - [VAL] Epoch: 13/30 | Batch: 81/97 (84.5%) | Loss: 0.4847 | Batch time: 0.02s
2025-03-02 20:38:37,762 - INFO - [VAL] Epoch: 13/30 | Batch: 90/97 (93.8%) | Loss: 0.3559 | Batch time: 0.02s
2025-03-02 20:38:37,872 - INFO - [VAL] Epoch: 13/30 | Batch: 96/97 (100.0%) | Loss: 0.5392 | Batch time: 0.01s
2025-03-02 20:38:37,875 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:37,875 - INFO - Epoch 13/30 completed in 17.47s
2025-03-02 20:38:37,875 - INFO - Training   - Loss: 0.8412, Accuracy: 0.7295, F1: 0.7319
2025-03-02 20:38:37,875 - INFO - Validation - Loss: 0.4781, Accuracy: 0.8437, F1: 0.8466
2025-03-02 20:38:37,875 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:37,875 - INFO - Epoch 14/30
2025-03-02 20:38:37,875 - INFO - ----------------------------------------
2025-03-02 20:38:38,101 - INFO - [TRAIN] Epoch: 14/30 | Batch: 0/452 (0.2%) | Loss: 0.7096 | Batch time: 0.03s
2025-03-02 20:38:39,839 - INFO - [TRAIN] Epoch: 14/30 | Batch: 45/452 (10.2%) | Loss: 1.1831 | Batch time: 0.03s
2025-03-02 20:38:41,381 - INFO - [TRAIN] Epoch: 14/30 | Batch: 90/452 (20.1%) | Loss: 0.5986 | Batch time: 0.03s
2025-03-02 20:38:42,934 - INFO - [TRAIN] Epoch: 14/30 | Batch: 135/452 (30.1%) | Loss: 0.7717 | Batch time: 0.03s
2025-03-02 20:38:44,493 - INFO - [TRAIN] Epoch: 14/30 | Batch: 180/452 (40.0%) | Loss: 0.9809 | Batch time: 0.03s
2025-03-02 20:38:46,054 - INFO - [TRAIN] Epoch: 14/30 | Batch: 225/452 (50.0%) | Loss: 1.0959 | Batch time: 0.03s
2025-03-02 20:38:47,610 - INFO - [TRAIN] Epoch: 14/30 | Batch: 270/452 (60.0%) | Loss: 0.9671 | Batch time: 0.03s
2025-03-02 20:38:49,171 - INFO - [TRAIN] Epoch: 14/30 | Batch: 315/452 (69.9%) | Loss: 2.2144 | Batch time: 0.03s
2025-03-02 20:38:50,750 - INFO - [TRAIN] Epoch: 14/30 | Batch: 360/452 (79.9%) | Loss: 1.2270 | Batch time: 0.03s
2025-03-02 20:38:52,314 - INFO - [TRAIN] Epoch: 14/30 | Batch: 405/452 (89.8%) | Loss: 0.6136 | Batch time: 0.03s
2025-03-02 20:38:53,560 - INFO - [TRAIN] Epoch: 14/30 | Batch: 450/452 (99.8%) | Loss: 0.7705 | Batch time: 0.02s
2025-03-02 20:38:53,577 - INFO - [TRAIN] Epoch: 14/30 | Batch: 451/452 (100.0%) | Loss: 1.0158 | Batch time: 0.02s
2025-03-02 20:38:53,676 - INFO - [VAL] Epoch: 14/30 | Batch: 0/97 (1.0%) | Loss: 0.4626 | Batch time: 0.02s
2025-03-02 20:38:53,851 - INFO - [VAL] Epoch: 14/30 | Batch: 9/97 (10.3%) | Loss: 0.5543 | Batch time: 0.02s
2025-03-02 20:38:54,015 - INFO - [VAL] Epoch: 14/30 | Batch: 18/97 (19.6%) | Loss: 0.4441 | Batch time: 0.02s
2025-03-02 20:38:54,181 - INFO - [VAL] Epoch: 14/30 | Batch: 27/97 (28.9%) | Loss: 0.3166 | Batch time: 0.02s
2025-03-02 20:38:54,349 - INFO - [VAL] Epoch: 14/30 | Batch: 36/97 (38.1%) | Loss: 0.3183 | Batch time: 0.02s
2025-03-02 20:38:54,519 - INFO - [VAL] Epoch: 14/30 | Batch: 45/97 (47.4%) | Loss: 0.3573 | Batch time: 0.02s
2025-03-02 20:38:54,691 - INFO - [VAL] Epoch: 14/30 | Batch: 54/97 (56.7%) | Loss: 0.8572 | Batch time: 0.02s
2025-03-02 20:38:54,865 - INFO - [VAL] Epoch: 14/30 | Batch: 63/97 (66.0%) | Loss: 0.2517 | Batch time: 0.02s
2025-03-02 20:38:55,037 - INFO - [VAL] Epoch: 14/30 | Batch: 72/97 (75.3%) | Loss: 0.3929 | Batch time: 0.02s
2025-03-02 20:38:55,209 - INFO - [VAL] Epoch: 14/30 | Batch: 81/97 (84.5%) | Loss: 0.4761 | Batch time: 0.02s
2025-03-02 20:38:55,381 - INFO - [VAL] Epoch: 14/30 | Batch: 90/97 (93.8%) | Loss: 0.3844 | Batch time: 0.02s
2025-03-02 20:38:55,491 - INFO - [VAL] Epoch: 14/30 | Batch: 96/97 (100.0%) | Loss: 0.6290 | Batch time: 0.01s
2025-03-02 20:38:55,495 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:55,495 - INFO - Epoch 14/30 completed in 17.62s
2025-03-02 20:38:55,495 - INFO - Training   - Loss: 0.8261, Accuracy: 0.7329, F1: 0.7353
2025-03-02 20:38:55,495 - INFO - Validation - Loss: 0.4946, Accuracy: 0.8437, F1: 0.8467
2025-03-02 20:38:55,495 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:38:55,495 - INFO - Epoch 15/30
2025-03-02 20:38:55,495 - INFO - ----------------------------------------
2025-03-02 20:38:55,701 - INFO - [TRAIN] Epoch: 15/30 | Batch: 0/452 (0.2%) | Loss: 0.9928 | Batch time: 0.03s
2025-03-02 20:38:57,459 - INFO - [TRAIN] Epoch: 15/30 | Batch: 45/452 (10.2%) | Loss: 1.3448 | Batch time: 0.04s
2025-03-02 20:38:58,997 - INFO - [TRAIN] Epoch: 15/30 | Batch: 90/452 (20.1%) | Loss: 0.9518 | Batch time: 0.03s
2025-03-02 20:39:00,539 - INFO - [TRAIN] Epoch: 15/30 | Batch: 135/452 (30.1%) | Loss: 0.7717 | Batch time: 0.03s
2025-03-02 20:39:02,125 - INFO - [TRAIN] Epoch: 15/30 | Batch: 180/452 (40.0%) | Loss: 0.6180 | Batch time: 0.03s
2025-03-02 20:39:03,670 - INFO - [TRAIN] Epoch: 15/30 | Batch: 225/452 (50.0%) | Loss: 1.0218 | Batch time: 0.03s
2025-03-02 20:39:05,254 - INFO - [TRAIN] Epoch: 15/30 | Batch: 270/452 (60.0%) | Loss: 0.6260 | Batch time: 0.03s
2025-03-02 20:39:06,804 - INFO - [TRAIN] Epoch: 15/30 | Batch: 315/452 (69.9%) | Loss: 1.1780 | Batch time: 0.03s
2025-03-02 20:39:08,403 - INFO - [TRAIN] Epoch: 15/30 | Batch: 360/452 (79.9%) | Loss: 1.0749 | Batch time: 0.03s
2025-03-02 20:39:10,024 - INFO - [TRAIN] Epoch: 15/30 | Batch: 405/452 (89.8%) | Loss: 1.0544 | Batch time: 0.04s
2025-03-02 20:39:11,269 - INFO - [TRAIN] Epoch: 15/30 | Batch: 450/452 (99.8%) | Loss: 0.9780 | Batch time: 0.02s
2025-03-02 20:39:11,287 - INFO - [TRAIN] Epoch: 15/30 | Batch: 451/452 (100.0%) | Loss: 1.0357 | Batch time: 0.02s
2025-03-02 20:39:11,385 - INFO - [VAL] Epoch: 15/30 | Batch: 0/97 (1.0%) | Loss: 0.5027 | Batch time: 0.03s
2025-03-02 20:39:11,569 - INFO - [VAL] Epoch: 15/30 | Batch: 9/97 (10.3%) | Loss: 0.6028 | Batch time: 0.02s
2025-03-02 20:39:11,733 - INFO - [VAL] Epoch: 15/30 | Batch: 18/97 (19.6%) | Loss: 0.5029 | Batch time: 0.02s
2025-03-02 20:39:11,898 - INFO - [VAL] Epoch: 15/30 | Batch: 27/97 (28.9%) | Loss: 0.3195 | Batch time: 0.02s
2025-03-02 20:39:12,065 - INFO - [VAL] Epoch: 15/30 | Batch: 36/97 (38.1%) | Loss: 0.3067 | Batch time: 0.02s
2025-03-02 20:39:12,234 - INFO - [VAL] Epoch: 15/30 | Batch: 45/97 (47.4%) | Loss: 0.3444 | Batch time: 0.02s
2025-03-02 20:39:12,413 - INFO - [VAL] Epoch: 15/30 | Batch: 54/97 (56.7%) | Loss: 0.9779 | Batch time: 0.02s
2025-03-02 20:39:12,590 - INFO - [VAL] Epoch: 15/30 | Batch: 63/97 (66.0%) | Loss: 0.2745 | Batch time: 0.02s
2025-03-02 20:39:12,764 - INFO - [VAL] Epoch: 15/30 | Batch: 72/97 (75.3%) | Loss: 0.4165 | Batch time: 0.02s
2025-03-02 20:39:12,937 - INFO - [VAL] Epoch: 15/30 | Batch: 81/97 (84.5%) | Loss: 0.4938 | Batch time: 0.02s
2025-03-02 20:39:13,109 - INFO - [VAL] Epoch: 15/30 | Batch: 90/97 (93.8%) | Loss: 0.3794 | Batch time: 0.02s
2025-03-02 20:39:13,221 - INFO - [VAL] Epoch: 15/30 | Batch: 96/97 (100.0%) | Loss: 0.6316 | Batch time: 0.01s
2025-03-02 20:39:13,224 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:39:13,224 - INFO - Epoch 15/30 completed in 17.73s
2025-03-02 20:39:13,224 - INFO - Training   - Loss: 0.8504, Accuracy: 0.7279, F1: 0.7308
2025-03-02 20:39:13,225 - INFO - Validation - Loss: 0.5220, Accuracy: 0.8317, F1: 0.8367
2025-03-02 20:39:13,225 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:39:13,305 - INFO - Checkpoint saved: checkpoint_epoch_15.pth (Epoch 15)
2025-03-02 20:39:13,306 - INFO - Epoch 16/30
2025-03-02 20:39:13,306 - INFO - ----------------------------------------
2025-03-02 20:39:13,541 - INFO - [TRAIN] Epoch: 16/30 | Batch: 0/452 (0.2%) | Loss: 0.8538 | Batch time: 0.03s
2025-03-02 20:39:15,350 - INFO - [TRAIN] Epoch: 16/30 | Batch: 45/452 (10.2%) | Loss: 0.6353 | Batch time: 0.03s
2025-03-02 20:39:16,810 - INFO - [TRAIN] Epoch: 16/30 | Batch: 90/452 (20.1%) | Loss: 1.3142 | Batch time: 0.03s
2025-03-02 20:39:18,408 - INFO - [TRAIN] Epoch: 16/30 | Batch: 135/452 (30.1%) | Loss: 0.4077 | Batch time: 0.04s
2025-03-02 20:39:19,963 - INFO - [TRAIN] Epoch: 16/30 | Batch: 180/452 (40.0%) | Loss: 1.0171 | Batch time: 0.03s
2025-03-02 20:39:21,494 - INFO - [TRAIN] Epoch: 16/30 | Batch: 225/452 (50.0%) | Loss: 0.8848 | Batch time: 0.04s
2025-03-02 20:39:23,278 - INFO - [TRAIN] Epoch: 16/30 | Batch: 270/452 (60.0%) | Loss: 1.3576 | Batch time: 0.03s
2025-03-02 20:39:24,753 - INFO - [TRAIN] Epoch: 16/30 | Batch: 315/452 (69.9%) | Loss: 0.8639 | Batch time: 0.03s
2025-03-02 20:39:26,259 - INFO - [TRAIN] Epoch: 16/30 | Batch: 360/452 (79.9%) | Loss: 0.7725 | Batch time: 0.03s
2025-03-02 20:39:27,747 - INFO - [TRAIN] Epoch: 16/30 | Batch: 405/452 (89.8%) | Loss: 0.4856 | Batch time: 0.03s
2025-03-02 20:39:28,956 - INFO - [TRAIN] Epoch: 16/30 | Batch: 450/452 (99.8%) | Loss: 0.6040 | Batch time: 0.02s
2025-03-02 20:39:28,974 - INFO - [TRAIN] Epoch: 16/30 | Batch: 451/452 (100.0%) | Loss: 0.4953 | Batch time: 0.02s
2025-03-02 20:39:29,067 - INFO - [VAL] Epoch: 16/30 | Batch: 0/97 (1.0%) | Loss: 0.4384 | Batch time: 0.02s
2025-03-02 20:39:29,240 - INFO - [VAL] Epoch: 16/30 | Batch: 9/97 (10.3%) | Loss: 0.5334 | Batch time: 0.02s
2025-03-02 20:39:29,405 - INFO - [VAL] Epoch: 16/30 | Batch: 18/97 (19.6%) | Loss: 0.4738 | Batch time: 0.02s
2025-03-02 20:39:29,572 - INFO - [VAL] Epoch: 16/30 | Batch: 27/97 (28.9%) | Loss: 0.2947 | Batch time: 0.02s
2025-03-02 20:39:29,741 - INFO - [VAL] Epoch: 16/30 | Batch: 36/97 (38.1%) | Loss: 0.2672 | Batch time: 0.02s
2025-03-02 20:39:29,912 - INFO - [VAL] Epoch: 16/30 | Batch: 45/97 (47.4%) | Loss: 0.3113 | Batch time: 0.02s
2025-03-02 20:39:30,084 - INFO - [VAL] Epoch: 16/30 | Batch: 54/97 (56.7%) | Loss: 0.8294 | Batch time: 0.02s
2025-03-02 20:39:30,256 - INFO - [VAL] Epoch: 16/30 | Batch: 63/97 (66.0%) | Loss: 0.2473 | Batch time: 0.02s
2025-03-02 20:39:30,428 - INFO - [VAL] Epoch: 16/30 | Batch: 72/97 (75.3%) | Loss: 0.3780 | Batch time: 0.02s
2025-03-02 20:39:30,600 - INFO - [VAL] Epoch: 16/30 | Batch: 81/97 (84.5%) | Loss: 0.4573 | Batch time: 0.02s
2025-03-02 20:39:30,772 - INFO - [VAL] Epoch: 16/30 | Batch: 90/97 (93.8%) | Loss: 0.3707 | Batch time: 0.02s
2025-03-02 20:39:30,882 - INFO - [VAL] Epoch: 16/30 | Batch: 96/97 (100.0%) | Loss: 0.4817 | Batch time: 0.01s
2025-03-02 20:39:31,015 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 16)
2025-03-02 20:39:31,015 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:39:31,015 - INFO - Epoch 16/30 completed in 17.71s
2025-03-02 20:39:31,016 - INFO - Training   - Loss: 0.8305, Accuracy: 0.7327, F1: 0.7353
2025-03-02 20:39:31,016 - INFO - Validation - Loss: 0.4691, Accuracy: 0.8547, F1: 0.8580
2025-03-02 20:39:31,016 - INFO - Validation F1 improved from 0.8514 to 0.8580
2025-03-02 20:39:31,016 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:39:31,016 - INFO - Epoch 17/30
2025-03-02 20:39:31,016 - INFO - ----------------------------------------
2025-03-02 20:39:31,230 - INFO - [TRAIN] Epoch: 17/30 | Batch: 0/452 (0.2%) | Loss: 1.2954 | Batch time: 0.04s
2025-03-02 20:39:32,862 - INFO - [TRAIN] Epoch: 17/30 | Batch: 45/452 (10.2%) | Loss: 0.8091 | Batch time: 0.03s
2025-03-02 20:39:34,426 - INFO - [TRAIN] Epoch: 17/30 | Batch: 90/452 (20.1%) | Loss: 0.5243 | Batch time: 0.03s
2025-03-02 20:39:35,926 - INFO - [TRAIN] Epoch: 17/30 | Batch: 135/452 (30.1%) | Loss: 1.2307 | Batch time: 0.03s
2025-03-02 20:39:37,444 - INFO - [TRAIN] Epoch: 17/30 | Batch: 180/452 (40.0%) | Loss: 0.3285 | Batch time: 0.03s
2025-03-02 20:39:38,935 - INFO - [TRAIN] Epoch: 17/30 | Batch: 225/452 (50.0%) | Loss: 0.8545 | Batch time: 0.03s
2025-03-02 20:39:40,458 - INFO - [TRAIN] Epoch: 17/30 | Batch: 270/452 (60.0%) | Loss: 0.9049 | Batch time: 0.03s
2025-03-02 20:39:41,964 - INFO - [TRAIN] Epoch: 17/30 | Batch: 315/452 (69.9%) | Loss: 0.6082 | Batch time: 0.03s
2025-03-02 20:39:43,479 - INFO - [TRAIN] Epoch: 17/30 | Batch: 360/452 (79.9%) | Loss: 0.5439 | Batch time: 0.04s
2025-03-02 20:39:45,015 - INFO - [TRAIN] Epoch: 17/30 | Batch: 405/452 (89.8%) | Loss: 1.3507 | Batch time: 0.04s
2025-03-02 20:39:46,234 - INFO - [TRAIN] Epoch: 17/30 | Batch: 450/452 (99.8%) | Loss: 0.5921 | Batch time: 0.02s
2025-03-02 20:39:46,251 - INFO - [TRAIN] Epoch: 17/30 | Batch: 451/452 (100.0%) | Loss: 0.6949 | Batch time: 0.02s
2025-03-02 20:39:46,342 - INFO - [VAL] Epoch: 17/30 | Batch: 0/97 (1.0%) | Loss: 0.4379 | Batch time: 0.02s
2025-03-02 20:39:46,520 - INFO - [VAL] Epoch: 17/30 | Batch: 9/97 (10.3%) | Loss: 0.5542 | Batch time: 0.02s
2025-03-02 20:39:46,689 - INFO - [VAL] Epoch: 17/30 | Batch: 18/97 (19.6%) | Loss: 0.4277 | Batch time: 0.02s
2025-03-02 20:39:46,854 - INFO - [VAL] Epoch: 17/30 | Batch: 27/97 (28.9%) | Loss: 0.2929 | Batch time: 0.02s
2025-03-02 20:39:47,022 - INFO - [VAL] Epoch: 17/30 | Batch: 36/97 (38.1%) | Loss: 0.2611 | Batch time: 0.02s
2025-03-02 20:39:47,190 - INFO - [VAL] Epoch: 17/30 | Batch: 45/97 (47.4%) | Loss: 0.3273 | Batch time: 0.02s
2025-03-02 20:39:47,359 - INFO - [VAL] Epoch: 17/30 | Batch: 54/97 (56.7%) | Loss: 0.8590 | Batch time: 0.02s
2025-03-02 20:39:47,530 - INFO - [VAL] Epoch: 17/30 | Batch: 63/97 (66.0%) | Loss: 0.2562 | Batch time: 0.02s
2025-03-02 20:39:47,703 - INFO - [VAL] Epoch: 17/30 | Batch: 72/97 (75.3%) | Loss: 0.3994 | Batch time: 0.02s
2025-03-02 20:39:47,873 - INFO - [VAL] Epoch: 17/30 | Batch: 81/97 (84.5%) | Loss: 0.4742 | Batch time: 0.02s
2025-03-02 20:39:48,044 - INFO - [VAL] Epoch: 17/30 | Batch: 90/97 (93.8%) | Loss: 0.3387 | Batch time: 0.02s
2025-03-02 20:39:48,153 - INFO - [VAL] Epoch: 17/30 | Batch: 96/97 (100.0%) | Loss: 0.5027 | Batch time: 0.01s
2025-03-02 20:39:48,156 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:39:48,156 - INFO - Epoch 17/30 completed in 17.14s
2025-03-02 20:39:48,156 - INFO - Training   - Loss: 0.8486, Accuracy: 0.7304, F1: 0.7328
2025-03-02 20:39:48,156 - INFO - Validation - Loss: 0.4670, Accuracy: 0.8495, F1: 0.8526
2025-03-02 20:39:48,156 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:39:48,156 - INFO - Epoch 18/30
2025-03-02 20:39:48,156 - INFO - ----------------------------------------
2025-03-02 20:39:48,371 - INFO - [TRAIN] Epoch: 18/30 | Batch: 0/452 (0.2%) | Loss: 0.5793 | Batch time: 0.04s
2025-03-02 20:39:49,997 - INFO - [TRAIN] Epoch: 18/30 | Batch: 45/452 (10.2%) | Loss: 0.8478 | Batch time: 0.03s
2025-03-02 20:39:51,504 - INFO - [TRAIN] Epoch: 18/30 | Batch: 90/452 (20.1%) | Loss: 0.8544 | Batch time: 0.03s
2025-03-02 20:39:52,978 - INFO - [TRAIN] Epoch: 18/30 | Batch: 135/452 (30.1%) | Loss: 1.0855 | Batch time: 0.03s
2025-03-02 20:39:54,541 - INFO - [TRAIN] Epoch: 18/30 | Batch: 180/452 (40.0%) | Loss: 0.9729 | Batch time: 0.03s
2025-03-02 20:39:56,490 - INFO - [TRAIN] Epoch: 18/30 | Batch: 225/452 (50.0%) | Loss: 0.8334 | Batch time: 0.04s
2025-03-02 20:39:57,959 - INFO - [TRAIN] Epoch: 18/30 | Batch: 270/452 (60.0%) | Loss: 0.6614 | Batch time: 0.03s
2025-03-02 20:39:59,388 - INFO - [TRAIN] Epoch: 18/30 | Batch: 315/452 (69.9%) | Loss: 1.0895 | Batch time: 0.03s
2025-03-02 20:40:00,857 - INFO - [TRAIN] Epoch: 18/30 | Batch: 360/452 (79.9%) | Loss: 0.6937 | Batch time: 0.03s
2025-03-02 20:40:02,481 - INFO - [TRAIN] Epoch: 18/30 | Batch: 405/452 (89.8%) | Loss: 0.6144 | Batch time: 0.04s
2025-03-02 20:40:03,700 - INFO - [TRAIN] Epoch: 18/30 | Batch: 450/452 (99.8%) | Loss: 0.5470 | Batch time: 0.02s
2025-03-02 20:40:03,718 - INFO - [TRAIN] Epoch: 18/30 | Batch: 451/452 (100.0%) | Loss: 1.1588 | Batch time: 0.02s
2025-03-02 20:40:03,812 - INFO - [VAL] Epoch: 18/30 | Batch: 0/97 (1.0%) | Loss: 0.4169 | Batch time: 0.02s
2025-03-02 20:40:03,981 - INFO - [VAL] Epoch: 18/30 | Batch: 9/97 (10.3%) | Loss: 0.5298 | Batch time: 0.02s
2025-03-02 20:40:04,146 - INFO - [VAL] Epoch: 18/30 | Batch: 18/97 (19.6%) | Loss: 0.4834 | Batch time: 0.02s
2025-03-02 20:40:04,312 - INFO - [VAL] Epoch: 18/30 | Batch: 27/97 (28.9%) | Loss: 0.2953 | Batch time: 0.02s
2025-03-02 20:40:04,480 - INFO - [VAL] Epoch: 18/30 | Batch: 36/97 (38.1%) | Loss: 0.2918 | Batch time: 0.02s
2025-03-02 20:40:04,648 - INFO - [VAL] Epoch: 18/30 | Batch: 45/97 (47.4%) | Loss: 0.3385 | Batch time: 0.02s
2025-03-02 20:40:04,820 - INFO - [VAL] Epoch: 18/30 | Batch: 54/97 (56.7%) | Loss: 0.7544 | Batch time: 0.02s
2025-03-02 20:40:04,992 - INFO - [VAL] Epoch: 18/30 | Batch: 63/97 (66.0%) | Loss: 0.2357 | Batch time: 0.02s
2025-03-02 20:40:05,173 - INFO - [VAL] Epoch: 18/30 | Batch: 72/97 (75.3%) | Loss: 0.3766 | Batch time: 0.02s
2025-03-02 20:40:05,344 - INFO - [VAL] Epoch: 18/30 | Batch: 81/97 (84.5%) | Loss: 0.4508 | Batch time: 0.02s
2025-03-02 20:40:05,515 - INFO - [VAL] Epoch: 18/30 | Batch: 90/97 (93.8%) | Loss: 0.3713 | Batch time: 0.02s
2025-03-02 20:40:05,624 - INFO - [VAL] Epoch: 18/30 | Batch: 96/97 (100.0%) | Loss: 0.5162 | Batch time: 0.01s
2025-03-02 20:40:05,627 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:05,627 - INFO - Epoch 18/30 completed in 17.47s
2025-03-02 20:40:05,627 - INFO - Training   - Loss: 0.8398, Accuracy: 0.7289, F1: 0.7317
2025-03-02 20:40:05,627 - INFO - Validation - Loss: 0.4774, Accuracy: 0.8459, F1: 0.8483
2025-03-02 20:40:05,627 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:05,627 - INFO - Epoch 19/30
2025-03-02 20:40:05,627 - INFO - ----------------------------------------
2025-03-02 20:40:05,833 - INFO - [TRAIN] Epoch: 19/30 | Batch: 0/452 (0.2%) | Loss: 1.0240 | Batch time: 0.03s
2025-03-02 20:40:07,459 - INFO - [TRAIN] Epoch: 19/30 | Batch: 45/452 (10.2%) | Loss: 1.0603 | Batch time: 0.03s
2025-03-02 20:40:08,943 - INFO - [TRAIN] Epoch: 19/30 | Batch: 90/452 (20.1%) | Loss: 0.5649 | Batch time: 0.03s
2025-03-02 20:40:10,406 - INFO - [TRAIN] Epoch: 19/30 | Batch: 135/452 (30.1%) | Loss: 0.5179 | Batch time: 0.03s
2025-03-02 20:40:11,904 - INFO - [TRAIN] Epoch: 19/30 | Batch: 180/452 (40.0%) | Loss: 0.7335 | Batch time: 0.03s
2025-03-02 20:40:13,386 - INFO - [TRAIN] Epoch: 19/30 | Batch: 225/452 (50.0%) | Loss: 0.8794 | Batch time: 0.03s
2025-03-02 20:40:14,952 - INFO - [TRAIN] Epoch: 19/30 | Batch: 270/452 (60.0%) | Loss: 0.6825 | Batch time: 0.03s
2025-03-02 20:40:16,441 - INFO - [TRAIN] Epoch: 19/30 | Batch: 315/452 (69.9%) | Loss: 0.9688 | Batch time: 0.03s
2025-03-02 20:40:17,943 - INFO - [TRAIN] Epoch: 19/30 | Batch: 360/452 (79.9%) | Loss: 0.8730 | Batch time: 0.03s
2025-03-02 20:40:19,448 - INFO - [TRAIN] Epoch: 19/30 | Batch: 405/452 (89.8%) | Loss: 0.4388 | Batch time: 0.03s
2025-03-02 20:40:20,662 - INFO - [TRAIN] Epoch: 19/30 | Batch: 450/452 (99.8%) | Loss: 0.9079 | Batch time: 0.02s
2025-03-02 20:40:20,680 - INFO - [TRAIN] Epoch: 19/30 | Batch: 451/452 (100.0%) | Loss: 0.4533 | Batch time: 0.02s
2025-03-02 20:40:20,771 - INFO - [VAL] Epoch: 19/30 | Batch: 0/97 (1.0%) | Loss: 0.4677 | Batch time: 0.02s
2025-03-02 20:40:20,944 - INFO - [VAL] Epoch: 19/30 | Batch: 9/97 (10.3%) | Loss: 0.6172 | Batch time: 0.02s
2025-03-02 20:40:21,108 - INFO - [VAL] Epoch: 19/30 | Batch: 18/97 (19.6%) | Loss: 0.4627 | Batch time: 0.02s
2025-03-02 20:40:21,273 - INFO - [VAL] Epoch: 19/30 | Batch: 27/97 (28.9%) | Loss: 0.2801 | Batch time: 0.02s
2025-03-02 20:40:21,439 - INFO - [VAL] Epoch: 19/30 | Batch: 36/97 (38.1%) | Loss: 0.3142 | Batch time: 0.02s
2025-03-02 20:40:21,608 - INFO - [VAL] Epoch: 19/30 | Batch: 45/97 (47.4%) | Loss: 0.3395 | Batch time: 0.02s
2025-03-02 20:40:21,778 - INFO - [VAL] Epoch: 19/30 | Batch: 54/97 (56.7%) | Loss: 0.9414 | Batch time: 0.02s
2025-03-02 20:40:21,948 - INFO - [VAL] Epoch: 19/30 | Batch: 63/97 (66.0%) | Loss: 0.2652 | Batch time: 0.02s
2025-03-02 20:40:22,118 - INFO - [VAL] Epoch: 19/30 | Batch: 72/97 (75.3%) | Loss: 0.4095 | Batch time: 0.02s
2025-03-02 20:40:22,287 - INFO - [VAL] Epoch: 19/30 | Batch: 81/97 (84.5%) | Loss: 0.4827 | Batch time: 0.02s
2025-03-02 20:40:22,455 - INFO - [VAL] Epoch: 19/30 | Batch: 90/97 (93.8%) | Loss: 0.3622 | Batch time: 0.02s
2025-03-02 20:40:22,563 - INFO - [VAL] Epoch: 19/30 | Batch: 96/97 (100.0%) | Loss: 0.5698 | Batch time: 0.01s
2025-03-02 20:40:22,566 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:22,566 - INFO - Epoch 19/30 completed in 16.94s
2025-03-02 20:40:22,566 - INFO - Training   - Loss: 0.8538, Accuracy: 0.7297, F1: 0.7323
2025-03-02 20:40:22,566 - INFO - Validation - Loss: 0.4896, Accuracy: 0.8421, F1: 0.8462
2025-03-02 20:40:22,566 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:22,566 - INFO - Epoch 20/30
2025-03-02 20:40:22,566 - INFO - ----------------------------------------
2025-03-02 20:40:22,782 - INFO - [TRAIN] Epoch: 20/30 | Batch: 0/452 (0.2%) | Loss: 0.4518 | Batch time: 0.04s
2025-03-02 20:40:24,436 - INFO - [TRAIN] Epoch: 20/30 | Batch: 45/452 (10.2%) | Loss: 0.9394 | Batch time: 0.03s
2025-03-02 20:40:25,961 - INFO - [TRAIN] Epoch: 20/30 | Batch: 90/452 (20.1%) | Loss: 0.8016 | Batch time: 0.03s
2025-03-02 20:40:27,430 - INFO - [TRAIN] Epoch: 20/30 | Batch: 135/452 (30.1%) | Loss: 1.0996 | Batch time: 0.03s
2025-03-02 20:40:28,934 - INFO - [TRAIN] Epoch: 20/30 | Batch: 180/452 (40.0%) | Loss: 1.2320 | Batch time: 0.03s
2025-03-02 20:40:30,423 - INFO - [TRAIN] Epoch: 20/30 | Batch: 225/452 (50.0%) | Loss: 0.5808 | Batch time: 0.03s
2025-03-02 20:40:31,898 - INFO - [TRAIN] Epoch: 20/30 | Batch: 270/452 (60.0%) | Loss: 0.7555 | Batch time: 0.03s
2025-03-02 20:40:33,385 - INFO - [TRAIN] Epoch: 20/30 | Batch: 315/452 (69.9%) | Loss: 0.9964 | Batch time: 0.03s
2025-03-02 20:40:34,873 - INFO - [TRAIN] Epoch: 20/30 | Batch: 360/452 (79.9%) | Loss: 0.8262 | Batch time: 0.03s
2025-03-02 20:40:36,363 - INFO - [TRAIN] Epoch: 20/30 | Batch: 405/452 (89.8%) | Loss: 0.8595 | Batch time: 0.03s
2025-03-02 20:40:37,571 - INFO - [TRAIN] Epoch: 20/30 | Batch: 450/452 (99.8%) | Loss: 0.7618 | Batch time: 0.02s
2025-03-02 20:40:37,587 - INFO - [TRAIN] Epoch: 20/30 | Batch: 451/452 (100.0%) | Loss: 1.0059 | Batch time: 0.02s
2025-03-02 20:40:37,682 - INFO - [VAL] Epoch: 20/30 | Batch: 0/97 (1.0%) | Loss: 0.3899 | Batch time: 0.02s
2025-03-02 20:40:37,851 - INFO - [VAL] Epoch: 20/30 | Batch: 9/97 (10.3%) | Loss: 0.4972 | Batch time: 0.02s
2025-03-02 20:40:38,013 - INFO - [VAL] Epoch: 20/30 | Batch: 18/97 (19.6%) | Loss: 0.4434 | Batch time: 0.02s
2025-03-02 20:40:38,177 - INFO - [VAL] Epoch: 20/30 | Batch: 27/97 (28.9%) | Loss: 0.2873 | Batch time: 0.02s
2025-03-02 20:40:38,342 - INFO - [VAL] Epoch: 20/30 | Batch: 36/97 (38.1%) | Loss: 0.3104 | Batch time: 0.02s
2025-03-02 20:40:38,510 - INFO - [VAL] Epoch: 20/30 | Batch: 45/97 (47.4%) | Loss: 0.3515 | Batch time: 0.02s
2025-03-02 20:40:38,679 - INFO - [VAL] Epoch: 20/30 | Batch: 54/97 (56.7%) | Loss: 0.7759 | Batch time: 0.02s
2025-03-02 20:40:38,848 - INFO - [VAL] Epoch: 20/30 | Batch: 63/97 (66.0%) | Loss: 0.2427 | Batch time: 0.02s
2025-03-02 20:40:39,017 - INFO - [VAL] Epoch: 20/30 | Batch: 72/97 (75.3%) | Loss: 0.3817 | Batch time: 0.02s
2025-03-02 20:40:39,185 - INFO - [VAL] Epoch: 20/30 | Batch: 81/97 (84.5%) | Loss: 0.4657 | Batch time: 0.02s
2025-03-02 20:40:39,352 - INFO - [VAL] Epoch: 20/30 | Batch: 90/97 (93.8%) | Loss: 0.3636 | Batch time: 0.02s
2025-03-02 20:40:39,460 - INFO - [VAL] Epoch: 20/30 | Batch: 96/97 (100.0%) | Loss: 0.5341 | Batch time: 0.01s
2025-03-02 20:40:39,463 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:39,463 - INFO - Epoch 20/30 completed in 16.90s
2025-03-02 20:40:39,463 - INFO - Training   - Loss: 0.8487, Accuracy: 0.7303, F1: 0.7335
2025-03-02 20:40:39,463 - INFO - Validation - Loss: 0.4802, Accuracy: 0.8453, F1: 0.8478
2025-03-02 20:40:39,463 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:39,537 - INFO - Checkpoint saved: checkpoint_epoch_20.pth (Epoch 20)
2025-03-02 20:40:39,537 - INFO - Epoch 21/30
2025-03-02 20:40:39,537 - INFO - ----------------------------------------
2025-03-02 20:40:39,782 - INFO - [TRAIN] Epoch: 21/30 | Batch: 0/452 (0.2%) | Loss: 0.8771 | Batch time: 0.04s
2025-03-02 20:40:41,469 - INFO - [TRAIN] Epoch: 21/30 | Batch: 45/452 (10.2%) | Loss: 0.7138 | Batch time: 0.03s
2025-03-02 20:40:43,223 - INFO - [TRAIN] Epoch: 21/30 | Batch: 90/452 (20.1%) | Loss: 1.0102 | Batch time: 0.05s
2025-03-02 20:40:45,161 - INFO - [TRAIN] Epoch: 21/30 | Batch: 135/452 (30.1%) | Loss: 0.9797 | Batch time: 0.04s
2025-03-02 20:40:46,545 - INFO - [TRAIN] Epoch: 21/30 | Batch: 180/452 (40.0%) | Loss: 0.6129 | Batch time: 0.03s
2025-03-02 20:40:48,280 - INFO - [TRAIN] Epoch: 21/30 | Batch: 225/452 (50.0%) | Loss: 1.2750 | Batch time: 0.04s
2025-03-02 20:40:49,841 - INFO - [TRAIN] Epoch: 21/30 | Batch: 270/452 (60.0%) | Loss: 0.4390 | Batch time: 0.03s
2025-03-02 20:40:51,267 - INFO - [TRAIN] Epoch: 21/30 | Batch: 315/452 (69.9%) | Loss: 0.9075 | Batch time: 0.03s
2025-03-02 20:40:52,704 - INFO - [TRAIN] Epoch: 21/30 | Batch: 360/452 (79.9%) | Loss: 0.7994 | Batch time: 0.03s
2025-03-02 20:40:54,117 - INFO - [TRAIN] Epoch: 21/30 | Batch: 405/452 (89.8%) | Loss: 0.5912 | Batch time: 0.03s
2025-03-02 20:40:55,361 - INFO - [TRAIN] Epoch: 21/30 | Batch: 450/452 (99.8%) | Loss: 0.7826 | Batch time: 0.02s
2025-03-02 20:40:55,377 - INFO - [TRAIN] Epoch: 21/30 | Batch: 451/452 (100.0%) | Loss: 1.4114 | Batch time: 0.02s
2025-03-02 20:40:55,465 - INFO - [VAL] Epoch: 21/30 | Batch: 0/97 (1.0%) | Loss: 0.4628 | Batch time: 0.02s
2025-03-02 20:40:55,648 - INFO - [VAL] Epoch: 21/30 | Batch: 9/97 (10.3%) | Loss: 0.5767 | Batch time: 0.02s
2025-03-02 20:40:55,826 - INFO - [VAL] Epoch: 21/30 | Batch: 18/97 (19.6%) | Loss: 0.4634 | Batch time: 0.02s
2025-03-02 20:40:56,006 - INFO - [VAL] Epoch: 21/30 | Batch: 27/97 (28.9%) | Loss: 0.2904 | Batch time: 0.02s
2025-03-02 20:40:56,183 - INFO - [VAL] Epoch: 21/30 | Batch: 36/97 (38.1%) | Loss: 0.2873 | Batch time: 0.02s
2025-03-02 20:40:56,362 - INFO - [VAL] Epoch: 21/30 | Batch: 45/97 (47.4%) | Loss: 0.3467 | Batch time: 0.02s
2025-03-02 20:40:56,545 - INFO - [VAL] Epoch: 21/30 | Batch: 54/97 (56.7%) | Loss: 0.8677 | Batch time: 0.02s
2025-03-02 20:40:56,727 - INFO - [VAL] Epoch: 21/30 | Batch: 63/97 (66.0%) | Loss: 0.2748 | Batch time: 0.02s
2025-03-02 20:40:56,907 - INFO - [VAL] Epoch: 21/30 | Batch: 72/97 (75.3%) | Loss: 0.4225 | Batch time: 0.02s
2025-03-02 20:40:57,091 - INFO - [VAL] Epoch: 21/30 | Batch: 81/97 (84.5%) | Loss: 0.4746 | Batch time: 0.02s
2025-03-02 20:40:57,268 - INFO - [VAL] Epoch: 21/30 | Batch: 90/97 (93.8%) | Loss: 0.3804 | Batch time: 0.02s
2025-03-02 20:40:57,381 - INFO - [VAL] Epoch: 21/30 | Batch: 96/97 (100.0%) | Loss: 0.6105 | Batch time: 0.02s
2025-03-02 20:40:57,385 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:57,385 - INFO - Epoch 21/30 completed in 17.85s
2025-03-02 20:40:57,385 - INFO - Training   - Loss: 0.8452, Accuracy: 0.7296, F1: 0.7323
2025-03-02 20:40:57,385 - INFO - Validation - Loss: 0.4966, Accuracy: 0.8427, F1: 0.8470
2025-03-02 20:40:57,385 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:40:57,385 - INFO - Epoch 22/30
2025-03-02 20:40:57,385 - INFO - ----------------------------------------
2025-03-02 20:40:57,599 - INFO - [TRAIN] Epoch: 22/30 | Batch: 0/452 (0.2%) | Loss: 0.9853 | Batch time: 0.03s
2025-03-02 20:40:59,227 - INFO - [TRAIN] Epoch: 22/30 | Batch: 45/452 (10.2%) | Loss: 1.0626 | Batch time: 0.03s
2025-03-02 20:41:00,756 - INFO - [TRAIN] Epoch: 22/30 | Batch: 90/452 (20.1%) | Loss: 1.0950 | Batch time: 0.03s
2025-03-02 20:41:02,248 - INFO - [TRAIN] Epoch: 22/30 | Batch: 135/452 (30.1%) | Loss: 1.1768 | Batch time: 0.03s
2025-03-02 20:41:03,764 - INFO - [TRAIN] Epoch: 22/30 | Batch: 180/452 (40.0%) | Loss: 1.0693 | Batch time: 0.03s
2025-03-02 20:41:05,282 - INFO - [TRAIN] Epoch: 22/30 | Batch: 225/452 (50.0%) | Loss: 0.7522 | Batch time: 0.03s
2025-03-02 20:41:06,828 - INFO - [TRAIN] Epoch: 22/30 | Batch: 270/452 (60.0%) | Loss: 0.7534 | Batch time: 0.03s
2025-03-02 20:41:08,348 - INFO - [TRAIN] Epoch: 22/30 | Batch: 315/452 (69.9%) | Loss: 1.0041 | Batch time: 0.03s
2025-03-02 20:41:09,921 - INFO - [TRAIN] Epoch: 22/30 | Batch: 360/452 (79.9%) | Loss: 1.3073 | Batch time: 0.04s
2025-03-02 20:41:11,468 - INFO - [TRAIN] Epoch: 22/30 | Batch: 405/452 (89.8%) | Loss: 0.8575 | Batch time: 0.04s
2025-03-02 20:41:12,715 - INFO - [TRAIN] Epoch: 22/30 | Batch: 450/452 (99.8%) | Loss: 0.6706 | Batch time: 0.02s
2025-03-02 20:41:12,732 - INFO - [TRAIN] Epoch: 22/30 | Batch: 451/452 (100.0%) | Loss: 0.6732 | Batch time: 0.02s
2025-03-02 20:41:12,825 - INFO - [VAL] Epoch: 22/30 | Batch: 0/97 (1.0%) | Loss: 0.5561 | Batch time: 0.02s
2025-03-02 20:41:13,009 - INFO - [VAL] Epoch: 22/30 | Batch: 9/97 (10.3%) | Loss: 0.7059 | Batch time: 0.02s
2025-03-02 20:41:13,183 - INFO - [VAL] Epoch: 22/30 | Batch: 18/97 (19.6%) | Loss: 0.4752 | Batch time: 0.02s
2025-03-02 20:41:13,359 - INFO - [VAL] Epoch: 22/30 | Batch: 27/97 (28.9%) | Loss: 0.3055 | Batch time: 0.02s
2025-03-02 20:41:13,537 - INFO - [VAL] Epoch: 22/30 | Batch: 36/97 (38.1%) | Loss: 0.3193 | Batch time: 0.02s
2025-03-02 20:41:13,715 - INFO - [VAL] Epoch: 22/30 | Batch: 45/97 (47.4%) | Loss: 0.3161 | Batch time: 0.02s
2025-03-02 20:41:13,896 - INFO - [VAL] Epoch: 22/30 | Batch: 54/97 (56.7%) | Loss: 0.9751 | Batch time: 0.02s
2025-03-02 20:41:14,079 - INFO - [VAL] Epoch: 22/30 | Batch: 63/97 (66.0%) | Loss: 0.2845 | Batch time: 0.02s
2025-03-02 20:41:14,259 - INFO - [VAL] Epoch: 22/30 | Batch: 72/97 (75.3%) | Loss: 0.4134 | Batch time: 0.02s
2025-03-02 20:41:14,437 - INFO - [VAL] Epoch: 22/30 | Batch: 81/97 (84.5%) | Loss: 0.4831 | Batch time: 0.02s
2025-03-02 20:41:14,618 - INFO - [VAL] Epoch: 22/30 | Batch: 90/97 (93.8%) | Loss: 0.3694 | Batch time: 0.02s
2025-03-02 20:41:14,736 - INFO - [VAL] Epoch: 22/30 | Batch: 96/97 (100.0%) | Loss: 0.7950 | Batch time: 0.02s
2025-03-02 20:41:14,740 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:41:14,740 - INFO - Epoch 22/30 completed in 17.35s
2025-03-02 20:41:14,740 - INFO - Training   - Loss: 0.8335, Accuracy: 0.7380, F1: 0.7410
2025-03-02 20:41:14,740 - INFO - Validation - Loss: 0.5135, Accuracy: 0.8295, F1: 0.8345
2025-03-02 20:41:14,740 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:41:14,740 - INFO - Epoch 23/30
2025-03-02 20:41:14,740 - INFO - ----------------------------------------
2025-03-02 20:41:14,981 - INFO - [TRAIN] Epoch: 23/30 | Batch: 0/452 (0.2%) | Loss: 0.7001 | Batch time: 0.05s
2025-03-02 20:41:16,764 - INFO - [TRAIN] Epoch: 23/30 | Batch: 45/452 (10.2%) | Loss: 0.8003 | Batch time: 0.03s
2025-03-02 20:41:18,347 - INFO - [TRAIN] Epoch: 23/30 | Batch: 90/452 (20.1%) | Loss: 1.1001 | Batch time: 0.03s
2025-03-02 20:41:19,962 - INFO - [TRAIN] Epoch: 23/30 | Batch: 135/452 (30.1%) | Loss: 1.2023 | Batch time: 0.04s
2025-03-02 20:41:21,542 - INFO - [TRAIN] Epoch: 23/30 | Batch: 180/452 (40.0%) | Loss: 0.6745 | Batch time: 0.04s
2025-03-02 20:41:23,178 - INFO - [TRAIN] Epoch: 23/30 | Batch: 225/452 (50.0%) | Loss: 0.6439 | Batch time: 0.05s
2025-03-02 20:41:24,774 - INFO - [TRAIN] Epoch: 23/30 | Batch: 270/452 (60.0%) | Loss: 1.1211 | Batch time: 0.03s
2025-03-02 20:41:26,377 - INFO - [TRAIN] Epoch: 23/30 | Batch: 315/452 (69.9%) | Loss: 1.0162 | Batch time: 0.03s
2025-03-02 20:41:27,977 - INFO - [TRAIN] Epoch: 23/30 | Batch: 360/452 (79.9%) | Loss: 0.8143 | Batch time: 0.04s
2025-03-02 20:41:29,576 - INFO - [TRAIN] Epoch: 23/30 | Batch: 405/452 (89.8%) | Loss: 0.9634 | Batch time: 0.03s
2025-03-02 20:41:30,898 - INFO - [TRAIN] Epoch: 23/30 | Batch: 450/452 (99.8%) | Loss: 1.3138 | Batch time: 0.02s
2025-03-02 20:41:30,915 - INFO - [TRAIN] Epoch: 23/30 | Batch: 451/452 (100.0%) | Loss: 1.0049 | Batch time: 0.02s
2025-03-02 20:41:31,015 - INFO - [VAL] Epoch: 23/30 | Batch: 0/97 (1.0%) | Loss: 0.4504 | Batch time: 0.02s
2025-03-02 20:41:31,199 - INFO - [VAL] Epoch: 23/30 | Batch: 9/97 (10.3%) | Loss: 0.5726 | Batch time: 0.02s
2025-03-02 20:41:31,370 - INFO - [VAL] Epoch: 23/30 | Batch: 18/97 (19.6%) | Loss: 0.4554 | Batch time: 0.02s
2025-03-02 20:41:31,543 - INFO - [VAL] Epoch: 23/30 | Batch: 27/97 (28.9%) | Loss: 0.3035 | Batch time: 0.02s
2025-03-02 20:41:31,717 - INFO - [VAL] Epoch: 23/30 | Batch: 36/97 (38.1%) | Loss: 0.2837 | Batch time: 0.02s
2025-03-02 20:41:31,894 - INFO - [VAL] Epoch: 23/30 | Batch: 45/97 (47.4%) | Loss: 0.3349 | Batch time: 0.02s
2025-03-02 20:41:32,072 - INFO - [VAL] Epoch: 23/30 | Batch: 54/97 (56.7%) | Loss: 0.8659 | Batch time: 0.02s
2025-03-02 20:41:32,252 - INFO - [VAL] Epoch: 23/30 | Batch: 63/97 (66.0%) | Loss: 0.2588 | Batch time: 0.02s
2025-03-02 20:41:32,432 - INFO - [VAL] Epoch: 23/30 | Batch: 72/97 (75.3%) | Loss: 0.3911 | Batch time: 0.02s
2025-03-02 20:41:32,607 - INFO - [VAL] Epoch: 23/30 | Batch: 81/97 (84.5%) | Loss: 0.4613 | Batch time: 0.02s
2025-03-02 20:41:32,782 - INFO - [VAL] Epoch: 23/30 | Batch: 90/97 (93.8%) | Loss: 0.3608 | Batch time: 0.02s
2025-03-02 20:41:32,896 - INFO - [VAL] Epoch: 23/30 | Batch: 96/97 (100.0%) | Loss: 0.5415 | Batch time: 0.01s
2025-03-02 20:41:32,899 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:41:32,899 - INFO - Epoch 23/30 completed in 18.16s
2025-03-02 20:41:32,899 - INFO - Training   - Loss: 0.8485, Accuracy: 0.7292, F1: 0.7319
2025-03-02 20:41:32,899 - INFO - Validation - Loss: 0.4818, Accuracy: 0.8398, F1: 0.8430
2025-03-02 20:41:32,899 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:41:32,899 - INFO - Epoch 24/30
2025-03-02 20:41:32,899 - INFO - ----------------------------------------
2025-03-02 20:41:33,135 - INFO - [TRAIN] Epoch: 24/30 | Batch: 0/452 (0.2%) | Loss: 0.8061 | Batch time: 0.04s
2025-03-02 20:41:34,951 - INFO - [TRAIN] Epoch: 24/30 | Batch: 45/452 (10.2%) | Loss: 1.0185 | Batch time: 0.03s
2025-03-02 20:41:36,500 - INFO - [TRAIN] Epoch: 24/30 | Batch: 90/452 (20.1%) | Loss: 0.7179 | Batch time: 0.04s
2025-03-02 20:41:38,046 - INFO - [TRAIN] Epoch: 24/30 | Batch: 135/452 (30.1%) | Loss: 0.7410 | Batch time: 0.03s
2025-03-02 20:41:39,609 - INFO - [TRAIN] Epoch: 24/30 | Batch: 180/452 (40.0%) | Loss: 1.6457 | Batch time: 0.03s
2025-03-02 20:41:41,185 - INFO - [TRAIN] Epoch: 24/30 | Batch: 225/452 (50.0%) | Loss: 1.3485 | Batch time: 0.03s
2025-03-02 20:41:42,768 - INFO - [TRAIN] Epoch: 24/30 | Batch: 270/452 (60.0%) | Loss: 0.6887 | Batch time: 0.03s
2025-03-02 20:41:44,481 - INFO - [TRAIN] Epoch: 24/30 | Batch: 315/452 (69.9%) | Loss: 0.8084 | Batch time: 0.03s
2025-03-02 20:41:45,998 - INFO - [TRAIN] Epoch: 24/30 | Batch: 360/452 (79.9%) | Loss: 1.2548 | Batch time: 0.04s
2025-03-02 20:41:47,548 - INFO - [TRAIN] Epoch: 24/30 | Batch: 405/452 (89.8%) | Loss: 1.0660 | Batch time: 0.03s
2025-03-02 20:41:48,789 - INFO - [TRAIN] Epoch: 24/30 | Batch: 450/452 (99.8%) | Loss: 0.8472 | Batch time: 0.02s
2025-03-02 20:41:48,806 - INFO - [TRAIN] Epoch: 24/30 | Batch: 451/452 (100.0%) | Loss: 0.8651 | Batch time: 0.02s
2025-03-02 20:41:48,900 - INFO - [VAL] Epoch: 24/30 | Batch: 0/97 (1.0%) | Loss: 0.3678 | Batch time: 0.02s
2025-03-02 20:41:49,091 - INFO - [VAL] Epoch: 24/30 | Batch: 9/97 (10.3%) | Loss: 0.5165 | Batch time: 0.02s
2025-03-02 20:41:49,264 - INFO - [VAL] Epoch: 24/30 | Batch: 18/97 (19.6%) | Loss: 0.3926 | Batch time: 0.02s
2025-03-02 20:41:49,439 - INFO - [VAL] Epoch: 24/30 | Batch: 27/97 (28.9%) | Loss: 0.2830 | Batch time: 0.02s
2025-03-02 20:41:49,617 - INFO - [VAL] Epoch: 24/30 | Batch: 36/97 (38.1%) | Loss: 0.2683 | Batch time: 0.02s
2025-03-02 20:41:49,797 - INFO - [VAL] Epoch: 24/30 | Batch: 45/97 (47.4%) | Loss: 0.3244 | Batch time: 0.02s
2025-03-02 20:41:49,976 - INFO - [VAL] Epoch: 24/30 | Batch: 54/97 (56.7%) | Loss: 0.7943 | Batch time: 0.02s
2025-03-02 20:41:50,152 - INFO - [VAL] Epoch: 24/30 | Batch: 63/97 (66.0%) | Loss: 0.2380 | Batch time: 0.02s
2025-03-02 20:41:50,331 - INFO - [VAL] Epoch: 24/30 | Batch: 72/97 (75.3%) | Loss: 0.3463 | Batch time: 0.02s
2025-03-02 20:41:50,508 - INFO - [VAL] Epoch: 24/30 | Batch: 81/97 (84.5%) | Loss: 0.4690 | Batch time: 0.02s
2025-03-02 20:41:50,681 - INFO - [VAL] Epoch: 24/30 | Batch: 90/97 (93.8%) | Loss: 0.3538 | Batch time: 0.02s
2025-03-02 20:41:50,793 - INFO - [VAL] Epoch: 24/30 | Batch: 96/97 (100.0%) | Loss: 0.4869 | Batch time: 0.01s
2025-03-02 20:41:50,797 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:41:50,797 - INFO - Epoch 24/30 completed in 17.90s
2025-03-02 20:41:50,797 - INFO - Training   - Loss: 0.8442, Accuracy: 0.7345, F1: 0.7374
2025-03-02 20:41:50,797 - INFO - Validation - Loss: 0.4579, Accuracy: 0.8537, F1: 0.8564
2025-03-02 20:41:50,797 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:41:50,797 - INFO - Epoch 25/30
2025-03-02 20:41:50,797 - INFO - ----------------------------------------
2025-03-02 20:41:51,012 - INFO - [TRAIN] Epoch: 25/30 | Batch: 0/452 (0.2%) | Loss: 0.4125 | Batch time: 0.03s
2025-03-02 20:41:52,789 - INFO - [TRAIN] Epoch: 25/30 | Batch: 45/452 (10.2%) | Loss: 1.0516 | Batch time: 0.03s
2025-03-02 20:41:54,360 - INFO - [TRAIN] Epoch: 25/30 | Batch: 90/452 (20.1%) | Loss: 1.4989 | Batch time: 0.03s
2025-03-02 20:41:55,957 - INFO - [TRAIN] Epoch: 25/30 | Batch: 135/452 (30.1%) | Loss: 0.6341 | Batch time: 0.04s
2025-03-02 20:41:57,546 - INFO - [TRAIN] Epoch: 25/30 | Batch: 180/452 (40.0%) | Loss: 0.6926 | Batch time: 0.03s
2025-03-02 20:41:59,135 - INFO - [TRAIN] Epoch: 25/30 | Batch: 225/452 (50.0%) | Loss: 0.8151 | Batch time: 0.03s
2025-03-02 20:42:00,746 - INFO - [TRAIN] Epoch: 25/30 | Batch: 270/452 (60.0%) | Loss: 0.9230 | Batch time: 0.04s
2025-03-02 20:42:02,333 - INFO - [TRAIN] Epoch: 25/30 | Batch: 315/452 (69.9%) | Loss: 0.7985 | Batch time: 0.03s
2025-03-02 20:42:03,926 - INFO - [TRAIN] Epoch: 25/30 | Batch: 360/452 (79.9%) | Loss: 0.8955 | Batch time: 0.04s
2025-03-02 20:42:05,541 - INFO - [TRAIN] Epoch: 25/30 | Batch: 405/452 (89.8%) | Loss: 0.8529 | Batch time: 0.03s
2025-03-02 20:42:06,846 - INFO - [TRAIN] Epoch: 25/30 | Batch: 450/452 (99.8%) | Loss: 0.6256 | Batch time: 0.02s
2025-03-02 20:42:06,863 - INFO - [TRAIN] Epoch: 25/30 | Batch: 451/452 (100.0%) | Loss: 1.6064 | Batch time: 0.02s
2025-03-02 20:42:06,959 - INFO - [VAL] Epoch: 25/30 | Batch: 0/97 (1.0%) | Loss: 0.4686 | Batch time: 0.02s
2025-03-02 20:42:07,145 - INFO - [VAL] Epoch: 25/30 | Batch: 9/97 (10.3%) | Loss: 0.5993 | Batch time: 0.02s
2025-03-02 20:42:07,319 - INFO - [VAL] Epoch: 25/30 | Batch: 18/97 (19.6%) | Loss: 0.4401 | Batch time: 0.02s
2025-03-02 20:42:07,493 - INFO - [VAL] Epoch: 25/30 | Batch: 27/97 (28.9%) | Loss: 0.2883 | Batch time: 0.02s
2025-03-02 20:42:07,667 - INFO - [VAL] Epoch: 25/30 | Batch: 36/97 (38.1%) | Loss: 0.2888 | Batch time: 0.02s
2025-03-02 20:42:07,845 - INFO - [VAL] Epoch: 25/30 | Batch: 45/97 (47.4%) | Loss: 0.3340 | Batch time: 0.02s
2025-03-02 20:42:08,023 - INFO - [VAL] Epoch: 25/30 | Batch: 54/97 (56.7%) | Loss: 0.9332 | Batch time: 0.02s
2025-03-02 20:42:08,200 - INFO - [VAL] Epoch: 25/30 | Batch: 63/97 (66.0%) | Loss: 0.2912 | Batch time: 0.02s
2025-03-02 20:42:08,378 - INFO - [VAL] Epoch: 25/30 | Batch: 72/97 (75.3%) | Loss: 0.3893 | Batch time: 0.02s
2025-03-02 20:42:08,555 - INFO - [VAL] Epoch: 25/30 | Batch: 81/97 (84.5%) | Loss: 0.5270 | Batch time: 0.02s
2025-03-02 20:42:08,731 - INFO - [VAL] Epoch: 25/30 | Batch: 90/97 (93.8%) | Loss: 0.3832 | Batch time: 0.02s
2025-03-02 20:42:08,844 - INFO - [VAL] Epoch: 25/30 | Batch: 96/97 (100.0%) | Loss: 0.5615 | Batch time: 0.01s
2025-03-02 20:42:08,848 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:42:08,848 - INFO - Epoch 25/30 completed in 18.05s
2025-03-02 20:42:08,848 - INFO - Training   - Loss: 0.8298, Accuracy: 0.7320, F1: 0.7350
2025-03-02 20:42:08,848 - INFO - Validation - Loss: 0.4847, Accuracy: 0.8469, F1: 0.8503
2025-03-02 20:42:08,848 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:42:08,924 - INFO - Checkpoint saved: checkpoint_epoch_25.pth (Epoch 25)
2025-03-02 20:42:08,924 - INFO - Epoch 26/30
2025-03-02 20:42:08,924 - INFO - ----------------------------------------
2025-03-02 20:42:09,159 - INFO - [TRAIN] Epoch: 26/30 | Batch: 0/452 (0.2%) | Loss: 1.1067 | Batch time: 0.04s
2025-03-02 20:42:10,924 - INFO - [TRAIN] Epoch: 26/30 | Batch: 45/452 (10.2%) | Loss: 0.4953 | Batch time: 0.03s
2025-03-02 20:42:12,467 - INFO - [TRAIN] Epoch: 26/30 | Batch: 90/452 (20.1%) | Loss: 0.7241 | Batch time: 0.04s
2025-03-02 20:42:14,091 - INFO - [TRAIN] Epoch: 26/30 | Batch: 135/452 (30.1%) | Loss: 0.9209 | Batch time: 0.04s
2025-03-02 20:42:15,807 - INFO - [TRAIN] Epoch: 26/30 | Batch: 180/452 (40.0%) | Loss: 0.4986 | Batch time: 0.04s
2025-03-02 20:42:17,353 - INFO - [TRAIN] Epoch: 26/30 | Batch: 225/452 (50.0%) | Loss: 1.0406 | Batch time: 0.03s
2025-03-02 20:42:19,164 - INFO - [TRAIN] Epoch: 26/30 | Batch: 270/452 (60.0%) | Loss: 0.7786 | Batch time: 0.04s
2025-03-02 20:42:20,647 - INFO - [TRAIN] Epoch: 26/30 | Batch: 315/452 (69.9%) | Loss: 0.6714 | Batch time: 0.03s
2025-03-02 20:42:22,130 - INFO - [TRAIN] Epoch: 26/30 | Batch: 360/452 (79.9%) | Loss: 0.8361 | Batch time: 0.03s
2025-03-02 20:42:23,639 - INFO - [TRAIN] Epoch: 26/30 | Batch: 405/452 (89.8%) | Loss: 0.7837 | Batch time: 0.03s
2025-03-02 20:42:24,898 - INFO - [TRAIN] Epoch: 26/30 | Batch: 450/452 (99.8%) | Loss: 0.9752 | Batch time: 0.02s
2025-03-02 20:42:24,914 - INFO - [TRAIN] Epoch: 26/30 | Batch: 451/452 (100.0%) | Loss: 1.1484 | Batch time: 0.02s
2025-03-02 20:42:25,008 - INFO - [VAL] Epoch: 26/30 | Batch: 0/97 (1.0%) | Loss: 0.5573 | Batch time: 0.02s
2025-03-02 20:42:25,188 - INFO - [VAL] Epoch: 26/30 | Batch: 9/97 (10.3%) | Loss: 0.6868 | Batch time: 0.02s
2025-03-02 20:42:25,361 - INFO - [VAL] Epoch: 26/30 | Batch: 18/97 (19.6%) | Loss: 0.4767 | Batch time: 0.02s
2025-03-02 20:42:25,536 - INFO - [VAL] Epoch: 26/30 | Batch: 27/97 (28.9%) | Loss: 0.3201 | Batch time: 0.02s
2025-03-02 20:42:25,712 - INFO - [VAL] Epoch: 26/30 | Batch: 36/97 (38.1%) | Loss: 0.3368 | Batch time: 0.02s
2025-03-02 20:42:25,891 - INFO - [VAL] Epoch: 26/30 | Batch: 45/97 (47.4%) | Loss: 0.3230 | Batch time: 0.02s
2025-03-02 20:42:26,070 - INFO - [VAL] Epoch: 26/30 | Batch: 54/97 (56.7%) | Loss: 1.0101 | Batch time: 0.02s
2025-03-02 20:42:26,250 - INFO - [VAL] Epoch: 26/30 | Batch: 63/97 (66.0%) | Loss: 0.3077 | Batch time: 0.02s
2025-03-02 20:42:26,429 - INFO - [VAL] Epoch: 26/30 | Batch: 72/97 (75.3%) | Loss: 0.4656 | Batch time: 0.02s
2025-03-02 20:42:26,607 - INFO - [VAL] Epoch: 26/30 | Batch: 81/97 (84.5%) | Loss: 0.5308 | Batch time: 0.02s
2025-03-02 20:42:26,785 - INFO - [VAL] Epoch: 26/30 | Batch: 90/97 (93.8%) | Loss: 0.4068 | Batch time: 0.02s
2025-03-02 20:42:26,898 - INFO - [VAL] Epoch: 26/30 | Batch: 96/97 (100.0%) | Loss: 0.8508 | Batch time: 0.01s
2025-03-02 20:42:26,902 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:42:26,902 - INFO - Epoch 26/30 completed in 17.98s
2025-03-02 20:42:26,902 - INFO - Training   - Loss: 0.8361, Accuracy: 0.7347, F1: 0.7377
2025-03-02 20:42:26,902 - INFO - Validation - Loss: 0.5445, Accuracy: 0.8233, F1: 0.8291
2025-03-02 20:42:26,902 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:42:26,902 - INFO - Epoch 27/30
2025-03-02 20:42:26,902 - INFO - ----------------------------------------
2025-03-02 20:42:27,130 - INFO - [TRAIN] Epoch: 27/30 | Batch: 0/452 (0.2%) | Loss: 0.6299 | Batch time: 0.03s
2025-03-02 20:42:28,831 - INFO - [TRAIN] Epoch: 27/30 | Batch: 45/452 (10.2%) | Loss: 0.6377 | Batch time: 0.04s
2025-03-02 20:42:30,377 - INFO - [TRAIN] Epoch: 27/30 | Batch: 90/452 (20.1%) | Loss: 1.0999 | Batch time: 0.03s
2025-03-02 20:42:31,936 - INFO - [TRAIN] Epoch: 27/30 | Batch: 135/452 (30.1%) | Loss: 0.7121 | Batch time: 0.03s
2025-03-02 20:42:33,477 - INFO - [TRAIN] Epoch: 27/30 | Batch: 180/452 (40.0%) | Loss: 0.6045 | Batch time: 0.03s
2025-03-02 20:42:35,052 - INFO - [TRAIN] Epoch: 27/30 | Batch: 225/452 (50.0%) | Loss: 0.6757 | Batch time: 0.03s
2025-03-02 20:42:36,598 - INFO - [TRAIN] Epoch: 27/30 | Batch: 270/452 (60.0%) | Loss: 0.8227 | Batch time: 0.04s
2025-03-02 20:42:38,148 - INFO - [TRAIN] Epoch: 27/30 | Batch: 315/452 (69.9%) | Loss: 0.6740 | Batch time: 0.03s
2025-03-02 20:42:39,708 - INFO - [TRAIN] Epoch: 27/30 | Batch: 360/452 (79.9%) | Loss: 0.4593 | Batch time: 0.03s
2025-03-02 20:42:41,258 - INFO - [TRAIN] Epoch: 27/30 | Batch: 405/452 (89.8%) | Loss: 1.0944 | Batch time: 0.03s
2025-03-02 20:42:42,583 - INFO - [TRAIN] Epoch: 27/30 | Batch: 450/452 (99.8%) | Loss: 0.9429 | Batch time: 0.02s
2025-03-02 20:42:42,600 - INFO - [TRAIN] Epoch: 27/30 | Batch: 451/452 (100.0%) | Loss: 0.6580 | Batch time: 0.02s
2025-03-02 20:42:42,698 - INFO - [VAL] Epoch: 27/30 | Batch: 0/97 (1.0%) | Loss: 0.4464 | Batch time: 0.02s
2025-03-02 20:42:42,878 - INFO - [VAL] Epoch: 27/30 | Batch: 9/97 (10.3%) | Loss: 0.5408 | Batch time: 0.02s
2025-03-02 20:42:43,050 - INFO - [VAL] Epoch: 27/30 | Batch: 18/97 (19.6%) | Loss: 0.4718 | Batch time: 0.02s
2025-03-02 20:42:43,223 - INFO - [VAL] Epoch: 27/30 | Batch: 27/97 (28.9%) | Loss: 0.2730 | Batch time: 0.02s
2025-03-02 20:42:43,398 - INFO - [VAL] Epoch: 27/30 | Batch: 36/97 (38.1%) | Loss: 0.2605 | Batch time: 0.02s
2025-03-02 20:42:43,574 - INFO - [VAL] Epoch: 27/30 | Batch: 45/97 (47.4%) | Loss: 0.3525 | Batch time: 0.02s
2025-03-02 20:42:43,751 - INFO - [VAL] Epoch: 27/30 | Batch: 54/97 (56.7%) | Loss: 0.8621 | Batch time: 0.02s
2025-03-02 20:42:43,930 - INFO - [VAL] Epoch: 27/30 | Batch: 63/97 (66.0%) | Loss: 0.2535 | Batch time: 0.02s
2025-03-02 20:42:44,106 - INFO - [VAL] Epoch: 27/30 | Batch: 72/97 (75.3%) | Loss: 0.4063 | Batch time: 0.02s
2025-03-02 20:42:44,280 - INFO - [VAL] Epoch: 27/30 | Batch: 81/97 (84.5%) | Loss: 0.4970 | Batch time: 0.02s
2025-03-02 20:42:44,455 - INFO - [VAL] Epoch: 27/30 | Batch: 90/97 (93.8%) | Loss: 0.3694 | Batch time: 0.02s
2025-03-02 20:42:44,567 - INFO - [VAL] Epoch: 27/30 | Batch: 96/97 (100.0%) | Loss: 0.5427 | Batch time: 0.02s
2025-03-02 20:42:44,570 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:42:44,570 - INFO - Epoch 27/30 completed in 17.67s
2025-03-02 20:42:44,570 - INFO - Training   - Loss: 0.8494, Accuracy: 0.7276, F1: 0.7302
2025-03-02 20:42:44,570 - INFO - Validation - Loss: 0.4842, Accuracy: 0.8472, F1: 0.8503
2025-03-02 20:42:44,570 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:42:44,570 - INFO - Epoch 28/30
2025-03-02 20:42:44,570 - INFO - ----------------------------------------
2025-03-02 20:42:44,809 - INFO - [TRAIN] Epoch: 28/30 | Batch: 0/452 (0.2%) | Loss: 0.5710 | Batch time: 0.04s
2025-03-02 20:42:46,581 - INFO - [TRAIN] Epoch: 28/30 | Batch: 45/452 (10.2%) | Loss: 0.7067 | Batch time: 0.03s
2025-03-02 20:42:48,105 - INFO - [TRAIN] Epoch: 28/30 | Batch: 90/452 (20.1%) | Loss: 0.6693 | Batch time: 0.03s
2025-03-02 20:42:49,698 - INFO - [TRAIN] Epoch: 28/30 | Batch: 135/452 (30.1%) | Loss: 0.7727 | Batch time: 0.03s
2025-03-02 20:42:51,260 - INFO - [TRAIN] Epoch: 28/30 | Batch: 180/452 (40.0%) | Loss: 1.0144 | Batch time: 0.03s
2025-03-02 20:42:52,839 - INFO - [TRAIN] Epoch: 28/30 | Batch: 225/452 (50.0%) | Loss: 0.9099 | Batch time: 0.03s
2025-03-02 20:42:54,401 - INFO - [TRAIN] Epoch: 28/30 | Batch: 270/452 (60.0%) | Loss: 2.5851 | Batch time: 0.03s
2025-03-02 20:42:55,994 - INFO - [TRAIN] Epoch: 28/30 | Batch: 315/452 (69.9%) | Loss: 0.7861 | Batch time: 0.03s
2025-03-02 20:42:57,545 - INFO - [TRAIN] Epoch: 28/30 | Batch: 360/452 (79.9%) | Loss: 0.7000 | Batch time: 0.03s
2025-03-02 20:42:59,131 - INFO - [TRAIN] Epoch: 28/30 | Batch: 405/452 (89.8%) | Loss: 0.9431 | Batch time: 0.03s
2025-03-02 20:43:00,396 - INFO - [TRAIN] Epoch: 28/30 | Batch: 450/452 (99.8%) | Loss: 1.0462 | Batch time: 0.02s
2025-03-02 20:43:00,413 - INFO - [TRAIN] Epoch: 28/30 | Batch: 451/452 (100.0%) | Loss: 1.7203 | Batch time: 0.02s
2025-03-02 20:43:00,510 - INFO - [VAL] Epoch: 28/30 | Batch: 0/97 (1.0%) | Loss: 0.3961 | Batch time: 0.02s
2025-03-02 20:43:00,690 - INFO - [VAL] Epoch: 28/30 | Batch: 9/97 (10.3%) | Loss: 0.5339 | Batch time: 0.02s
2025-03-02 20:43:00,863 - INFO - [VAL] Epoch: 28/30 | Batch: 18/97 (19.6%) | Loss: 0.4329 | Batch time: 0.02s
2025-03-02 20:43:01,036 - INFO - [VAL] Epoch: 28/30 | Batch: 27/97 (28.9%) | Loss: 0.2688 | Batch time: 0.02s
2025-03-02 20:43:01,211 - INFO - [VAL] Epoch: 28/30 | Batch: 36/97 (38.1%) | Loss: 0.2830 | Batch time: 0.02s
2025-03-02 20:43:01,388 - INFO - [VAL] Epoch: 28/30 | Batch: 45/97 (47.4%) | Loss: 0.3500 | Batch time: 0.02s
2025-03-02 20:43:01,565 - INFO - [VAL] Epoch: 28/30 | Batch: 54/97 (56.7%) | Loss: 0.8602 | Batch time: 0.02s
2025-03-02 20:43:01,744 - INFO - [VAL] Epoch: 28/30 | Batch: 63/97 (66.0%) | Loss: 0.2601 | Batch time: 0.02s
2025-03-02 20:43:01,921 - INFO - [VAL] Epoch: 28/30 | Batch: 72/97 (75.3%) | Loss: 0.4019 | Batch time: 0.02s
2025-03-02 20:43:02,095 - INFO - [VAL] Epoch: 28/30 | Batch: 81/97 (84.5%) | Loss: 0.4858 | Batch time: 0.02s
2025-03-02 20:43:02,269 - INFO - [VAL] Epoch: 28/30 | Batch: 90/97 (93.8%) | Loss: 0.3719 | Batch time: 0.02s
2025-03-02 20:43:02,381 - INFO - [VAL] Epoch: 28/30 | Batch: 96/97 (100.0%) | Loss: 0.5403 | Batch time: 0.02s
2025-03-02 20:43:02,385 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:02,385 - INFO - Epoch 28/30 completed in 17.81s
2025-03-02 20:43:02,385 - INFO - Training   - Loss: 0.8407, Accuracy: 0.7316, F1: 0.7338
2025-03-02 20:43:02,385 - INFO - Validation - Loss: 0.4830, Accuracy: 0.8404, F1: 0.8450
2025-03-02 20:43:02,385 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:02,385 - INFO - Epoch 29/30
2025-03-02 20:43:02,385 - INFO - ----------------------------------------
2025-03-02 20:43:02,604 - INFO - [TRAIN] Epoch: 29/30 | Batch: 0/452 (0.2%) | Loss: 1.1084 | Batch time: 0.03s
2025-03-02 20:43:04,390 - INFO - [TRAIN] Epoch: 29/30 | Batch: 45/452 (10.2%) | Loss: 0.7436 | Batch time: 0.03s
2025-03-02 20:43:05,944 - INFO - [TRAIN] Epoch: 29/30 | Batch: 90/452 (20.1%) | Loss: 0.9093 | Batch time: 0.03s
2025-03-02 20:43:07,481 - INFO - [TRAIN] Epoch: 29/30 | Batch: 135/452 (30.1%) | Loss: 1.0900 | Batch time: 0.03s
2025-03-02 20:43:09,045 - INFO - [TRAIN] Epoch: 29/30 | Batch: 180/452 (40.0%) | Loss: 0.8486 | Batch time: 0.03s
2025-03-02 20:43:10,629 - INFO - [TRAIN] Epoch: 29/30 | Batch: 225/452 (50.0%) | Loss: 0.8655 | Batch time: 0.04s
2025-03-02 20:43:12,212 - INFO - [TRAIN] Epoch: 29/30 | Batch: 270/452 (60.0%) | Loss: 0.7814 | Batch time: 0.04s
2025-03-02 20:43:13,788 - INFO - [TRAIN] Epoch: 29/30 | Batch: 315/452 (69.9%) | Loss: 0.7446 | Batch time: 0.03s
2025-03-02 20:43:15,413 - INFO - [TRAIN] Epoch: 29/30 | Batch: 360/452 (79.9%) | Loss: 0.7219 | Batch time: 0.03s
2025-03-02 20:43:16,985 - INFO - [TRAIN] Epoch: 29/30 | Batch: 405/452 (89.8%) | Loss: 0.8916 | Batch time: 0.03s
2025-03-02 20:43:18,261 - INFO - [TRAIN] Epoch: 29/30 | Batch: 450/452 (99.8%) | Loss: 1.1010 | Batch time: 0.02s
2025-03-02 20:43:18,278 - INFO - [TRAIN] Epoch: 29/30 | Batch: 451/452 (100.0%) | Loss: 1.0296 | Batch time: 0.02s
2025-03-02 20:43:18,369 - INFO - [VAL] Epoch: 29/30 | Batch: 0/97 (1.0%) | Loss: 0.4664 | Batch time: 0.02s
2025-03-02 20:43:18,548 - INFO - [VAL] Epoch: 29/30 | Batch: 9/97 (10.3%) | Loss: 0.5877 | Batch time: 0.02s
2025-03-02 20:43:18,721 - INFO - [VAL] Epoch: 29/30 | Batch: 18/97 (19.6%) | Loss: 0.4243 | Batch time: 0.02s
2025-03-02 20:43:18,894 - INFO - [VAL] Epoch: 29/30 | Batch: 27/97 (28.9%) | Loss: 0.3080 | Batch time: 0.02s
2025-03-02 20:43:19,070 - INFO - [VAL] Epoch: 29/30 | Batch: 36/97 (38.1%) | Loss: 0.3069 | Batch time: 0.02s
2025-03-02 20:43:19,246 - INFO - [VAL] Epoch: 29/30 | Batch: 45/97 (47.4%) | Loss: 0.3168 | Batch time: 0.02s
2025-03-02 20:43:19,424 - INFO - [VAL] Epoch: 29/30 | Batch: 54/97 (56.7%) | Loss: 0.8828 | Batch time: 0.02s
2025-03-02 20:43:19,601 - INFO - [VAL] Epoch: 29/30 | Batch: 63/97 (66.0%) | Loss: 0.2646 | Batch time: 0.02s
2025-03-02 20:43:19,778 - INFO - [VAL] Epoch: 29/30 | Batch: 72/97 (75.3%) | Loss: 0.3947 | Batch time: 0.02s
2025-03-02 20:43:19,953 - INFO - [VAL] Epoch: 29/30 | Batch: 81/97 (84.5%) | Loss: 0.4937 | Batch time: 0.02s
2025-03-02 20:43:20,127 - INFO - [VAL] Epoch: 29/30 | Batch: 90/97 (93.8%) | Loss: 0.3524 | Batch time: 0.02s
2025-03-02 20:43:20,237 - INFO - [VAL] Epoch: 29/30 | Batch: 96/97 (100.0%) | Loss: 0.5457 | Batch time: 0.01s
2025-03-02 20:43:20,240 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:20,240 - INFO - Epoch 29/30 completed in 17.86s
2025-03-02 20:43:20,240 - INFO - Training   - Loss: 0.8339, Accuracy: 0.7320, F1: 0.7350
2025-03-02 20:43:20,240 - INFO - Validation - Loss: 0.4883, Accuracy: 0.8453, F1: 0.8491
2025-03-02 20:43:20,240 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:20,240 - INFO - Epoch 30/30
2025-03-02 20:43:20,240 - INFO - ----------------------------------------
2025-03-02 20:43:20,480 - INFO - [TRAIN] Epoch: 30/30 | Batch: 0/452 (0.2%) | Loss: 1.2373 | Batch time: 0.04s
2025-03-02 20:43:22,195 - INFO - [TRAIN] Epoch: 30/30 | Batch: 45/452 (10.2%) | Loss: 0.9378 | Batch time: 0.03s
2025-03-02 20:43:23,729 - INFO - [TRAIN] Epoch: 30/30 | Batch: 90/452 (20.1%) | Loss: 0.7499 | Batch time: 0.03s
2025-03-02 20:43:25,263 - INFO - [TRAIN] Epoch: 30/30 | Batch: 135/452 (30.1%) | Loss: 0.9147 | Batch time: 0.03s
2025-03-02 20:43:26,804 - INFO - [TRAIN] Epoch: 30/30 | Batch: 180/452 (40.0%) | Loss: 0.9100 | Batch time: 0.03s
2025-03-02 20:43:28,348 - INFO - [TRAIN] Epoch: 30/30 | Batch: 225/452 (50.0%) | Loss: 1.6563 | Batch time: 0.03s
2025-03-02 20:43:29,897 - INFO - [TRAIN] Epoch: 30/30 | Batch: 270/452 (60.0%) | Loss: 0.8899 | Batch time: 0.03s
2025-03-02 20:43:31,464 - INFO - [TRAIN] Epoch: 30/30 | Batch: 315/452 (69.9%) | Loss: 1.0286 | Batch time: 0.03s
2025-03-02 20:43:33,006 - INFO - [TRAIN] Epoch: 30/30 | Batch: 360/452 (79.9%) | Loss: 0.9302 | Batch time: 0.03s
2025-03-02 20:43:34,563 - INFO - [TRAIN] Epoch: 30/30 | Batch: 405/452 (89.8%) | Loss: 0.7865 | Batch time: 0.03s
2025-03-02 20:43:35,835 - INFO - [TRAIN] Epoch: 30/30 | Batch: 450/452 (99.8%) | Loss: 0.7534 | Batch time: 0.02s
2025-03-02 20:43:35,852 - INFO - [TRAIN] Epoch: 30/30 | Batch: 451/452 (100.0%) | Loss: 1.4558 | Batch time: 0.02s
2025-03-02 20:43:35,946 - INFO - [VAL] Epoch: 30/30 | Batch: 0/97 (1.0%) | Loss: 0.6365 | Batch time: 0.02s
2025-03-02 20:43:36,128 - INFO - [VAL] Epoch: 30/30 | Batch: 9/97 (10.3%) | Loss: 0.7938 | Batch time: 0.02s
2025-03-02 20:43:36,307 - INFO - [VAL] Epoch: 30/30 | Batch: 18/97 (19.6%) | Loss: 0.5279 | Batch time: 0.02s
2025-03-02 20:43:36,478 - INFO - [VAL] Epoch: 30/30 | Batch: 27/97 (28.9%) | Loss: 0.3621 | Batch time: 0.02s
2025-03-02 20:43:36,651 - INFO - [VAL] Epoch: 30/30 | Batch: 36/97 (38.1%) | Loss: 0.3458 | Batch time: 0.02s
2025-03-02 20:43:36,828 - INFO - [VAL] Epoch: 30/30 | Batch: 45/97 (47.4%) | Loss: 0.2919 | Batch time: 0.02s
2025-03-02 20:43:37,006 - INFO - [VAL] Epoch: 30/30 | Batch: 54/97 (56.7%) | Loss: 1.0997 | Batch time: 0.02s
2025-03-02 20:43:37,182 - INFO - [VAL] Epoch: 30/30 | Batch: 63/97 (66.0%) | Loss: 0.3192 | Batch time: 0.02s
2025-03-02 20:43:37,359 - INFO - [VAL] Epoch: 30/30 | Batch: 72/97 (75.3%) | Loss: 0.4859 | Batch time: 0.02s
2025-03-02 20:43:37,533 - INFO - [VAL] Epoch: 30/30 | Batch: 81/97 (84.5%) | Loss: 0.5365 | Batch time: 0.02s
2025-03-02 20:43:37,709 - INFO - [VAL] Epoch: 30/30 | Batch: 90/97 (93.8%) | Loss: 0.4100 | Batch time: 0.02s
2025-03-02 20:43:37,821 - INFO - [VAL] Epoch: 30/30 | Batch: 96/97 (100.0%) | Loss: 0.9944 | Batch time: 0.01s
2025-03-02 20:43:37,824 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:37,824 - INFO - Epoch 30/30 completed in 17.58s
2025-03-02 20:43:37,824 - INFO - Training   - Loss: 0.8364, Accuracy: 0.7264, F1: 0.7292
2025-03-02 20:43:37,824 - INFO - Validation - Loss: 0.5727, Accuracy: 0.8127, F1: 0.8198
2025-03-02 20:43:37,824 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:37,900 - INFO - Checkpoint saved: checkpoint_epoch_30.pth (Epoch 30)
2025-03-02 20:43:37,900 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:37,900 - INFO - Training completed in 0h 9m 8.06s
2025-03-02 20:43:37,900 - INFO - Best validation F1: 0.8580 (Epoch 16)
2025-03-02 20:43:37,900 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:38,376 - INFO - Final model saved to models/efficientnet_b0_v1/models/efficientnet_b0_v1_final.pth

2025-03-04 22:57:25,705 - INFO - Starting experiment: efficientnet_b0
2025-03-04 22:57:25,705 - INFO - Command line arguments: Namespace(data_dir='data/raw', output_dir='models/efficientnet_b0_v2', model='efficientnet', img_size=224, batch_size=32, num_workers=16, epochs=30, lr=0.001, weight_decay=0.0001, use_weights=True, freeze_backbone=True, no_cuda=False, no_mps=False, use_mps=True, use_amp=False, memory_efficient=True, cache_dataset=True, mps_graph=True, mps_fallback=False, pin_memory=False, optimize_for_m_series=True, patience=10, keep_top_k=3, version=None, find_lr=False, experiment_name='efficientnet_b0', resnet_version=50)
2025-03-04 22:57:25,705 - INFO - Processing dataset...
2025-03-04 22:57:25,909 - INFO - Class distribution:
2025-03-04 22:57:25,909 - INFO -   Strawberry___healthy: 1000 images
2025-03-04 22:57:25,909 - INFO -   Grape___Black_rot: 1180 images
2025-03-04 22:57:25,909 - INFO -   Potato___Early_blight: 1000 images
2025-03-04 22:57:25,909 - INFO -   Blueberry___healthy: 1502 images
2025-03-04 22:57:25,909 - INFO -   Cherry___Powdery_mildew: 1052 images
2025-03-04 22:57:25,909 - INFO -   Tomato___Target_Spot: 1404 images
2025-03-04 22:57:25,909 - INFO -   Peach___healthy: 1000 images
2025-03-04 22:57:25,909 - INFO -   Potato___Late_blight: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Late_blight: 1909 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Tomato_mosaic_virus: 1000 images
2025-03-04 22:57:25,910 - INFO -   Pepper,_bell___healthy: 1478 images
2025-03-04 22:57:25,910 - INFO -   Orange___Haunglongbing_(Citrus_greening): 5507 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Leaf_Mold: 1000 images
2025-03-04 22:57:25,910 - INFO -   Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 1076 images
2025-03-04 22:57:25,910 - INFO -   Apple___Cedar_apple_rust: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Bacterial_spot: 2127 images
2025-03-04 22:57:25,910 - INFO -   Grape___healthy: 1000 images
2025-03-04 22:57:25,910 - INFO -   Corn___Cercospora_leaf_spot Gray_leaf_spot: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Early_blight: 1000 images
2025-03-04 22:57:25,910 - INFO -   Grape___Esca_(Black_Measles): 1383 images
2025-03-04 22:57:25,910 - INFO -   Raspberry___healthy: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___healthy: 1591 images
2025-03-04 22:57:25,910 - INFO -   Corn___Northern_Leaf_Blight: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Tomato_Yellow_Leaf_Curl_Virus: 5357 images
2025-03-04 22:57:25,910 - INFO -   Cherry___healthy: 1000 images
2025-03-04 22:57:25,910 - INFO -   Apple___Apple_scab: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Spider_mites Two-spotted_spider_mite: 1676 images
2025-03-04 22:57:25,910 - INFO -   Corn___Common_rust: 1192 images
2025-03-04 22:57:25,910 - INFO -   Background_without_leaves: 1143 images
2025-03-04 22:57:25,910 - INFO -   Peach___Bacterial_spot: 2297 images
2025-03-04 22:57:25,910 - INFO -   Pepper,_bell___Bacterial_spot: 1000 images
2025-03-04 22:57:25,910 - INFO -   Tomato___Septoria_leaf_spot: 1771 images
2025-03-04 22:57:25,910 - INFO -   Corn___healthy: 1162 images
2025-03-04 22:57:25,910 - INFO -   Squash___Powdery_mildew: 1835 images
2025-03-04 22:57:25,910 - INFO -   Apple___Black_rot: 1000 images
2025-03-04 22:57:25,910 - INFO -   Apple___healthy: 1645 images
2025-03-04 22:57:25,910 - INFO -   Strawberry___Leaf_scorch: 1109 images
2025-03-04 22:57:25,910 - INFO -   Potato___healthy: 1000 images
2025-03-04 22:57:25,910 - INFO -   Soybean___healthy: 5090 images
2025-03-04 22:57:25,910 - INFO - Creating model: efficientnet with 39 classes
2025-03-04 22:57:26,161 - INFO - Model architecture:
EfficientNet(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
    )
    (2): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.025, mode=row)
      )
    )
    (3): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.125, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)
      )
    )
    (7): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)
      )
    )
    (8): Conv2dNormActivation(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=39, bias=True)
  )
)
2025-03-04 22:57:26,168 - INFO - Using class weights: [1.2614028  1.0689855  1.2614028  0.83981544 1.1990521  0.89843506
 1.2614028  1.2614028  0.66076624 1.2614028  0.8534525  0.22905444
 1.2614028  1.1723075  1.2614028  0.59304315 1.2614028  1.2614028
 1.2614028  0.91207725 1.2614028  0.7928365  1.2614028  0.23546813
 1.2614028  1.2614028  0.75262696 1.0582238  1.1035895  0.5491523
 1.2614028  0.7122546  1.0855446  0.687413   1.2614028  0.76681024
 1.1374236  1.2614028  0.24781981]
2025-03-04 22:57:26,168 - INFO - Training only 2 parameters (classifier)
2025-03-04 22:57:26,171 - INFO - Starting training for 30 epochs
2025-03-04 22:57:26,171 - INFO - Using Automatic Mixed Precision: False
2025-03-04 22:57:26,171 - INFO - Early stopping patience: 10
2025-03-04 22:57:26,171 - INFO - --------------------------------------------------------------------------------
2025-03-04 22:57:26,171 - INFO - Starting training: efficientnet_b0
2025-03-04 22:57:26,171 - INFO - Total epochs: 30
2025-03-04 22:57:26,171 - INFO - Training batches per epoch: 1345
2025-03-04 22:57:26,171 - INFO - Validation batches per epoch: 289
2025-03-04 22:57:26,171 - INFO - --------------------------------------------------------------------------------
2025-03-04 22:57:26,171 - INFO - Training model: efficientnet_b0_v1
2025-03-04 22:57:26,171 - INFO - Epoch 1/30
2025-03-04 22:57:26,171 - INFO - ----------------------------------------
2025-03-04 22:58:07,980 - INFO - [TRAIN] Epoch: 1/30 | Batch: 0/1345 (0.1%) | Loss: 3.7364 | Batch time: 2.73s
2025-03-04 22:58:14,513 - INFO - [TRAIN] Epoch: 1/30 | Batch: 134/1345 (10.0%) | Loss: 2.2401 | Batch time: 0.06s
2025-03-04 22:58:21,063 - INFO - [TRAIN] Epoch: 1/30 | Batch: 268/1345 (20.0%) | Loss: 1.9598 | Batch time: 0.06s
2025-03-04 22:58:27,659 - INFO - [TRAIN] Epoch: 1/30 | Batch: 402/1345 (30.0%) | Loss: 1.7878 | Batch time: 0.06s
2025-03-04 22:58:34,155 - INFO - [TRAIN] Epoch: 1/30 | Batch: 536/1345 (39.9%) | Loss: 1.4880 | Batch time: 0.02s
2025-03-04 22:58:40,991 - INFO - [TRAIN] Epoch: 1/30 | Batch: 670/1345 (49.9%) | Loss: 1.0129 | Batch time: 0.04s
2025-03-04 22:58:46,493 - INFO - [TRAIN] Epoch: 1/30 | Batch: 804/1345 (59.9%) | Loss: 1.2737 | Batch time: 0.07s
2025-03-04 22:58:53,178 - INFO - [TRAIN] Epoch: 1/30 | Batch: 938/1345 (69.8%) | Loss: 1.3994 | Batch time: 0.06s
2025-03-04 22:58:59,824 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1072/1345 (79.8%) | Loss: 0.9290 | Batch time: 0.07s
2025-03-04 22:59:06,593 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1206/1345 (89.7%) | Loss: 1.0387 | Batch time: 0.03s
2025-03-04 22:59:13,332 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1340/1345 (99.7%) | Loss: 1.1753 | Batch time: 0.03s
2025-03-04 22:59:13,550 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1344/1345 (100.0%) | Loss: 0.7805 | Batch time: 0.05s
2025-03-04 22:59:53,771 - INFO - [VAL] Epoch: 1/30 | Batch: 0/289 (0.3%) | Loss: 0.6001 | Batch time: 0.38s
2025-03-04 22:59:55,005 - INFO - [VAL] Epoch: 1/30 | Batch: 28/289 (10.0%) | Loss: 0.7145 | Batch time: 0.04s
2025-03-04 22:59:56,124 - INFO - [VAL] Epoch: 1/30 | Batch: 56/289 (19.7%) | Loss: 0.9363 | Batch time: 0.05s
2025-03-04 22:59:57,197 - INFO - [VAL] Epoch: 1/30 | Batch: 84/289 (29.4%) | Loss: 0.5886 | Batch time: 0.05s
2025-03-04 22:59:58,248 - INFO - [VAL] Epoch: 1/30 | Batch: 112/289 (39.1%) | Loss: 0.5851 | Batch time: 0.05s
2025-03-04 22:59:59,310 - INFO - [VAL] Epoch: 1/30 | Batch: 140/289 (48.8%) | Loss: 0.4484 | Batch time: 0.03s
2025-03-04 23:00:00,358 - INFO - [VAL] Epoch: 1/30 | Batch: 168/289 (58.5%) | Loss: 0.4657 | Batch time: 0.02s
2025-03-04 23:00:01,410 - INFO - [VAL] Epoch: 1/30 | Batch: 196/289 (68.2%) | Loss: 0.4507 | Batch time: 0.02s
2025-03-04 23:00:02,452 - INFO - [VAL] Epoch: 1/30 | Batch: 224/289 (77.9%) | Loss: 0.5304 | Batch time: 0.02s
2025-03-04 23:00:03,494 - INFO - [VAL] Epoch: 1/30 | Batch: 252/289 (87.5%) | Loss: 0.8039 | Batch time: 0.02s
2025-03-04 23:00:04,568 - INFO - [VAL] Epoch: 1/30 | Batch: 280/289 (97.2%) | Loss: 0.7289 | Batch time: 0.04s
2025-03-04 23:00:05,488 - INFO - [VAL] Epoch: 1/30 | Batch: 288/289 (100.0%) | Loss: 0.2825 | Batch time: 0.61s
2025-03-04 23:00:06,022 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 1)
2025-03-04 23:00:06,022 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:00:06,022 - INFO - Epoch 1/30 completed in 159.85s
2025-03-04 23:00:06,022 - INFO - Training   - Loss: 1.4255, Accuracy: 0.6521, F1: 0.6539
2025-03-04 23:00:06,022 - INFO - Validation - Loss: 0.6127, Accuracy: 0.8432, F1: 0.8436
2025-03-04 23:00:06,022 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:00:06,023 - INFO - Epoch 2/30
2025-03-04 23:00:06,023 - INFO - ----------------------------------------
2025-03-04 23:00:06,661 - INFO - [TRAIN] Epoch: 2/30 | Batch: 0/1345 (0.1%) | Loss: 0.8994 | Batch time: 0.14s
2025-03-04 23:00:13,734 - INFO - [TRAIN] Epoch: 2/30 | Batch: 134/1345 (10.0%) | Loss: 1.0431 | Batch time: 0.03s
2025-03-04 23:00:20,716 - INFO - [TRAIN] Epoch: 2/30 | Batch: 268/1345 (20.0%) | Loss: 0.7235 | Batch time: 0.05s
2025-03-04 23:00:27,513 - INFO - [TRAIN] Epoch: 2/30 | Batch: 402/1345 (30.0%) | Loss: 1.0514 | Batch time: 0.04s
2025-03-04 23:00:34,273 - INFO - [TRAIN] Epoch: 2/30 | Batch: 536/1345 (39.9%) | Loss: 0.9622 | Batch time: 0.05s
2025-03-04 23:00:40,831 - INFO - [TRAIN] Epoch: 2/30 | Batch: 670/1345 (49.9%) | Loss: 1.3249 | Batch time: 0.07s
2025-03-04 23:00:47,102 - INFO - [TRAIN] Epoch: 2/30 | Batch: 804/1345 (59.9%) | Loss: 0.8162 | Batch time: 0.04s
2025-03-04 23:00:54,982 - INFO - [TRAIN] Epoch: 2/30 | Batch: 938/1345 (69.8%) | Loss: 0.9212 | Batch time: 0.03s
2025-03-04 23:00:59,832 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1072/1345 (79.8%) | Loss: 1.0558 | Batch time: 0.03s
2025-03-04 23:01:06,394 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1206/1345 (89.7%) | Loss: 0.6077 | Batch time: 0.06s
2025-03-04 23:01:13,120 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1340/1345 (99.7%) | Loss: 0.8132 | Batch time: 0.04s
2025-03-04 23:01:13,342 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1344/1345 (100.0%) | Loss: 0.6305 | Batch time: 0.05s
2025-03-04 23:01:13,606 - INFO - [VAL] Epoch: 2/30 | Batch: 0/289 (0.3%) | Loss: 0.4602 | Batch time: 0.05s
2025-03-04 23:01:14,717 - INFO - [VAL] Epoch: 2/30 | Batch: 28/289 (10.0%) | Loss: 0.5629 | Batch time: 0.02s
2025-03-04 23:01:15,840 - INFO - [VAL] Epoch: 2/30 | Batch: 56/289 (19.7%) | Loss: 0.6501 | Batch time: 0.03s
2025-03-04 23:01:16,983 - INFO - [VAL] Epoch: 2/30 | Batch: 84/289 (29.4%) | Loss: 0.3630 | Batch time: 0.05s
2025-03-04 23:01:18,088 - INFO - [VAL] Epoch: 2/30 | Batch: 112/289 (39.1%) | Loss: 0.5667 | Batch time: 0.05s
2025-03-04 23:01:19,244 - INFO - [VAL] Epoch: 2/30 | Batch: 140/289 (48.8%) | Loss: 0.2810 | Batch time: 0.04s
2025-03-04 23:01:20,367 - INFO - [VAL] Epoch: 2/30 | Batch: 168/289 (58.5%) | Loss: 0.3352 | Batch time: 0.04s
2025-03-04 23:01:21,485 - INFO - [VAL] Epoch: 2/30 | Batch: 196/289 (68.2%) | Loss: 0.3370 | Batch time: 0.05s
2025-03-04 23:01:22,605 - INFO - [VAL] Epoch: 2/30 | Batch: 224/289 (77.9%) | Loss: 0.4300 | Batch time: 0.05s
2025-03-04 23:01:23,721 - INFO - [VAL] Epoch: 2/30 | Batch: 252/289 (87.5%) | Loss: 0.5909 | Batch time: 0.05s
2025-03-04 23:01:24,812 - INFO - [VAL] Epoch: 2/30 | Batch: 280/289 (97.2%) | Loss: 0.5918 | Batch time: 0.02s
2025-03-04 23:01:25,102 - INFO - [VAL] Epoch: 2/30 | Batch: 288/289 (100.0%) | Loss: 0.1007 | Batch time: 0.03s
2025-03-04 23:01:25,606 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 2)
2025-03-04 23:01:25,606 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:01:25,606 - INFO - Epoch 2/30 completed in 79.58s
2025-03-04 23:01:25,606 - INFO - Training   - Loss: 0.9811, Accuracy: 0.7276, F1: 0.7290
2025-03-04 23:01:25,606 - INFO - Validation - Loss: 0.4993, Accuracy: 0.8579, F1: 0.8576
2025-03-04 23:01:25,606 - INFO - Validation F1 improved from 0.8436 to 0.8576
2025-03-04 23:01:25,606 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:01:25,606 - INFO - Epoch 3/30
2025-03-04 23:01:25,606 - INFO - ----------------------------------------
2025-03-04 23:01:26,027 - INFO - [TRAIN] Epoch: 3/30 | Batch: 0/1345 (0.1%) | Loss: 1.4599 | Batch time: 0.15s
2025-03-04 23:01:32,897 - INFO - [TRAIN] Epoch: 3/30 | Batch: 134/1345 (10.0%) | Loss: 1.2979 | Batch time: 0.08s
2025-03-04 23:01:40,257 - INFO - [TRAIN] Epoch: 3/30 | Batch: 268/1345 (20.0%) | Loss: 0.7715 | Batch time: 0.06s
2025-03-04 23:01:47,078 - INFO - [TRAIN] Epoch: 3/30 | Batch: 402/1345 (30.0%) | Loss: 0.8260 | Batch time: 0.06s
2025-03-04 23:01:54,166 - INFO - [TRAIN] Epoch: 3/30 | Batch: 536/1345 (39.9%) | Loss: 0.9722 | Batch time: 0.04s
2025-03-04 23:02:00,929 - INFO - [TRAIN] Epoch: 3/30 | Batch: 670/1345 (49.9%) | Loss: 0.9432 | Batch time: 0.06s
2025-03-04 23:02:07,450 - INFO - [TRAIN] Epoch: 3/30 | Batch: 804/1345 (59.9%) | Loss: 0.8842 | Batch time: 0.05s
2025-03-04 23:02:14,099 - INFO - [TRAIN] Epoch: 3/30 | Batch: 938/1345 (69.8%) | Loss: 0.4951 | Batch time: 0.04s
2025-03-04 23:02:20,284 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1072/1345 (79.8%) | Loss: 1.0415 | Batch time: 0.05s
2025-03-04 23:02:27,402 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1206/1345 (89.7%) | Loss: 0.7976 | Batch time: 0.06s
2025-03-04 23:02:34,537 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1340/1345 (99.7%) | Loss: 0.7629 | Batch time: 0.02s
2025-03-04 23:02:34,704 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1344/1345 (100.0%) | Loss: 0.6285 | Batch time: 0.06s
2025-03-04 23:02:34,943 - INFO - [VAL] Epoch: 3/30 | Batch: 0/289 (0.3%) | Loss: 0.5053 | Batch time: 0.10s
2025-03-04 23:02:35,999 - INFO - [VAL] Epoch: 3/30 | Batch: 28/289 (10.0%) | Loss: 0.5215 | Batch time: 0.05s
2025-03-04 23:02:37,163 - INFO - [VAL] Epoch: 3/30 | Batch: 56/289 (19.7%) | Loss: 0.7081 | Batch time: 0.05s
2025-03-04 23:02:38,328 - INFO - [VAL] Epoch: 3/30 | Batch: 84/289 (29.4%) | Loss: 0.4236 | Batch time: 0.06s
2025-03-04 23:02:39,457 - INFO - [VAL] Epoch: 3/30 | Batch: 112/289 (39.1%) | Loss: 0.4807 | Batch time: 0.04s
2025-03-04 23:02:40,590 - INFO - [VAL] Epoch: 3/30 | Batch: 140/289 (48.8%) | Loss: 0.3287 | Batch time: 0.05s
2025-03-04 23:02:41,744 - INFO - [VAL] Epoch: 3/30 | Batch: 168/289 (58.5%) | Loss: 0.3168 | Batch time: 0.05s
2025-03-04 23:02:42,856 - INFO - [VAL] Epoch: 3/30 | Batch: 196/289 (68.2%) | Loss: 0.3686 | Batch time: 0.05s
2025-03-04 23:02:43,992 - INFO - [VAL] Epoch: 3/30 | Batch: 224/289 (77.9%) | Loss: 0.3899 | Batch time: 0.06s
2025-03-04 23:02:45,159 - INFO - [VAL] Epoch: 3/30 | Batch: 252/289 (87.5%) | Loss: 0.5184 | Batch time: 0.05s
2025-03-04 23:02:46,264 - INFO - [VAL] Epoch: 3/30 | Batch: 280/289 (97.2%) | Loss: 0.7056 | Batch time: 0.05s
2025-03-04 23:02:46,524 - INFO - [VAL] Epoch: 3/30 | Batch: 288/289 (100.0%) | Loss: 0.1362 | Batch time: 0.03s
2025-03-04 23:02:46,973 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 3)
2025-03-04 23:02:46,973 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:02:46,973 - INFO - Epoch 3/30 completed in 81.37s
2025-03-04 23:02:46,973 - INFO - Training   - Loss: 0.9138, Accuracy: 0.7427, F1: 0.7443
2025-03-04 23:02:46,973 - INFO - Validation - Loss: 0.4649, Accuracy: 0.8678, F1: 0.8673
2025-03-04 23:02:46,973 - INFO - Validation F1 improved from 0.8576 to 0.8673
2025-03-04 23:02:46,973 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:02:46,973 - INFO - Epoch 4/30
2025-03-04 23:02:46,973 - INFO - ----------------------------------------
2025-03-04 23:02:47,521 - INFO - [TRAIN] Epoch: 4/30 | Batch: 0/1345 (0.1%) | Loss: 0.5170 | Batch time: 0.14s
2025-03-04 23:02:55,064 - INFO - [TRAIN] Epoch: 4/30 | Batch: 134/1345 (10.0%) | Loss: 0.8240 | Batch time: 0.04s
2025-03-04 23:03:02,649 - INFO - [TRAIN] Epoch: 4/30 | Batch: 268/1345 (20.0%) | Loss: 0.5306 | Batch time: 0.06s
2025-03-04 23:03:10,460 - INFO - [TRAIN] Epoch: 4/30 | Batch: 402/1345 (30.0%) | Loss: 0.7023 | Batch time: 0.06s
2025-03-04 23:03:18,251 - INFO - [TRAIN] Epoch: 4/30 | Batch: 536/1345 (39.9%) | Loss: 1.0635 | Batch time: 0.05s
2025-03-04 23:03:25,349 - INFO - [TRAIN] Epoch: 4/30 | Batch: 670/1345 (49.9%) | Loss: 1.0929 | Batch time: 0.04s
2025-03-04 23:03:32,588 - INFO - [TRAIN] Epoch: 4/30 | Batch: 804/1345 (59.9%) | Loss: 0.9643 | Batch time: 0.03s
2025-03-04 23:03:38,795 - INFO - [TRAIN] Epoch: 4/30 | Batch: 938/1345 (69.8%) | Loss: 0.9448 | Batch time: 0.03s
2025-03-04 23:03:45,840 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1072/1345 (79.8%) | Loss: 1.5471 | Batch time: 0.04s
2025-03-04 23:03:52,940 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8400 | Batch time: 0.03s
2025-03-04 23:04:00,064 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1340/1345 (99.7%) | Loss: 0.5771 | Batch time: 0.06s
2025-03-04 23:04:00,290 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1344/1345 (100.0%) | Loss: 0.9998 | Batch time: 0.05s
2025-03-04 23:04:00,696 - INFO - [VAL] Epoch: 4/30 | Batch: 0/289 (0.3%) | Loss: 0.4176 | Batch time: 0.16s
2025-03-04 23:04:01,815 - INFO - [VAL] Epoch: 4/30 | Batch: 28/289 (10.0%) | Loss: 0.5549 | Batch time: 0.05s
2025-03-04 23:04:02,954 - INFO - [VAL] Epoch: 4/30 | Batch: 56/289 (19.7%) | Loss: 0.6993 | Batch time: 0.05s
2025-03-04 23:04:04,138 - INFO - [VAL] Epoch: 4/30 | Batch: 84/289 (29.4%) | Loss: 0.3810 | Batch time: 0.05s
2025-03-04 23:04:05,301 - INFO - [VAL] Epoch: 4/30 | Batch: 112/289 (39.1%) | Loss: 0.4155 | Batch time: 0.04s
2025-03-04 23:04:06,439 - INFO - [VAL] Epoch: 4/30 | Batch: 140/289 (48.8%) | Loss: 0.3546 | Batch time: 0.05s
2025-03-04 23:04:07,571 - INFO - [VAL] Epoch: 4/30 | Batch: 168/289 (58.5%) | Loss: 0.3805 | Batch time: 0.04s
2025-03-04 23:04:08,711 - INFO - [VAL] Epoch: 4/30 | Batch: 196/289 (68.2%) | Loss: 0.2839 | Batch time: 0.05s
2025-03-04 23:04:09,846 - INFO - [VAL] Epoch: 4/30 | Batch: 224/289 (77.9%) | Loss: 0.3792 | Batch time: 0.05s
2025-03-04 23:04:10,995 - INFO - [VAL] Epoch: 4/30 | Batch: 252/289 (87.5%) | Loss: 0.5747 | Batch time: 0.06s
2025-03-04 23:04:12,102 - INFO - [VAL] Epoch: 4/30 | Batch: 280/289 (97.2%) | Loss: 0.4921 | Batch time: 0.05s
2025-03-04 23:04:12,371 - INFO - [VAL] Epoch: 4/30 | Batch: 288/289 (100.0%) | Loss: 0.1514 | Batch time: 0.03s
2025-03-04 23:04:12,381 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:04:12,381 - INFO - Epoch 4/30 completed in 85.41s
2025-03-04 23:04:12,381 - INFO - Training   - Loss: 0.8897, Accuracy: 0.7453, F1: 0.7470
2025-03-04 23:04:12,382 - INFO - Validation - Loss: 0.4493, Accuracy: 0.8665, F1: 0.8668
2025-03-04 23:04:12,382 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:04:12,382 - INFO - Epoch 5/30
2025-03-04 23:04:12,382 - INFO - ----------------------------------------
2025-03-04 23:04:12,781 - INFO - [TRAIN] Epoch: 5/30 | Batch: 0/1345 (0.1%) | Loss: 0.5683 | Batch time: 0.06s
2025-03-04 23:04:20,352 - INFO - [TRAIN] Epoch: 5/30 | Batch: 134/1345 (10.0%) | Loss: 0.8413 | Batch time: 0.06s
2025-03-04 23:04:27,980 - INFO - [TRAIN] Epoch: 5/30 | Batch: 268/1345 (20.0%) | Loss: 0.7830 | Batch time: 0.04s
2025-03-04 23:04:36,241 - INFO - [TRAIN] Epoch: 5/30 | Batch: 402/1345 (30.0%) | Loss: 1.2391 | Batch time: 0.05s
2025-03-04 23:04:43,439 - INFO - [TRAIN] Epoch: 5/30 | Batch: 536/1345 (39.9%) | Loss: 0.6116 | Batch time: 0.07s
2025-03-04 23:04:51,180 - INFO - [TRAIN] Epoch: 5/30 | Batch: 670/1345 (49.9%) | Loss: 0.4971 | Batch time: 0.04s
2025-03-04 23:04:57,001 - INFO - [TRAIN] Epoch: 5/30 | Batch: 804/1345 (59.9%) | Loss: 0.7910 | Batch time: 0.07s
2025-03-04 23:05:04,232 - INFO - [TRAIN] Epoch: 5/30 | Batch: 938/1345 (69.8%) | Loss: 1.1177 | Batch time: 0.03s
2025-03-04 23:05:11,392 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1072/1345 (79.8%) | Loss: 0.7626 | Batch time: 0.06s
2025-03-04 23:05:18,434 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1206/1345 (89.7%) | Loss: 0.9893 | Batch time: 0.03s
2025-03-04 23:05:25,576 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1340/1345 (99.7%) | Loss: 1.1334 | Batch time: 0.04s
2025-03-04 23:05:25,815 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1344/1345 (100.0%) | Loss: 0.8186 | Batch time: 0.06s
2025-03-04 23:05:26,098 - INFO - [VAL] Epoch: 5/30 | Batch: 0/289 (0.3%) | Loss: 0.2963 | Batch time: 0.10s
2025-03-04 23:05:27,225 - INFO - [VAL] Epoch: 5/30 | Batch: 28/289 (10.0%) | Loss: 0.4649 | Batch time: 0.04s
2025-03-04 23:05:28,416 - INFO - [VAL] Epoch: 5/30 | Batch: 56/289 (19.7%) | Loss: 0.6394 | Batch time: 0.05s
2025-03-04 23:05:29,574 - INFO - [VAL] Epoch: 5/30 | Batch: 84/289 (29.4%) | Loss: 0.3448 | Batch time: 0.04s
2025-03-04 23:05:30,716 - INFO - [VAL] Epoch: 5/30 | Batch: 112/289 (39.1%) | Loss: 0.3616 | Batch time: 0.05s
2025-03-04 23:05:31,820 - INFO - [VAL] Epoch: 5/30 | Batch: 140/289 (48.8%) | Loss: 0.2490 | Batch time: 0.05s
2025-03-04 23:05:32,961 - INFO - [VAL] Epoch: 5/30 | Batch: 168/289 (58.5%) | Loss: 0.3063 | Batch time: 0.04s
2025-03-04 23:05:34,063 - INFO - [VAL] Epoch: 5/30 | Batch: 196/289 (68.2%) | Loss: 0.2675 | Batch time: 0.04s
2025-03-04 23:05:35,160 - INFO - [VAL] Epoch: 5/30 | Batch: 224/289 (77.9%) | Loss: 0.3555 | Batch time: 0.05s
2025-03-04 23:05:36,253 - INFO - [VAL] Epoch: 5/30 | Batch: 252/289 (87.5%) | Loss: 0.3881 | Batch time: 0.05s
2025-03-04 23:05:37,331 - INFO - [VAL] Epoch: 5/30 | Batch: 280/289 (97.2%) | Loss: 0.4451 | Batch time: 0.03s
2025-03-04 23:05:37,612 - INFO - [VAL] Epoch: 5/30 | Batch: 288/289 (100.0%) | Loss: 0.1004 | Batch time: 0.02s
2025-03-04 23:05:38,171 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 5)
2025-03-04 23:05:38,171 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:05:38,171 - INFO - Epoch 5/30 completed in 85.79s
2025-03-04 23:05:38,171 - INFO - Training   - Loss: 0.8705, Accuracy: 0.7471, F1: 0.7490
2025-03-04 23:05:38,171 - INFO - Validation - Loss: 0.3886, Accuracy: 0.8833, F1: 0.8833
2025-03-04 23:05:38,171 - INFO - Validation F1 improved from 0.8673 to 0.8833
2025-03-04 23:05:38,171 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:05:38,396 - INFO - Checkpoint saved: checkpoint_epoch_5.pth (Epoch 5)
2025-03-04 23:05:38,396 - INFO - Epoch 6/30
2025-03-04 23:05:38,396 - INFO - ----------------------------------------
2025-03-04 23:05:38,775 - INFO - [TRAIN] Epoch: 6/30 | Batch: 0/1345 (0.1%) | Loss: 1.6057 | Batch time: 0.05s
2025-03-04 23:05:46,062 - INFO - [TRAIN] Epoch: 6/30 | Batch: 134/1345 (10.0%) | Loss: 0.4701 | Batch time: 0.03s
2025-03-04 23:05:53,147 - INFO - [TRAIN] Epoch: 6/30 | Batch: 268/1345 (20.0%) | Loss: 0.9454 | Batch time: 0.06s
2025-03-04 23:06:00,176 - INFO - [TRAIN] Epoch: 6/30 | Batch: 402/1345 (30.0%) | Loss: 1.0419 | Batch time: 0.04s
2025-03-04 23:06:07,041 - INFO - [TRAIN] Epoch: 6/30 | Batch: 536/1345 (39.9%) | Loss: 1.0854 | Batch time: 0.06s
2025-03-04 23:06:13,413 - INFO - [TRAIN] Epoch: 6/30 | Batch: 670/1345 (49.9%) | Loss: 0.4328 | Batch time: 0.03s
2025-03-04 23:06:19,433 - INFO - [TRAIN] Epoch: 6/30 | Batch: 804/1345 (59.9%) | Loss: 0.8678 | Batch time: 0.05s
2025-03-04 23:06:26,626 - INFO - [TRAIN] Epoch: 6/30 | Batch: 938/1345 (69.8%) | Loss: 0.4714 | Batch time: 0.05s
2025-03-04 23:06:33,906 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1072/1345 (79.8%) | Loss: 1.7804 | Batch time: 0.09s
2025-03-04 23:06:41,867 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8512 | Batch time: 0.07s
2025-03-04 23:06:49,604 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1340/1345 (99.7%) | Loss: 0.6480 | Batch time: 0.06s
2025-03-04 23:06:49,848 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1344/1345 (100.0%) | Loss: 1.0654 | Batch time: 0.06s
2025-03-04 23:06:50,178 - INFO - [VAL] Epoch: 6/30 | Batch: 0/289 (0.3%) | Loss: 0.3081 | Batch time: 0.08s
2025-03-04 23:06:51,374 - INFO - [VAL] Epoch: 6/30 | Batch: 28/289 (10.0%) | Loss: 0.4896 | Batch time: 0.04s
2025-03-04 23:06:52,513 - INFO - [VAL] Epoch: 6/30 | Batch: 56/289 (19.7%) | Loss: 0.6780 | Batch time: 0.03s
2025-03-04 23:06:53,653 - INFO - [VAL] Epoch: 6/30 | Batch: 84/289 (29.4%) | Loss: 0.2754 | Batch time: 0.05s
2025-03-04 23:06:54,935 - INFO - [VAL] Epoch: 6/30 | Batch: 112/289 (39.1%) | Loss: 0.2949 | Batch time: 0.05s
2025-03-04 23:06:56,050 - INFO - [VAL] Epoch: 6/30 | Batch: 140/289 (48.8%) | Loss: 0.2517 | Batch time: 0.04s
2025-03-04 23:06:57,197 - INFO - [VAL] Epoch: 6/30 | Batch: 168/289 (58.5%) | Loss: 0.3125 | Batch time: 0.05s
2025-03-04 23:06:58,367 - INFO - [VAL] Epoch: 6/30 | Batch: 196/289 (68.2%) | Loss: 0.2894 | Batch time: 0.06s
2025-03-04 23:06:59,555 - INFO - [VAL] Epoch: 6/30 | Batch: 224/289 (77.9%) | Loss: 0.3491 | Batch time: 0.06s
2025-03-04 23:07:00,676 - INFO - [VAL] Epoch: 6/30 | Batch: 252/289 (87.5%) | Loss: 0.3209 | Batch time: 0.05s
2025-03-04 23:07:01,774 - INFO - [VAL] Epoch: 6/30 | Batch: 280/289 (97.2%) | Loss: 0.4703 | Batch time: 0.05s
2025-03-04 23:07:02,145 - INFO - [VAL] Epoch: 6/30 | Batch: 288/289 (100.0%) | Loss: 0.2445 | Batch time: 0.03s
2025-03-04 23:07:02,610 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 6)
2025-03-04 23:07:02,611 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:07:02,611 - INFO - Epoch 6/30 completed in 84.21s
2025-03-04 23:07:02,611 - INFO - Training   - Loss: 0.8483, Accuracy: 0.7548, F1: 0.7563
2025-03-04 23:07:02,611 - INFO - Validation - Loss: 0.3712, Accuracy: 0.8884, F1: 0.8881
2025-03-04 23:07:02,611 - INFO - Validation F1 improved from 0.8833 to 0.8881
2025-03-04 23:07:02,611 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:07:02,611 - INFO - Epoch 7/30
2025-03-04 23:07:02,611 - INFO - ----------------------------------------
2025-03-04 23:07:03,412 - INFO - [TRAIN] Epoch: 7/30 | Batch: 0/1345 (0.1%) | Loss: 0.6206 | Batch time: 0.16s
2025-03-04 23:07:11,663 - INFO - [TRAIN] Epoch: 7/30 | Batch: 134/1345 (10.0%) | Loss: 0.6926 | Batch time: 0.07s
2025-03-04 23:07:19,867 - INFO - [TRAIN] Epoch: 7/30 | Batch: 268/1345 (20.0%) | Loss: 0.5174 | Batch time: 0.06s
2025-03-04 23:07:28,205 - INFO - [TRAIN] Epoch: 7/30 | Batch: 402/1345 (30.0%) | Loss: 0.6577 | Batch time: 0.05s
2025-03-04 23:07:36,270 - INFO - [TRAIN] Epoch: 7/30 | Batch: 536/1345 (39.9%) | Loss: 0.7732 | Batch time: 0.03s
2025-03-04 23:07:43,537 - INFO - [TRAIN] Epoch: 7/30 | Batch: 670/1345 (49.9%) | Loss: 1.1745 | Batch time: 0.04s
2025-03-04 23:07:50,938 - INFO - [TRAIN] Epoch: 7/30 | Batch: 804/1345 (59.9%) | Loss: 0.7732 | Batch time: 0.04s
2025-03-04 23:07:58,643 - INFO - [TRAIN] Epoch: 7/30 | Batch: 938/1345 (69.8%) | Loss: 0.8088 | Batch time: 0.06s
2025-03-04 23:08:06,672 - INFO - [TRAIN] Epoch: 7/30 | Batch: 1072/1345 (79.8%) | Loss: 0.5663 | Batch time: 0.07s
2025-03-04 23:08:14,270 - INFO - [TRAIN] Epoch: 7/30 | Batch: 1206/1345 (89.7%) | Loss: 1.1468 | Batch time: 0.05s
2025-03-04 23:08:22,445 - INFO - [TRAIN] Epoch: 7/30 | Batch: 1340/1345 (99.7%) | Loss: 0.7335 | Batch time: 0.06s
2025-03-04 23:08:22,676 - INFO - [TRAIN] Epoch: 7/30 | Batch: 1344/1345 (100.0%) | Loss: 0.9181 | Batch time: 0.05s
2025-03-04 23:08:22,953 - INFO - [VAL] Epoch: 7/30 | Batch: 0/289 (0.3%) | Loss: 0.3010 | Batch time: 0.07s
2025-03-04 23:08:24,066 - INFO - [VAL] Epoch: 7/30 | Batch: 28/289 (10.0%) | Loss: 0.4158 | Batch time: 0.05s
2025-03-04 23:08:25,243 - INFO - [VAL] Epoch: 7/30 | Batch: 56/289 (19.7%) | Loss: 0.8049 | Batch time: 0.06s
2025-03-04 23:08:26,426 - INFO - [VAL] Epoch: 7/30 | Batch: 84/289 (29.4%) | Loss: 0.3480 | Batch time: 0.05s
2025-03-04 23:08:27,542 - INFO - [VAL] Epoch: 7/30 | Batch: 112/289 (39.1%) | Loss: 0.4078 | Batch time: 0.05s
2025-03-04 23:08:28,657 - INFO - [VAL] Epoch: 7/30 | Batch: 140/289 (48.8%) | Loss: 0.2280 | Batch time: 0.04s
2025-03-04 23:08:29,859 - INFO - [VAL] Epoch: 7/30 | Batch: 168/289 (58.5%) | Loss: 0.3574 | Batch time: 0.05s
2025-03-04 23:08:30,960 - INFO - [VAL] Epoch: 7/30 | Batch: 196/289 (68.2%) | Loss: 0.3785 | Batch time: 0.05s
2025-03-04 23:08:32,073 - INFO - [VAL] Epoch: 7/30 | Batch: 224/289 (77.9%) | Loss: 0.3278 | Batch time: 0.04s
2025-03-04 23:08:33,210 - INFO - [VAL] Epoch: 7/30 | Batch: 252/289 (87.5%) | Loss: 0.4835 | Batch time: 0.05s
2025-03-04 23:08:34,321 - INFO - [VAL] Epoch: 7/30 | Batch: 280/289 (97.2%) | Loss: 0.4195 | Batch time: 0.02s
2025-03-04 23:08:34,634 - INFO - [VAL] Epoch: 7/30 | Batch: 288/289 (100.0%) | Loss: 0.0768 | Batch time: 0.03s
2025-03-04 23:08:34,642 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:08:34,643 - INFO - Epoch 7/30 completed in 92.03s
2025-03-04 23:08:34,643 - INFO - Training   - Loss: 0.8527, Accuracy: 0.7502, F1: 0.7520
2025-03-04 23:08:34,643 - INFO - Validation - Loss: 0.4275, Accuracy: 0.8761, F1: 0.8750
2025-03-04 23:08:34,643 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:08:34,643 - INFO - Epoch 8/30
2025-03-04 23:08:34,643 - INFO - ----------------------------------------
2025-03-04 23:08:35,319 - INFO - [TRAIN] Epoch: 8/30 | Batch: 0/1345 (0.1%) | Loss: 1.2299 | Batch time: 0.20s
2025-03-04 23:08:42,801 - INFO - [TRAIN] Epoch: 8/30 | Batch: 134/1345 (10.0%) | Loss: 1.6589 | Batch time: 0.07s
2025-03-04 23:08:50,740 - INFO - [TRAIN] Epoch: 8/30 | Batch: 268/1345 (20.0%) | Loss: 0.5685 | Batch time: 0.06s
2025-03-04 23:08:58,201 - INFO - [TRAIN] Epoch: 8/30 | Batch: 402/1345 (30.0%) | Loss: 1.7521 | Batch time: 0.07s
2025-03-04 23:09:05,417 - INFO - [TRAIN] Epoch: 8/30 | Batch: 536/1345 (39.9%) | Loss: 0.9800 | Batch time: 0.06s
2025-03-04 23:09:13,111 - INFO - [TRAIN] Epoch: 8/30 | Batch: 670/1345 (49.9%) | Loss: 0.3789 | Batch time: 0.04s
2025-03-04 23:09:21,190 - INFO - [TRAIN] Epoch: 8/30 | Batch: 804/1345 (59.9%) | Loss: 0.5961 | Batch time: 0.06s
2025-03-04 23:09:29,145 - INFO - [TRAIN] Epoch: 8/30 | Batch: 938/1345 (69.8%) | Loss: 0.3870 | Batch time: 0.04s
2025-03-04 23:09:37,192 - INFO - [TRAIN] Epoch: 8/30 | Batch: 1072/1345 (79.8%) | Loss: 0.5546 | Batch time: 0.06s
2025-03-04 23:09:44,858 - INFO - [TRAIN] Epoch: 8/30 | Batch: 1206/1345 (89.7%) | Loss: 0.7892 | Batch time: 0.06s
2025-03-04 23:09:52,495 - INFO - [TRAIN] Epoch: 8/30 | Batch: 1340/1345 (99.7%) | Loss: 0.6675 | Batch time: 0.04s
2025-03-04 23:09:52,655 - INFO - [TRAIN] Epoch: 8/30 | Batch: 1344/1345 (100.0%) | Loss: 0.8512 | Batch time: 0.06s
2025-03-04 23:09:52,933 - INFO - [VAL] Epoch: 8/30 | Batch: 0/289 (0.3%) | Loss: 0.2769 | Batch time: 0.08s
2025-03-04 23:09:54,125 - INFO - [VAL] Epoch: 8/30 | Batch: 28/289 (10.0%) | Loss: 0.4506 | Batch time: 0.05s
2025-03-04 23:09:55,257 - INFO - [VAL] Epoch: 8/30 | Batch: 56/289 (19.7%) | Loss: 0.7595 | Batch time: 0.06s
2025-03-04 23:09:56,395 - INFO - [VAL] Epoch: 8/30 | Batch: 84/289 (29.4%) | Loss: 0.2987 | Batch time: 0.05s
2025-03-04 23:09:57,524 - INFO - [VAL] Epoch: 8/30 | Batch: 112/289 (39.1%) | Loss: 0.3826 | Batch time: 0.05s
2025-03-04 23:09:58,631 - INFO - [VAL] Epoch: 8/30 | Batch: 140/289 (48.8%) | Loss: 0.2294 | Batch time: 0.05s
2025-03-04 23:09:59,759 - INFO - [VAL] Epoch: 8/30 | Batch: 168/289 (58.5%) | Loss: 0.3074 | Batch time: 0.06s
2025-03-04 23:10:00,869 - INFO - [VAL] Epoch: 8/30 | Batch: 196/289 (68.2%) | Loss: 0.3015 | Batch time: 0.05s
2025-03-04 23:10:02,028 - INFO - [VAL] Epoch: 8/30 | Batch: 224/289 (77.9%) | Loss: 0.3039 | Batch time: 0.05s
2025-03-04 23:10:03,168 - INFO - [VAL] Epoch: 8/30 | Batch: 252/289 (87.5%) | Loss: 0.4305 | Batch time: 0.05s
2025-03-04 23:10:04,310 - INFO - [VAL] Epoch: 8/30 | Batch: 280/289 (97.2%) | Loss: 0.3953 | Batch time: 0.03s
2025-03-04 23:10:04,599 - INFO - [VAL] Epoch: 8/30 | Batch: 288/289 (100.0%) | Loss: 0.1044 | Batch time: 0.03s
2025-03-04 23:10:04,607 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:10:04,607 - INFO - Epoch 8/30 completed in 89.96s
2025-03-04 23:10:04,607 - INFO - Training   - Loss: 0.8270, Accuracy: 0.7590, F1: 0.7607
2025-03-04 23:10:04,607 - INFO - Validation - Loss: 0.3963, Accuracy: 0.8829, F1: 0.8821
2025-03-04 23:10:04,607 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:10:04,607 - INFO - Epoch 9/30
2025-03-04 23:10:04,607 - INFO - ----------------------------------------
2025-03-04 23:10:05,184 - INFO - [TRAIN] Epoch: 9/30 | Batch: 0/1345 (0.1%) | Loss: 0.7318 | Batch time: 0.11s
2025-03-04 23:10:13,877 - INFO - [TRAIN] Epoch: 9/30 | Batch: 134/1345 (10.0%) | Loss: 0.8188 | Batch time: 0.06s
2025-03-04 23:10:19,507 - INFO - [TRAIN] Epoch: 9/30 | Batch: 268/1345 (20.0%) | Loss: 0.6009 | Batch time: 0.04s
2025-03-04 23:10:26,018 - INFO - [TRAIN] Epoch: 9/30 | Batch: 402/1345 (30.0%) | Loss: 0.5787 | Batch time: 0.03s
2025-03-04 23:10:33,171 - INFO - [TRAIN] Epoch: 9/30 | Batch: 536/1345 (39.9%) | Loss: 0.9318 | Batch time: 0.04s
2025-03-04 23:10:40,075 - INFO - [TRAIN] Epoch: 9/30 | Batch: 670/1345 (49.9%) | Loss: 1.0464 | Batch time: 0.06s
2025-03-04 23:10:47,007 - INFO - [TRAIN] Epoch: 9/30 | Batch: 804/1345 (59.9%) | Loss: 1.4498 | Batch time: 0.03s
2025-03-04 23:10:53,901 - INFO - [TRAIN] Epoch: 9/30 | Batch: 938/1345 (69.8%) | Loss: 0.3914 | Batch time: 0.04s
2025-03-04 23:11:00,842 - INFO - [TRAIN] Epoch: 9/30 | Batch: 1072/1345 (79.8%) | Loss: 1.3777 | Batch time: 0.06s
2025-03-04 23:11:08,159 - INFO - [TRAIN] Epoch: 9/30 | Batch: 1206/1345 (89.7%) | Loss: 0.5774 | Batch time: 0.03s
2025-03-04 23:11:15,196 - INFO - [TRAIN] Epoch: 9/30 | Batch: 1340/1345 (99.7%) | Loss: 0.4987 | Batch time: 0.07s
2025-03-04 23:11:15,425 - INFO - [TRAIN] Epoch: 9/30 | Batch: 1344/1345 (100.0%) | Loss: 0.8899 | Batch time: 0.05s
2025-03-04 23:11:15,739 - INFO - [VAL] Epoch: 9/30 | Batch: 0/289 (0.3%) | Loss: 0.2643 | Batch time: 0.08s
2025-03-04 23:11:16,824 - INFO - [VAL] Epoch: 9/30 | Batch: 28/289 (10.0%) | Loss: 0.4144 | Batch time: 0.04s
2025-03-04 23:11:17,922 - INFO - [VAL] Epoch: 9/30 | Batch: 56/289 (19.7%) | Loss: 0.7225 | Batch time: 0.05s
2025-03-04 23:11:19,053 - INFO - [VAL] Epoch: 9/30 | Batch: 84/289 (29.4%) | Loss: 0.2403 | Batch time: 0.05s
2025-03-04 23:11:20,138 - INFO - [VAL] Epoch: 9/30 | Batch: 112/289 (39.1%) | Loss: 0.3237 | Batch time: 0.05s
2025-03-04 23:11:21,278 - INFO - [VAL] Epoch: 9/30 | Batch: 140/289 (48.8%) | Loss: 0.1958 | Batch time: 0.05s
2025-03-04 23:11:22,361 - INFO - [VAL] Epoch: 9/30 | Batch: 168/289 (58.5%) | Loss: 0.2984 | Batch time: 0.03s
2025-03-04 23:11:23,435 - INFO - [VAL] Epoch: 9/30 | Batch: 196/289 (68.2%) | Loss: 0.2440 | Batch time: 0.02s
2025-03-04 23:11:24,024 - INFO - [VAL] Epoch: 9/30 | Batch: 224/289 (77.9%) | Loss: 0.2810 | Batch time: 0.03s
2025-03-04 23:11:24,724 - INFO - [VAL] Epoch: 9/30 | Batch: 252/289 (87.5%) | Loss: 0.3353 | Batch time: 0.03s
2025-03-04 23:11:25,680 - INFO - [VAL] Epoch: 9/30 | Batch: 280/289 (97.2%) | Loss: 0.3640 | Batch time: 0.05s
2025-03-04 23:11:26,048 - INFO - [VAL] Epoch: 9/30 | Batch: 288/289 (100.0%) | Loss: 0.1201 | Batch time: 0.04s
2025-03-04 23:11:26,667 - INFO - Checkpoint saved: efficientnet_b0_v1_best.pth (Epoch 9)
2025-03-04 23:11:26,668 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:11:26,668 - INFO - Epoch 9/30 completed in 82.06s
2025-03-04 23:11:26,668 - INFO - Training   - Loss: 0.8172, Accuracy: 0.7635, F1: 0.7649
2025-03-04 23:11:26,668 - INFO - Validation - Loss: 0.3412, Accuracy: 0.9000, F1: 0.8999
2025-03-04 23:11:26,668 - INFO - Validation F1 improved from 0.8881 to 0.8999
2025-03-04 23:11:26,668 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:11:26,668 - INFO - Epoch 10/30
2025-03-04 23:11:26,668 - INFO - ----------------------------------------
2025-03-04 23:11:27,442 - INFO - [TRAIN] Epoch: 10/30 | Batch: 0/1345 (0.1%) | Loss: 0.8564 | Batch time: 0.11s
2025-03-04 23:11:34,430 - INFO - [TRAIN] Epoch: 10/30 | Batch: 134/1345 (10.0%) | Loss: 0.8324 | Batch time: 0.03s
2025-03-04 23:11:41,020 - INFO - [TRAIN] Epoch: 10/30 | Batch: 268/1345 (20.0%) | Loss: 0.8343 | Batch time: 0.03s
2025-03-04 23:11:47,925 - INFO - [TRAIN] Epoch: 10/30 | Batch: 402/1345 (30.0%) | Loss: 0.8563 | Batch time: 0.06s
2025-03-04 23:11:55,049 - INFO - [TRAIN] Epoch: 10/30 | Batch: 536/1345 (39.9%) | Loss: 0.9414 | Batch time: 0.06s
2025-03-04 23:12:02,170 - INFO - [TRAIN] Epoch: 10/30 | Batch: 670/1345 (49.9%) | Loss: 1.3668 | Batch time: 0.06s
2025-03-04 23:12:09,309 - INFO - [TRAIN] Epoch: 10/30 | Batch: 804/1345 (59.9%) | Loss: 0.9424 | Batch time: 0.05s
2025-03-04 23:12:16,490 - INFO - [TRAIN] Epoch: 10/30 | Batch: 938/1345 (69.8%) | Loss: 0.8921 | Batch time: 0.06s
2025-03-04 23:12:23,861 - INFO - [TRAIN] Epoch: 10/30 | Batch: 1072/1345 (79.8%) | Loss: 0.6076 | Batch time: 0.04s
2025-03-04 23:12:31,484 - INFO - [TRAIN] Epoch: 10/30 | Batch: 1206/1345 (89.7%) | Loss: 0.9520 | Batch time: 0.04s
2025-03-04 23:12:38,628 - INFO - [TRAIN] Epoch: 10/30 | Batch: 1340/1345 (99.7%) | Loss: 0.8379 | Batch time: 0.02s
2025-03-04 23:12:38,810 - INFO - [TRAIN] Epoch: 10/30 | Batch: 1344/1345 (100.0%) | Loss: 0.5311 | Batch time: 0.06s
2025-03-04 23:12:39,076 - INFO - [VAL] Epoch: 10/30 | Batch: 0/289 (0.3%) | Loss: 0.2794 | Batch time: 0.07s
2025-03-04 23:12:40,150 - INFO - [VAL] Epoch: 10/30 | Batch: 28/289 (10.0%) | Loss: 0.5045 | Batch time: 0.04s
2025-03-04 23:12:40,717 - INFO - [VAL] Epoch: 10/30 | Batch: 56/289 (19.7%) | Loss: 0.7729 | Batch time: 0.04s
2025-03-04 23:12:41,453 - INFO - [VAL] Epoch: 10/30 | Batch: 84/289 (29.4%) | Loss: 0.3400 | Batch time: 0.02s
2025-03-04 23:12:42,604 - INFO - [VAL] Epoch: 10/30 | Batch: 112/289 (39.1%) | Loss: 0.4174 | Batch time: 0.05s
2025-03-04 23:12:43,838 - INFO - [VAL] Epoch: 10/30 | Batch: 140/289 (48.8%) | Loss: 0.2363 | Batch time: 0.04s
2025-03-04 23:12:45,100 - INFO - [VAL] Epoch: 10/30 | Batch: 168/289 (58.5%) | Loss: 0.3371 | Batch time: 0.05s
2025-03-04 23:12:46,329 - INFO - [VAL] Epoch: 10/30 | Batch: 196/289 (68.2%) | Loss: 0.2928 | Batch time: 0.05s
2025-03-04 23:12:47,534 - INFO - [VAL] Epoch: 10/30 | Batch: 224/289 (77.9%) | Loss: 0.3154 | Batch time: 0.05s
2025-03-04 23:12:48,755 - INFO - [VAL] Epoch: 10/30 | Batch: 252/289 (87.5%) | Loss: 0.4853 | Batch time: 0.02s
2025-03-04 23:12:49,302 - INFO - [VAL] Epoch: 10/30 | Batch: 280/289 (97.2%) | Loss: 0.4571 | Batch time: 0.02s
2025-03-04 23:12:49,453 - INFO - [VAL] Epoch: 10/30 | Batch: 288/289 (100.0%) | Loss: 0.0964 | Batch time: 0.02s
2025-03-04 23:12:49,461 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:12:49,461 - INFO - Epoch 10/30 completed in 82.79s
2025-03-04 23:12:49,461 - INFO - Training   - Loss: 0.8155, Accuracy: 0.7631, F1: 0.7644
2025-03-04 23:12:49,461 - INFO - Validation - Loss: 0.3977, Accuracy: 0.8840, F1: 0.8837
2025-03-04 23:12:49,461 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:12:49,570 - INFO - Checkpoint saved: checkpoint_epoch_10.pth (Epoch 10)
2025-03-04 23:12:49,570 - INFO - Epoch 11/30
2025-03-04 23:12:49,570 - INFO - ----------------------------------------
2025-03-04 23:12:49,907 - INFO - [TRAIN] Epoch: 11/30 | Batch: 0/1345 (0.1%) | Loss: 1.2153 | Batch time: 0.04s
2025-03-04 23:12:56,505 - INFO - [TRAIN] Epoch: 11/30 | Batch: 134/1345 (10.0%) | Loss: 0.6850 | Batch time: 0.08s
2025-03-04 23:13:03,366 - INFO - [TRAIN] Epoch: 11/30 | Batch: 268/1345 (20.0%) | Loss: 1.3681 | Batch time: 0.06s
2025-03-04 23:13:10,179 - INFO - [TRAIN] Epoch: 11/30 | Batch: 402/1345 (30.0%) | Loss: 0.8983 | Batch time: 0.07s
2025-03-04 23:13:17,248 - INFO - [TRAIN] Epoch: 11/30 | Batch: 536/1345 (39.9%) | Loss: 0.5823 | Batch time: 0.08s
2025-03-04 23:13:24,306 - INFO - [TRAIN] Epoch: 11/30 | Batch: 670/1345 (49.9%) | Loss: 0.5837 | Batch time: 0.04s
2025-03-04 23:13:31,697 - INFO - [TRAIN] Epoch: 11/30 | Batch: 804/1345 (59.9%) | Loss: 0.5790 | Batch time: 0.03s
2025-03-04 23:13:38,744 - INFO - [TRAIN] Epoch: 11/30 | Batch: 938/1345 (69.8%) | Loss: 0.7676 | Batch time: 0.07s
2025-03-04 23:13:46,539 - INFO - [TRAIN] Epoch: 11/30 | Batch: 1072/1345 (79.8%) | Loss: 0.6857 | Batch time: 0.06s
2025-03-04 23:13:53,866 - INFO - [TRAIN] Epoch: 11/30 | Batch: 1206/1345 (89.7%) | Loss: 0.6994 | Batch time: 0.06s
2025-03-04 23:14:00,564 - INFO - [TRAIN] Epoch: 11/30 | Batch: 1340/1345 (99.7%) | Loss: 1.1913 | Batch time: 0.06s
2025-03-04 23:14:00,785 - INFO - [TRAIN] Epoch: 11/30 | Batch: 1344/1345 (100.0%) | Loss: 1.0026 | Batch time: 0.05s
2025-03-04 23:14:01,115 - INFO - [VAL] Epoch: 11/30 | Batch: 0/289 (0.3%) | Loss: 0.2956 | Batch time: 0.07s
2025-03-04 23:14:02,364 - INFO - [VAL] Epoch: 11/30 | Batch: 28/289 (10.0%) | Loss: 0.4998 | Batch time: 0.05s
2025-03-04 23:14:03,638 - INFO - [VAL] Epoch: 11/30 | Batch: 56/289 (19.7%) | Loss: 0.8175 | Batch time: 0.05s
2025-03-04 23:14:04,893 - INFO - [VAL] Epoch: 11/30 | Batch: 84/289 (29.4%) | Loss: 0.3982 | Batch time: 0.04s
2025-03-04 23:14:05,897 - INFO - [VAL] Epoch: 11/30 | Batch: 112/289 (39.1%) | Loss: 0.3889 | Batch time: 0.02s
2025-03-04 23:14:06,448 - INFO - [VAL] Epoch: 11/30 | Batch: 140/289 (48.8%) | Loss: 0.2202 | Batch time: 0.04s
2025-03-04 23:14:07,009 - INFO - [VAL] Epoch: 11/30 | Batch: 168/289 (58.5%) | Loss: 0.3219 | Batch time: 0.02s
2025-03-04 23:14:07,682 - INFO - [VAL] Epoch: 11/30 | Batch: 196/289 (68.2%) | Loss: 0.2974 | Batch time: 0.02s
2025-03-04 23:14:08,310 - INFO - [VAL] Epoch: 11/30 | Batch: 224/289 (77.9%) | Loss: 0.3256 | Batch time: 0.02s
2025-03-04 23:14:09,069 - INFO - [VAL] Epoch: 11/30 | Batch: 252/289 (87.5%) | Loss: 0.5136 | Batch time: 0.02s
2025-03-04 23:14:09,933 - INFO - [VAL] Epoch: 11/30 | Batch: 280/289 (97.2%) | Loss: 0.4719 | Batch time: 0.04s
2025-03-04 23:14:10,206 - INFO - [VAL] Epoch: 11/30 | Batch: 288/289 (100.0%) | Loss: 0.1367 | Batch time: 0.03s
2025-03-04 23:14:10,215 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:14:10,216 - INFO - Epoch 11/30 completed in 80.65s
2025-03-04 23:14:10,216 - INFO - Training   - Loss: 0.8033, Accuracy: 0.7645, F1: 0.7660
2025-03-04 23:14:10,216 - INFO - Validation - Loss: 0.3916, Accuracy: 0.8846, F1: 0.8849
2025-03-04 23:14:10,216 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:14:10,216 - INFO - Epoch 12/30
2025-03-04 23:14:10,216 - INFO - ----------------------------------------
2025-03-04 23:14:10,607 - INFO - [TRAIN] Epoch: 12/30 | Batch: 0/1345 (0.1%) | Loss: 0.4885 | Batch time: 0.03s
2025-03-04 23:14:17,730 - INFO - [TRAIN] Epoch: 12/30 | Batch: 134/1345 (10.0%) | Loss: 1.2247 | Batch time: 0.06s
2025-03-04 23:14:24,580 - INFO - [TRAIN] Epoch: 12/30 | Batch: 268/1345 (20.0%) | Loss: 0.8293 | Batch time: 0.05s
2025-03-04 23:14:31,523 - INFO - [TRAIN] Epoch: 12/30 | Batch: 402/1345 (30.0%) | Loss: 0.9704 | Batch time: 0.06s
2025-03-04 23:14:38,474 - INFO - [TRAIN] Epoch: 12/30 | Batch: 536/1345 (39.9%) | Loss: 0.7098 | Batch time: 0.06s
2025-03-04 23:14:45,770 - INFO - [TRAIN] Epoch: 12/30 | Batch: 670/1345 (49.9%) | Loss: 0.8615 | Batch time: 0.06s
2025-03-04 23:14:53,429 - INFO - [TRAIN] Epoch: 12/30 | Batch: 804/1345 (59.9%) | Loss: 1.5003 | Batch time: 0.03s
2025-03-04 23:15:00,544 - INFO - [TRAIN] Epoch: 12/30 | Batch: 938/1345 (69.8%) | Loss: 1.1420 | Batch time: 0.06s
2025-03-04 23:15:07,802 - INFO - [TRAIN] Epoch: 12/30 | Batch: 1072/1345 (79.8%) | Loss: 0.6361 | Batch time: 0.06s
2025-03-04 23:15:14,882 - INFO - [TRAIN] Epoch: 12/30 | Batch: 1206/1345 (89.7%) | Loss: 0.9784 | Batch time: 0.06s
2025-03-04 23:15:21,553 - INFO - [TRAIN] Epoch: 12/30 | Batch: 1340/1345 (99.7%) | Loss: 0.7469 | Batch time: 0.02s
2025-03-04 23:15:21,648 - INFO - [TRAIN] Epoch: 12/30 | Batch: 1344/1345 (100.0%) | Loss: 0.7642 | Batch time: 0.02s
2025-03-04 23:15:21,976 - INFO - [VAL] Epoch: 12/30 | Batch: 0/289 (0.3%) | Loss: 0.2643 | Batch time: 0.08s
2025-03-04 23:15:22,708 - INFO - [VAL] Epoch: 12/30 | Batch: 28/289 (10.0%) | Loss: 0.4428 | Batch time: 0.02s
2025-03-04 23:15:23,360 - INFO - [VAL] Epoch: 12/30 | Batch: 56/289 (19.7%) | Loss: 0.7386 | Batch time: 0.02s
2025-03-04 23:15:23,972 - INFO - [VAL] Epoch: 12/30 | Batch: 84/289 (29.4%) | Loss: 0.2598 | Batch time: 0.05s
2025-03-04 23:15:24,701 - INFO - [VAL] Epoch: 12/30 | Batch: 112/289 (39.1%) | Loss: 0.3445 | Batch time: 0.02s
2025-03-04 23:15:25,506 - INFO - [VAL] Epoch: 12/30 | Batch: 140/289 (48.8%) | Loss: 0.2009 | Batch time: 0.03s
2025-03-04 23:15:26,499 - INFO - [VAL] Epoch: 12/30 | Batch: 168/289 (58.5%) | Loss: 0.3156 | Batch time: 0.02s
2025-03-04 23:15:27,692 - INFO - [VAL] Epoch: 12/30 | Batch: 196/289 (68.2%) | Loss: 0.2610 | Batch time: 0.04s
2025-03-04 23:15:28,912 - INFO - [VAL] Epoch: 12/30 | Batch: 224/289 (77.9%) | Loss: 0.3084 | Batch time: 0.05s
2025-03-04 23:15:30,069 - INFO - [VAL] Epoch: 12/30 | Batch: 252/289 (87.5%) | Loss: 0.4070 | Batch time: 0.04s
2025-03-04 23:15:31,213 - INFO - [VAL] Epoch: 12/30 | Batch: 280/289 (97.2%) | Loss: 0.4077 | Batch time: 0.06s
2025-03-04 23:15:31,487 - INFO - [VAL] Epoch: 12/30 | Batch: 288/289 (100.0%) | Loss: 0.1129 | Batch time: 0.03s
2025-03-04 23:15:31,496 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:15:31,496 - INFO - Epoch 12/30 completed in 81.28s
2025-03-04 23:15:31,496 - INFO - Training   - Loss: 0.8035, Accuracy: 0.7680, F1: 0.7695
2025-03-04 23:15:31,496 - INFO - Validation - Loss: 0.3639, Accuracy: 0.8949, F1: 0.8944
2025-03-04 23:15:31,496 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:15:31,496 - INFO - Epoch 13/30
2025-03-04 23:15:31,496 - INFO - ----------------------------------------
2025-03-04 23:15:31,995 - INFO - [TRAIN] Epoch: 13/30 | Batch: 0/1345 (0.1%) | Loss: 1.7736 | Batch time: 0.13s
2025-03-04 23:15:38,871 - INFO - [TRAIN] Epoch: 13/30 | Batch: 134/1345 (10.0%) | Loss: 0.5291 | Batch time: 0.07s
2025-03-04 23:15:45,625 - INFO - [TRAIN] Epoch: 13/30 | Batch: 268/1345 (20.0%) | Loss: 0.8252 | Batch time: 0.07s
2025-03-04 23:15:52,475 - INFO - [TRAIN] Epoch: 13/30 | Batch: 402/1345 (30.0%) | Loss: 0.7376 | Batch time: 0.05s
2025-03-04 23:15:59,482 - INFO - [TRAIN] Epoch: 13/30 | Batch: 536/1345 (39.9%) | Loss: 1.0965 | Batch time: 0.08s
2025-03-04 23:16:06,511 - INFO - [TRAIN] Epoch: 13/30 | Batch: 670/1345 (49.9%) | Loss: 0.4477 | Batch time: 0.04s
2025-03-04 23:16:14,196 - INFO - [TRAIN] Epoch: 13/30 | Batch: 804/1345 (59.9%) | Loss: 0.9838 | Batch time: 0.03s
2025-03-04 23:16:21,228 - INFO - [TRAIN] Epoch: 13/30 | Batch: 938/1345 (69.8%) | Loss: 0.5804 | Batch time: 0.05s
2025-03-04 23:16:28,130 - INFO - [TRAIN] Epoch: 13/30 | Batch: 1072/1345 (79.8%) | Loss: 0.9013 | Batch time: 0.05s
2025-03-04 23:16:35,091 - INFO - [TRAIN] Epoch: 13/30 | Batch: 1206/1345 (89.7%) | Loss: 0.6309 | Batch time: 0.03s
2025-03-04 23:16:41,124 - INFO - [TRAIN] Epoch: 13/30 | Batch: 1340/1345 (99.7%) | Loss: 0.4925 | Batch time: 0.03s
2025-03-04 23:16:41,240 - INFO - [TRAIN] Epoch: 13/30 | Batch: 1344/1345 (100.0%) | Loss: 0.8254 | Batch time: 0.02s
2025-03-04 23:16:41,477 - INFO - [VAL] Epoch: 13/30 | Batch: 0/289 (0.3%) | Loss: 0.2522 | Batch time: 0.05s
2025-03-04 23:16:42,377 - INFO - [VAL] Epoch: 13/30 | Batch: 28/289 (10.0%) | Loss: 0.4498 | Batch time: 0.05s
2025-03-04 23:16:43,576 - INFO - [VAL] Epoch: 13/30 | Batch: 56/289 (19.7%) | Loss: 0.8094 | Batch time: 0.05s
2025-03-04 23:16:44,825 - INFO - [VAL] Epoch: 13/30 | Batch: 84/289 (29.4%) | Loss: 0.2994 | Batch time: 0.06s
2025-03-04 23:16:45,988 - INFO - [VAL] Epoch: 13/30 | Batch: 112/289 (39.1%) | Loss: 0.3868 | Batch time: 0.05s
2025-03-04 23:16:47,165 - INFO - [VAL] Epoch: 13/30 | Batch: 140/289 (48.8%) | Loss: 0.2346 | Batch time: 0.06s
2025-03-04 23:16:48,354 - INFO - [VAL] Epoch: 13/30 | Batch: 168/289 (58.5%) | Loss: 0.3276 | Batch time: 0.04s
2025-03-04 23:16:49,521 - INFO - [VAL] Epoch: 13/30 | Batch: 196/289 (68.2%) | Loss: 0.2841 | Batch time: 0.02s
2025-03-04 23:16:50,667 - INFO - [VAL] Epoch: 13/30 | Batch: 224/289 (77.9%) | Loss: 0.3345 | Batch time: 0.02s
2025-03-04 23:16:51,818 - INFO - [VAL] Epoch: 13/30 | Batch: 252/289 (87.5%) | Loss: 0.4974 | Batch time: 0.02s
2025-03-04 23:16:52,989 - INFO - [VAL] Epoch: 13/30 | Batch: 280/289 (97.2%) | Loss: 0.4551 | Batch time: 0.05s
2025-03-04 23:16:53,348 - INFO - [VAL] Epoch: 13/30 | Batch: 288/289 (100.0%) | Loss: 0.1535 | Batch time: 0.03s
2025-03-04 23:16:53,357 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:16:53,357 - INFO - Epoch 13/30 completed in 81.86s
2025-03-04 23:16:53,357 - INFO - Training   - Loss: 0.8001, Accuracy: 0.7669, F1: 0.7684
2025-03-04 23:16:53,357 - INFO - Validation - Loss: 0.4062, Accuracy: 0.8829, F1: 0.8819
2025-03-04 23:16:53,357 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:16:53,357 - INFO - Epoch 14/30
2025-03-04 23:16:53,357 - INFO - ----------------------------------------
2025-03-04 23:16:53,966 - INFO - [TRAIN] Epoch: 14/30 | Batch: 0/1345 (0.1%) | Loss: 0.5626 | Batch time: 0.17s
2025-03-04 23:17:01,494 - INFO - [TRAIN] Epoch: 14/30 | Batch: 134/1345 (10.0%) | Loss: 0.7948 | Batch time: 0.07s
2025-03-04 23:17:08,843 - INFO - [TRAIN] Epoch: 14/30 | Batch: 268/1345 (20.0%) | Loss: 0.6399 | Batch time: 0.06s
2025-03-04 23:17:15,846 - INFO - [TRAIN] Epoch: 14/30 | Batch: 402/1345 (30.0%) | Loss: 0.5177 | Batch time: 0.07s
2025-03-04 23:17:23,067 - INFO - [TRAIN] Epoch: 14/30 | Batch: 536/1345 (39.9%) | Loss: 0.9531 | Batch time: 0.05s
2025-03-04 23:17:30,892 - INFO - [TRAIN] Epoch: 14/30 | Batch: 670/1345 (49.9%) | Loss: 0.7023 | Batch time: 0.06s
2025-03-04 23:17:38,335 - INFO - [TRAIN] Epoch: 14/30 | Batch: 804/1345 (59.9%) | Loss: 1.0950 | Batch time: 0.06s
2025-03-04 23:17:45,056 - INFO - [TRAIN] Epoch: 14/30 | Batch: 938/1345 (69.8%) | Loss: 0.7489 | Batch time: 0.05s
2025-03-04 23:17:51,923 - INFO - [TRAIN] Epoch: 14/30 | Batch: 1072/1345 (79.8%) | Loss: 0.5196 | Batch time: 0.06s
2025-03-04 23:17:58,361 - INFO - [TRAIN] Epoch: 14/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8297 | Batch time: 0.03s
2025-03-04 23:18:05,076 - INFO - [TRAIN] Epoch: 14/30 | Batch: 1340/1345 (99.7%) | Loss: 0.9812 | Batch time: 0.03s
2025-03-04 23:18:05,300 - INFO - [TRAIN] Epoch: 14/30 | Batch: 1344/1345 (100.0%) | Loss: 0.9352 | Batch time: 0.06s
2025-03-04 23:18:05,554 - INFO - [VAL] Epoch: 14/30 | Batch: 0/289 (0.3%) | Loss: 0.2448 | Batch time: 0.04s
2025-03-04 23:18:06,717 - INFO - [VAL] Epoch: 14/30 | Batch: 28/289 (10.0%) | Loss: 0.4300 | Batch time: 0.06s
2025-03-04 23:18:07,861 - INFO - [VAL] Epoch: 14/30 | Batch: 56/289 (19.7%) | Loss: 0.8710 | Batch time: 0.04s
2025-03-04 23:18:09,076 - INFO - [VAL] Epoch: 14/30 | Batch: 84/289 (29.4%) | Loss: 0.2976 | Batch time: 0.04s
2025-03-04 23:18:10,325 - INFO - [VAL] Epoch: 14/30 | Batch: 112/289 (39.1%) | Loss: 0.3870 | Batch time: 0.06s
2025-03-04 23:18:11,528 - INFO - [VAL] Epoch: 14/30 | Batch: 140/289 (48.8%) | Loss: 0.2247 | Batch time: 0.04s
2025-03-04 23:18:12,708 - INFO - [VAL] Epoch: 14/30 | Batch: 168/289 (58.5%) | Loss: 0.3233 | Batch time: 0.03s
2025-03-04 23:18:13,822 - INFO - [VAL] Epoch: 14/30 | Batch: 196/289 (68.2%) | Loss: 0.2737 | Batch time: 0.03s
2025-03-04 23:18:14,911 - INFO - [VAL] Epoch: 14/30 | Batch: 224/289 (77.9%) | Loss: 0.3400 | Batch time: 0.03s
2025-03-04 23:18:15,994 - INFO - [VAL] Epoch: 14/30 | Batch: 252/289 (87.5%) | Loss: 0.4582 | Batch time: 0.02s
2025-03-04 23:18:17,083 - INFO - [VAL] Epoch: 14/30 | Batch: 280/289 (97.2%) | Loss: 0.4752 | Batch time: 0.02s
2025-03-04 23:18:17,434 - INFO - [VAL] Epoch: 14/30 | Batch: 288/289 (100.0%) | Loss: 0.1294 | Batch time: 0.03s
2025-03-04 23:18:17,444 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:18:17,444 - INFO - Epoch 14/30 completed in 84.09s
2025-03-04 23:18:17,444 - INFO - Training   - Loss: 0.8002, Accuracy: 0.7680, F1: 0.7696
2025-03-04 23:18:17,444 - INFO - Validation - Loss: 0.3917, Accuracy: 0.8869, F1: 0.8862
2025-03-04 23:18:17,444 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:18:17,444 - INFO - Epoch 15/30
2025-03-04 23:18:17,444 - INFO - ----------------------------------------
2025-03-04 23:18:17,867 - INFO - [TRAIN] Epoch: 15/30 | Batch: 0/1345 (0.1%) | Loss: 0.9237 | Batch time: 0.06s
2025-03-04 23:18:25,043 - INFO - [TRAIN] Epoch: 15/30 | Batch: 134/1345 (10.0%) | Loss: 0.6809 | Batch time: 0.03s
2025-03-04 23:18:31,929 - INFO - [TRAIN] Epoch: 15/30 | Batch: 268/1345 (20.0%) | Loss: 0.3870 | Batch time: 0.06s
2025-03-04 23:18:38,771 - INFO - [TRAIN] Epoch: 15/30 | Batch: 402/1345 (30.0%) | Loss: 0.6745 | Batch time: 0.04s
2025-03-04 23:18:46,087 - INFO - [TRAIN] Epoch: 15/30 | Batch: 536/1345 (39.9%) | Loss: 0.6859 | Batch time: 0.05s
2025-03-04 23:18:53,349 - INFO - [TRAIN] Epoch: 15/30 | Batch: 670/1345 (49.9%) | Loss: 1.0362 | Batch time: 0.06s
2025-03-04 23:19:00,822 - INFO - [TRAIN] Epoch: 15/30 | Batch: 804/1345 (59.9%) | Loss: 1.0274 | Batch time: 0.06s
2025-03-04 23:19:07,602 - INFO - [TRAIN] Epoch: 15/30 | Batch: 938/1345 (69.8%) | Loss: 0.5465 | Batch time: 0.04s
2025-03-04 23:19:14,349 - INFO - [TRAIN] Epoch: 15/30 | Batch: 1072/1345 (79.8%) | Loss: 0.7205 | Batch time: 0.04s
2025-03-04 23:19:20,838 - INFO - [TRAIN] Epoch: 15/30 | Batch: 1206/1345 (89.7%) | Loss: 0.9543 | Batch time: 0.06s
2025-03-04 23:19:27,767 - INFO - [TRAIN] Epoch: 15/30 | Batch: 1340/1345 (99.7%) | Loss: 1.0429 | Batch time: 0.02s
2025-03-04 23:19:27,948 - INFO - [TRAIN] Epoch: 15/30 | Batch: 1344/1345 (100.0%) | Loss: 1.1251 | Batch time: 0.06s
2025-03-04 23:19:28,243 - INFO - [VAL] Epoch: 15/30 | Batch: 0/289 (0.3%) | Loss: 0.2714 | Batch time: 0.10s
2025-03-04 23:19:29,237 - INFO - [VAL] Epoch: 15/30 | Batch: 28/289 (10.0%) | Loss: 0.4331 | Batch time: 0.03s
2025-03-04 23:19:30,356 - INFO - [VAL] Epoch: 15/30 | Batch: 56/289 (19.7%) | Loss: 0.8447 | Batch time: 0.04s
2025-03-04 23:19:31,511 - INFO - [VAL] Epoch: 15/30 | Batch: 84/289 (29.4%) | Loss: 0.2947 | Batch time: 0.04s
2025-03-04 23:19:32,703 - INFO - [VAL] Epoch: 15/30 | Batch: 112/289 (39.1%) | Loss: 0.3995 | Batch time: 0.05s
2025-03-04 23:19:33,860 - INFO - [VAL] Epoch: 15/30 | Batch: 140/289 (48.8%) | Loss: 0.2342 | Batch time: 0.05s
2025-03-04 23:19:35,022 - INFO - [VAL] Epoch: 15/30 | Batch: 168/289 (58.5%) | Loss: 0.3139 | Batch time: 0.02s
2025-03-04 23:19:36,165 - INFO - [VAL] Epoch: 15/30 | Batch: 196/289 (68.2%) | Loss: 0.3287 | Batch time: 0.02s
2025-03-04 23:19:37,344 - INFO - [VAL] Epoch: 15/30 | Batch: 224/289 (77.9%) | Loss: 0.3480 | Batch time: 0.05s
2025-03-04 23:19:38,530 - INFO - [VAL] Epoch: 15/30 | Batch: 252/289 (87.5%) | Loss: 0.4968 | Batch time: 0.04s
2025-03-04 23:19:39,712 - INFO - [VAL] Epoch: 15/30 | Batch: 280/289 (97.2%) | Loss: 0.4503 | Batch time: 0.05s
2025-03-04 23:19:39,981 - INFO - [VAL] Epoch: 15/30 | Batch: 288/289 (100.0%) | Loss: 0.1128 | Batch time: 0.03s
2025-03-04 23:19:39,991 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:19:39,991 - INFO - Epoch 15/30 completed in 82.55s
2025-03-04 23:19:39,991 - INFO - Training   - Loss: 0.7832, Accuracy: 0.7701, F1: 0.7716
2025-03-04 23:19:39,991 - INFO - Validation - Loss: 0.4163, Accuracy: 0.8817, F1: 0.8806
2025-03-04 23:19:39,991 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:19:40,286 - INFO - Checkpoint saved: checkpoint_epoch_15.pth (Epoch 15)
2025-03-04 23:19:40,286 - INFO - Epoch 16/30
2025-03-04 23:19:40,286 - INFO - ----------------------------------------
2025-03-04 23:19:40,716 - INFO - [TRAIN] Epoch: 16/30 | Batch: 0/1345 (0.1%) | Loss: 0.6190 | Batch time: 0.07s
2025-03-04 23:19:48,179 - INFO - [TRAIN] Epoch: 16/30 | Batch: 134/1345 (10.0%) | Loss: 1.0112 | Batch time: 0.05s
2025-03-04 23:19:55,416 - INFO - [TRAIN] Epoch: 16/30 | Batch: 268/1345 (20.0%) | Loss: 0.9149 | Batch time: 0.06s
2025-03-04 23:20:02,819 - INFO - [TRAIN] Epoch: 16/30 | Batch: 402/1345 (30.0%) | Loss: 0.5597 | Batch time: 0.03s
2025-03-04 23:20:09,694 - INFO - [TRAIN] Epoch: 16/30 | Batch: 536/1345 (39.9%) | Loss: 0.5907 | Batch time: 0.05s
2025-03-04 23:20:16,661 - INFO - [TRAIN] Epoch: 16/30 | Batch: 670/1345 (49.9%) | Loss: 0.6377 | Batch time: 0.06s
2025-03-04 23:20:23,505 - INFO - [TRAIN] Epoch: 16/30 | Batch: 804/1345 (59.9%) | Loss: 1.0477 | Batch time: 0.06s
2025-03-04 23:20:30,748 - INFO - [TRAIN] Epoch: 16/30 | Batch: 938/1345 (69.8%) | Loss: 0.4662 | Batch time: 0.04s
2025-03-04 23:20:37,469 - INFO - [TRAIN] Epoch: 16/30 | Batch: 1072/1345 (79.8%) | Loss: 0.8327 | Batch time: 0.03s
2025-03-04 23:20:44,792 - INFO - [TRAIN] Epoch: 16/30 | Batch: 1206/1345 (89.7%) | Loss: 0.9788 | Batch time: 0.03s
2025-03-04 23:20:51,760 - INFO - [TRAIN] Epoch: 16/30 | Batch: 1340/1345 (99.7%) | Loss: 0.6653 | Batch time: 0.03s
2025-03-04 23:20:51,966 - INFO - [TRAIN] Epoch: 16/30 | Batch: 1344/1345 (100.0%) | Loss: 0.8583 | Batch time: 0.06s
2025-03-04 23:20:52,214 - INFO - [VAL] Epoch: 16/30 | Batch: 0/289 (0.3%) | Loss: 0.2574 | Batch time: 0.06s
2025-03-04 23:20:53,317 - INFO - [VAL] Epoch: 16/30 | Batch: 28/289 (10.0%) | Loss: 0.4359 | Batch time: 0.02s
2025-03-04 23:20:54,402 - INFO - [VAL] Epoch: 16/30 | Batch: 56/289 (19.7%) | Loss: 0.7441 | Batch time: 0.02s
2025-03-04 23:20:55,494 - INFO - [VAL] Epoch: 16/30 | Batch: 84/289 (29.4%) | Loss: 0.2641 | Batch time: 0.02s
2025-03-04 23:20:56,584 - INFO - [VAL] Epoch: 16/30 | Batch: 112/289 (39.1%) | Loss: 0.3649 | Batch time: 0.02s
2025-03-04 23:20:57,695 - INFO - [VAL] Epoch: 16/30 | Batch: 140/289 (48.8%) | Loss: 0.2129 | Batch time: 0.04s
2025-03-04 23:20:58,821 - INFO - [VAL] Epoch: 16/30 | Batch: 168/289 (58.5%) | Loss: 0.2874 | Batch time: 0.05s
2025-03-04 23:20:59,968 - INFO - [VAL] Epoch: 16/30 | Batch: 196/289 (68.2%) | Loss: 0.2414 | Batch time: 0.06s
2025-03-04 23:21:01,086 - INFO - [VAL] Epoch: 16/30 | Batch: 224/289 (77.9%) | Loss: 0.2840 | Batch time: 0.05s
2025-03-04 23:21:02,163 - INFO - [VAL] Epoch: 16/30 | Batch: 252/289 (87.5%) | Loss: 0.3736 | Batch time: 0.05s
2025-03-04 23:21:03,278 - INFO - [VAL] Epoch: 16/30 | Batch: 280/289 (97.2%) | Loss: 0.4332 | Batch time: 0.05s
2025-03-04 23:21:03,637 - INFO - [VAL] Epoch: 16/30 | Batch: 288/289 (100.0%) | Loss: 0.1638 | Batch time: 0.02s
2025-03-04 23:21:03,645 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:21:03,646 - INFO - Epoch 16/30 completed in 83.36s
2025-03-04 23:21:03,646 - INFO - Training   - Loss: 0.7865, Accuracy: 0.7700, F1: 0.7714
2025-03-04 23:21:03,646 - INFO - Validation - Loss: 0.3467, Accuracy: 0.8962, F1: 0.8964
2025-03-04 23:21:03,646 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:21:03,646 - INFO - Epoch 17/30
2025-03-04 23:21:03,646 - INFO - ----------------------------------------
2025-03-04 23:21:04,127 - INFO - [TRAIN] Epoch: 17/30 | Batch: 0/1345 (0.1%) | Loss: 0.5952 | Batch time: 0.05s
2025-03-04 23:21:11,909 - INFO - [TRAIN] Epoch: 17/30 | Batch: 134/1345 (10.0%) | Loss: 0.8500 | Batch time: 0.06s
2025-03-04 23:21:18,976 - INFO - [TRAIN] Epoch: 17/30 | Batch: 268/1345 (20.0%) | Loss: 0.6966 | Batch time: 0.05s
2025-03-04 23:21:26,747 - INFO - [TRAIN] Epoch: 17/30 | Batch: 402/1345 (30.0%) | Loss: 0.8635 | Batch time: 0.07s
2025-03-04 23:21:35,055 - INFO - [TRAIN] Epoch: 17/30 | Batch: 536/1345 (39.9%) | Loss: 1.1842 | Batch time: 0.04s
2025-03-04 23:21:42,771 - INFO - [TRAIN] Epoch: 17/30 | Batch: 670/1345 (49.9%) | Loss: 0.5029 | Batch time: 0.07s
2025-03-04 23:21:50,603 - INFO - [TRAIN] Epoch: 17/30 | Batch: 804/1345 (59.9%) | Loss: 0.4842 | Batch time: 0.06s
2025-03-04 23:21:57,545 - INFO - [TRAIN] Epoch: 17/30 | Batch: 938/1345 (69.8%) | Loss: 0.6054 | Batch time: 0.05s
2025-03-04 23:22:04,787 - INFO - [TRAIN] Epoch: 17/30 | Batch: 1072/1345 (79.8%) | Loss: 0.9894 | Batch time: 0.06s
2025-03-04 23:22:12,128 - INFO - [TRAIN] Epoch: 17/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8788 | Batch time: 0.04s
2025-03-04 23:22:19,379 - INFO - [TRAIN] Epoch: 17/30 | Batch: 1340/1345 (99.7%) | Loss: 0.4941 | Batch time: 0.03s
2025-03-04 23:22:19,583 - INFO - [TRAIN] Epoch: 17/30 | Batch: 1344/1345 (100.0%) | Loss: 0.9549 | Batch time: 0.06s
2025-03-04 23:22:19,847 - INFO - [VAL] Epoch: 17/30 | Batch: 0/289 (0.3%) | Loss: 0.2317 | Batch time: 0.06s
2025-03-04 23:22:21,003 - INFO - [VAL] Epoch: 17/30 | Batch: 28/289 (10.0%) | Loss: 0.3740 | Batch time: 0.03s
2025-03-04 23:22:22,148 - INFO - [VAL] Epoch: 17/30 | Batch: 56/289 (19.7%) | Loss: 0.7515 | Batch time: 0.02s
2025-03-04 23:22:23,313 - INFO - [VAL] Epoch: 17/30 | Batch: 84/289 (29.4%) | Loss: 0.2909 | Batch time: 0.04s
2025-03-04 23:22:24,468 - INFO - [VAL] Epoch: 17/30 | Batch: 112/289 (39.1%) | Loss: 0.3025 | Batch time: 0.05s
2025-03-04 23:22:25,627 - INFO - [VAL] Epoch: 17/30 | Batch: 140/289 (48.8%) | Loss: 0.2050 | Batch time: 0.05s
2025-03-04 23:22:26,770 - INFO - [VAL] Epoch: 17/30 | Batch: 168/289 (58.5%) | Loss: 0.3268 | Batch time: 0.04s
2025-03-04 23:22:27,939 - INFO - [VAL] Epoch: 17/30 | Batch: 196/289 (68.2%) | Loss: 0.2735 | Batch time: 0.04s
2025-03-04 23:22:29,084 - INFO - [VAL] Epoch: 17/30 | Batch: 224/289 (77.9%) | Loss: 0.2929 | Batch time: 0.05s
2025-03-04 23:22:30,290 - INFO - [VAL] Epoch: 17/30 | Batch: 252/289 (87.5%) | Loss: 0.3802 | Batch time: 0.05s
2025-03-04 23:22:31,395 - INFO - [VAL] Epoch: 17/30 | Batch: 280/289 (97.2%) | Loss: 0.4269 | Batch time: 0.06s
2025-03-04 23:22:31,653 - INFO - [VAL] Epoch: 17/30 | Batch: 288/289 (100.0%) | Loss: 0.1795 | Batch time: 0.03s
2025-03-04 23:22:31,662 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:22:31,662 - INFO - Epoch 17/30 completed in 88.02s
2025-03-04 23:22:31,662 - INFO - Training   - Loss: 0.7873, Accuracy: 0.7708, F1: 0.7720
2025-03-04 23:22:31,662 - INFO - Validation - Loss: 0.3442, Accuracy: 0.8988, F1: 0.8982
2025-03-04 23:22:31,662 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:22:31,662 - INFO - Epoch 18/30
2025-03-04 23:22:31,662 - INFO - ----------------------------------------
2025-03-04 23:22:32,270 - INFO - [TRAIN] Epoch: 18/30 | Batch: 0/1345 (0.1%) | Loss: 0.6289 | Batch time: 0.05s
2025-03-04 23:22:40,570 - INFO - [TRAIN] Epoch: 18/30 | Batch: 134/1345 (10.0%) | Loss: 1.0131 | Batch time: 0.04s
2025-03-04 23:22:48,363 - INFO - [TRAIN] Epoch: 18/30 | Batch: 268/1345 (20.0%) | Loss: 1.0547 | Batch time: 0.05s
2025-03-04 23:22:55,812 - INFO - [TRAIN] Epoch: 18/30 | Batch: 402/1345 (30.0%) | Loss: 0.7616 | Batch time: 0.04s
2025-03-04 23:23:03,145 - INFO - [TRAIN] Epoch: 18/30 | Batch: 536/1345 (39.9%) | Loss: 0.4887 | Batch time: 0.04s
2025-03-04 23:23:10,900 - INFO - [TRAIN] Epoch: 18/30 | Batch: 670/1345 (49.9%) | Loss: 0.6103 | Batch time: 0.04s
2025-03-04 23:23:17,033 - INFO - [TRAIN] Epoch: 18/30 | Batch: 804/1345 (59.9%) | Loss: 0.8202 | Batch time: 0.06s
2025-03-04 23:23:23,873 - INFO - [TRAIN] Epoch: 18/30 | Batch: 938/1345 (69.8%) | Loss: 1.0676 | Batch time: 0.03s
2025-03-04 23:23:30,816 - INFO - [TRAIN] Epoch: 18/30 | Batch: 1072/1345 (79.8%) | Loss: 0.7327 | Batch time: 0.10s
2025-03-04 23:23:37,783 - INFO - [TRAIN] Epoch: 18/30 | Batch: 1206/1345 (89.7%) | Loss: 0.5371 | Batch time: 0.03s
2025-03-04 23:23:45,249 - INFO - [TRAIN] Epoch: 18/30 | Batch: 1340/1345 (99.7%) | Loss: 1.3919 | Batch time: 0.06s
2025-03-04 23:23:45,476 - INFO - [TRAIN] Epoch: 18/30 | Batch: 1344/1345 (100.0%) | Loss: 1.0860 | Batch time: 0.06s
2025-03-04 23:23:45,771 - INFO - [VAL] Epoch: 18/30 | Batch: 0/289 (0.3%) | Loss: 0.2813 | Batch time: 0.07s
2025-03-04 23:23:46,891 - INFO - [VAL] Epoch: 18/30 | Batch: 28/289 (10.0%) | Loss: 0.4481 | Batch time: 0.05s
2025-03-04 23:23:48,064 - INFO - [VAL] Epoch: 18/30 | Batch: 56/289 (19.7%) | Loss: 0.7960 | Batch time: 0.06s
2025-03-04 23:23:49,183 - INFO - [VAL] Epoch: 18/30 | Batch: 84/289 (29.4%) | Loss: 0.3012 | Batch time: 0.02s
2025-03-04 23:23:50,314 - INFO - [VAL] Epoch: 18/30 | Batch: 112/289 (39.1%) | Loss: 0.3816 | Batch time: 0.03s
2025-03-04 23:23:51,438 - INFO - [VAL] Epoch: 18/30 | Batch: 140/289 (48.8%) | Loss: 0.2343 | Batch time: 0.02s
2025-03-04 23:23:52,581 - INFO - [VAL] Epoch: 18/30 | Batch: 168/289 (58.5%) | Loss: 0.2825 | Batch time: 0.04s
2025-03-04 23:23:53,738 - INFO - [VAL] Epoch: 18/30 | Batch: 196/289 (68.2%) | Loss: 0.3038 | Batch time: 0.05s
2025-03-04 23:23:54,888 - INFO - [VAL] Epoch: 18/30 | Batch: 224/289 (77.9%) | Loss: 0.3210 | Batch time: 0.05s
2025-03-04 23:23:55,995 - INFO - [VAL] Epoch: 18/30 | Batch: 252/289 (87.5%) | Loss: 0.4483 | Batch time: 0.04s
2025-03-04 23:23:57,101 - INFO - [VAL] Epoch: 18/30 | Batch: 280/289 (97.2%) | Loss: 0.4573 | Batch time: 0.05s
2025-03-04 23:23:57,448 - INFO - [VAL] Epoch: 18/30 | Batch: 288/289 (100.0%) | Loss: 0.1304 | Batch time: 0.03s
2025-03-04 23:23:57,466 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:23:57,466 - INFO - Epoch 18/30 completed in 85.80s
2025-03-04 23:23:57,466 - INFO - Training   - Loss: 0.7982, Accuracy: 0.7688, F1: 0.7703
2025-03-04 23:23:57,466 - INFO - Validation - Loss: 0.3924, Accuracy: 0.8893, F1: 0.8888
2025-03-04 23:23:57,466 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:23:57,466 - INFO - Epoch 19/30
2025-03-04 23:23:57,466 - INFO - ----------------------------------------
2025-03-04 23:23:57,976 - INFO - [TRAIN] Epoch: 19/30 | Batch: 0/1345 (0.1%) | Loss: 0.9890 | Batch time: 0.08s
2025-03-04 23:24:05,741 - INFO - [TRAIN] Epoch: 19/30 | Batch: 134/1345 (10.0%) | Loss: 0.9639 | Batch time: 0.07s
2025-03-04 23:24:13,257 - INFO - [TRAIN] Epoch: 19/30 | Batch: 268/1345 (20.0%) | Loss: 0.6684 | Batch time: 0.06s
2025-03-04 23:24:21,020 - INFO - [TRAIN] Epoch: 19/30 | Batch: 402/1345 (30.0%) | Loss: 0.3668 | Batch time: 0.04s
2025-03-04 23:24:28,490 - INFO - [TRAIN] Epoch: 19/30 | Batch: 536/1345 (39.9%) | Loss: 0.7760 | Batch time: 0.05s
2025-03-04 23:24:34,798 - INFO - [TRAIN] Epoch: 19/30 | Batch: 670/1345 (49.9%) | Loss: 0.6948 | Batch time: 0.15s
2025-03-04 23:24:41,824 - INFO - [TRAIN] Epoch: 19/30 | Batch: 804/1345 (59.9%) | Loss: 0.9569 | Batch time: 0.07s
2025-03-04 23:24:48,794 - INFO - [TRAIN] Epoch: 19/30 | Batch: 938/1345 (69.8%) | Loss: 0.3958 | Batch time: 0.04s
2025-03-04 23:24:55,913 - INFO - [TRAIN] Epoch: 19/30 | Batch: 1072/1345 (79.8%) | Loss: 0.6040 | Batch time: 0.06s
2025-03-04 23:25:03,000 - INFO - [TRAIN] Epoch: 19/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8675 | Batch time: 0.06s
2025-03-04 23:25:10,072 - INFO - [TRAIN] Epoch: 19/30 | Batch: 1340/1345 (99.7%) | Loss: 0.6958 | Batch time: 0.06s
2025-03-04 23:25:10,275 - INFO - [TRAIN] Epoch: 19/30 | Batch: 1344/1345 (100.0%) | Loss: 0.7873 | Batch time: 0.02s
2025-03-04 23:25:10,511 - INFO - [VAL] Epoch: 19/30 | Batch: 0/289 (0.3%) | Loss: 0.2869 | Batch time: 0.03s
2025-03-04 23:25:11,662 - INFO - [VAL] Epoch: 19/30 | Batch: 28/289 (10.0%) | Loss: 0.4955 | Batch time: 0.05s
2025-03-04 23:25:12,756 - INFO - [VAL] Epoch: 19/30 | Batch: 56/289 (19.7%) | Loss: 0.7821 | Batch time: 0.04s
2025-03-04 23:25:13,944 - INFO - [VAL] Epoch: 19/30 | Batch: 84/289 (29.4%) | Loss: 0.3260 | Batch time: 0.05s
2025-03-04 23:25:15,084 - INFO - [VAL] Epoch: 19/30 | Batch: 112/289 (39.1%) | Loss: 0.3756 | Batch time: 0.05s
2025-03-04 23:25:16,186 - INFO - [VAL] Epoch: 19/30 | Batch: 140/289 (48.8%) | Loss: 0.2090 | Batch time: 0.03s
2025-03-04 23:25:17,261 - INFO - [VAL] Epoch: 19/30 | Batch: 168/289 (58.5%) | Loss: 0.3148 | Batch time: 0.05s
2025-03-04 23:25:18,405 - INFO - [VAL] Epoch: 19/30 | Batch: 196/289 (68.2%) | Loss: 0.2835 | Batch time: 0.05s
2025-03-04 23:25:19,534 - INFO - [VAL] Epoch: 19/30 | Batch: 224/289 (77.9%) | Loss: 0.3531 | Batch time: 0.06s
2025-03-04 23:25:20,666 - INFO - [VAL] Epoch: 19/30 | Batch: 252/289 (87.5%) | Loss: 0.4654 | Batch time: 0.04s
2025-03-04 23:25:21,753 - INFO - [VAL] Epoch: 19/30 | Batch: 280/289 (97.2%) | Loss: 0.4641 | Batch time: 0.02s
2025-03-04 23:25:22,061 - INFO - [VAL] Epoch: 19/30 | Batch: 288/289 (100.0%) | Loss: 0.1112 | Batch time: 0.03s
2025-03-04 23:25:22,071 - INFO - Early stopping triggered after 19 epochs
2025-03-04 23:25:22,071 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:25:22,071 - INFO - Epoch 19/30 completed in 84.60s
2025-03-04 23:25:22,071 - INFO - Training   - Loss: 0.7979, Accuracy: 0.7657, F1: 0.7668
2025-03-04 23:25:22,071 - INFO - Validation - Loss: 0.3903, Accuracy: 0.8868, F1: 0.8865
2025-03-04 23:25:22,071 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:25:22,071 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:25:22,071 - INFO - Training completed in 0h 27m 55.90s
2025-03-04 23:25:22,071 - INFO - Best validation F1: 0.8999 (Epoch 9)
2025-03-04 23:25:22,071 - INFO - --------------------------------------------------------------------------------
2025-03-04 23:25:22,946 - INFO - Final model saved to models/efficientnet_b0_v2/models/efficientnet_b0_v1_final.pth
2025-03-04 23:25:22,974 - INFO - Model registered in models/model_registry.json
2025-03-04 23:25:22,974 - INFO - Generating visualizations...
2025-03-04 23:25:22,974 - INFO - Generating standard visualizations and GradCAM
2025-03-04 23:26:44,176 - INFO - t-SNE visualization saved to models/efficientnet_b0_v2/visualizations
2025-03-04 23:26:44,176 - INFO - Training and visualization finished!

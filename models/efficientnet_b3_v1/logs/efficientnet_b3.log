2025-03-02 20:43:42,138 - INFO - Starting experiment: efficientnet_b3
2025-03-02 20:43:42,138 - INFO - Command line arguments: Namespace(data_dir='data/raw', output_dir='models/efficientnet_b3_v1', model='efficientnet_b3', img_size=224, batch_size=16, num_workers=16, epochs=30, lr=0.001, weight_decay=0.0001, pretrained=True, freeze_backbone=True, no_cuda=False, no_mps=False, use_mps=True, use_amp=False, memory_efficient=True, cache_dataset=True, mps_graph=True, mps_fallback=True, pin_memory=True, optimize_for_m_series=True, patience=30, keep_top_k=3, version=None, find_lr=False, experiment_name='efficientnet_b3', resnet_version=50)
2025-03-02 20:43:42,138 - INFO - Processing dataset...
2025-03-02 20:43:42,306 - INFO - Class distribution:
2025-03-02 20:43:42,307 - INFO -   Tomato_healthy: 1591 images
2025-03-02 20:43:42,307 - INFO -   Potato___Early_blight: 1000 images
2025-03-02 20:43:42,307 - INFO -   Tomato__Tomato_YellowLeaf__Curl_Virus: 3208 images
2025-03-02 20:43:42,307 - INFO -   Tomato_Early_blight: 1000 images
2025-03-02 20:43:42,307 - INFO -   Tomato__Target_Spot: 1404 images
2025-03-02 20:43:42,307 - INFO -   Potato___Late_blight: 1000 images
2025-03-02 20:43:42,307 - INFO -   Tomato_Leaf_Mold: 952 images
2025-03-02 20:43:42,307 - INFO -   Tomato_Spider_mites_Two_spotted_spider_mite: 1676 images
2025-03-02 20:43:42,307 - INFO -   Tomato_Septoria_leaf_spot: 1771 images
2025-03-02 20:43:42,307 - INFO -   Tomato__Tomato_mosaic_virus: 373 images
2025-03-02 20:43:42,308 - INFO -   Pepper__bell___Bacterial_spot: 997 images
2025-03-02 20:43:42,308 - INFO -   Tomato_Bacterial_spot: 2127 images
2025-03-02 20:43:42,308 - INFO -   Tomato_Late_blight: 1909 images
2025-03-02 20:43:42,308 - INFO -   Pepper__bell___healthy: 1478 images
2025-03-02 20:43:42,308 - INFO -   Potato___healthy: 152 images
2025-03-02 20:43:42,308 - INFO - Creating model: efficientnet_b3 with 15 classes
2025-03-02 20:43:42,525 - INFO - Model architecture:
EfficientNet(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)
      )
    )
    (2): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)
      )
    )
    (3): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)
            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)
      )
    )
    (7): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)
            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)
      )
    )
    (8): Conv2dNormActivation(
      (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.3, inplace=False)
    (1): Linear(in_features=1536, out_features=512, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=512, out_features=15, bias=True)
  )
)
2025-03-02 20:43:42,527 - INFO - Using class weights: [0.5015516  0.7979687  0.24874336 0.7979687  0.5683538  0.7979687
 0.8382024  0.47611496 0.4505752  2.1393263  0.8003698  0.3751616
 0.4180035  0.5398976  5.249794  ]
2025-03-02 20:43:42,527 - INFO - Training only 4 parameters (classifier)
2025-03-02 20:43:42,528 - INFO - Starting training for 30 epochs
2025-03-02 20:43:42,528 - INFO - Using Automatic Mixed Precision: False
2025-03-02 20:43:42,528 - INFO - Early stopping patience: 30
2025-03-02 20:43:42,528 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:42,528 - INFO - Starting training: efficientnet_b3
2025-03-02 20:43:42,528 - INFO - Total epochs: 30
2025-03-02 20:43:42,528 - INFO - Training batches per epoch: 903
2025-03-02 20:43:42,528 - INFO - Validation batches per epoch: 194
2025-03-02 20:43:42,528 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:43:42,528 - INFO - Training model: efficientnet_b3_v1
2025-03-02 20:43:42,528 - INFO - Epoch 1/30
2025-03-02 20:43:42,528 - INFO - ----------------------------------------
2025-03-02 20:44:10,726 - INFO - [TRAIN] Epoch: 1/30 | Batch: 0/903 (0.1%) | Loss: 2.7268 | Batch time: 0.50s
2025-03-02 20:44:13,016 - INFO - [TRAIN] Epoch: 1/30 | Batch: 90/903 (10.1%) | Loss: 2.1186 | Batch time: 0.03s
2025-03-02 20:44:15,415 - INFO - [TRAIN] Epoch: 1/30 | Batch: 180/903 (20.0%) | Loss: 1.0508 | Batch time: 0.03s
2025-03-02 20:44:17,774 - INFO - [TRAIN] Epoch: 1/30 | Batch: 270/903 (30.0%) | Loss: 1.2952 | Batch time: 0.03s
2025-03-02 20:44:20,108 - INFO - [TRAIN] Epoch: 1/30 | Batch: 360/903 (40.0%) | Loss: 1.6415 | Batch time: 0.03s
2025-03-02 20:44:22,438 - INFO - [TRAIN] Epoch: 1/30 | Batch: 450/903 (49.9%) | Loss: 1.5942 | Batch time: 0.02s
2025-03-02 20:44:24,733 - INFO - [TRAIN] Epoch: 1/30 | Batch: 540/903 (59.9%) | Loss: 1.1959 | Batch time: 0.03s
2025-03-02 20:44:27,026 - INFO - [TRAIN] Epoch: 1/30 | Batch: 630/903 (69.9%) | Loss: 1.4995 | Batch time: 0.03s
2025-03-02 20:44:29,662 - INFO - [TRAIN] Epoch: 1/30 | Batch: 720/903 (79.8%) | Loss: 1.3751 | Batch time: 0.03s
2025-03-02 20:44:31,959 - INFO - [TRAIN] Epoch: 1/30 | Batch: 810/903 (89.8%) | Loss: 0.9047 | Batch time: 0.03s
2025-03-02 20:44:34,225 - INFO - [TRAIN] Epoch: 1/30 | Batch: 900/903 (99.8%) | Loss: 1.3948 | Batch time: 0.02s
2025-03-02 20:44:34,592 - INFO - [TRAIN] Epoch: 1/30 | Batch: 902/903 (100.0%) | Loss: 1.7344 | Batch time: 0.34s
2025-03-02 20:45:02,292 - INFO - [VAL] Epoch: 1/30 | Batch: 0/194 (0.5%) | Loss: 1.4426 | Batch time: 0.10s
2025-03-02 20:45:02,656 - INFO - [VAL] Epoch: 1/30 | Batch: 19/194 (10.3%) | Loss: 0.9411 | Batch time: 0.02s
2025-03-02 20:45:03,017 - INFO - [VAL] Epoch: 1/30 | Batch: 38/194 (20.1%) | Loss: 0.9845 | Batch time: 0.02s
2025-03-02 20:45:03,380 - INFO - [VAL] Epoch: 1/30 | Batch: 57/194 (29.9%) | Loss: 1.0322 | Batch time: 0.02s
2025-03-02 20:45:03,742 - INFO - [VAL] Epoch: 1/30 | Batch: 76/194 (39.7%) | Loss: 0.5079 | Batch time: 0.02s
2025-03-02 20:45:04,102 - INFO - [VAL] Epoch: 1/30 | Batch: 95/194 (49.5%) | Loss: 0.7959 | Batch time: 0.02s
2025-03-02 20:45:04,469 - INFO - [VAL] Epoch: 1/30 | Batch: 114/194 (59.3%) | Loss: 1.7384 | Batch time: 0.02s
2025-03-02 20:45:04,831 - INFO - [VAL] Epoch: 1/30 | Batch: 133/194 (69.1%) | Loss: 0.9671 | Batch time: 0.02s
2025-03-02 20:45:05,195 - INFO - [VAL] Epoch: 1/30 | Batch: 152/194 (78.9%) | Loss: 1.1417 | Batch time: 0.02s
2025-03-02 20:45:05,555 - INFO - [VAL] Epoch: 1/30 | Batch: 171/194 (88.7%) | Loss: 1.5231 | Batch time: 0.02s
2025-03-02 20:45:05,914 - INFO - [VAL] Epoch: 1/30 | Batch: 190/194 (98.5%) | Loss: 1.0173 | Batch time: 0.02s
2025-03-02 20:45:06,209 - INFO - [VAL] Epoch: 1/30 | Batch: 193/194 (100.0%) | Loss: 0.6221 | Batch time: 0.26s
2025-03-02 20:45:06,523 - INFO - Checkpoint saved: efficientnet_b3_v1_best.pth (Epoch 1)
2025-03-02 20:45:06,523 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:45:06,523 - INFO - Epoch 1/30 completed in 83.99s
2025-03-02 20:45:06,523 - INFO - Training   - Loss: 1.4643, Accuracy: 0.5408, F1: 0.5446
2025-03-02 20:45:06,523 - INFO - Validation - Loss: 1.0005, Accuracy: 0.6825, F1: 0.6803
2025-03-02 20:45:06,523 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:45:06,523 - INFO - Epoch 2/30
2025-03-02 20:45:06,523 - INFO - ----------------------------------------
2025-03-02 20:45:06,685 - INFO - [TRAIN] Epoch: 2/30 | Batch: 0/903 (0.1%) | Loss: 1.0026 | Batch time: 0.05s
2025-03-02 20:45:08,993 - INFO - [TRAIN] Epoch: 2/30 | Batch: 90/903 (10.1%) | Loss: 0.8994 | Batch time: 0.02s
2025-03-02 20:45:11,239 - INFO - [TRAIN] Epoch: 2/30 | Batch: 180/903 (20.0%) | Loss: 1.3751 | Batch time: 0.03s
2025-03-02 20:45:13,489 - INFO - [TRAIN] Epoch: 2/30 | Batch: 270/903 (30.0%) | Loss: 1.3006 | Batch time: 0.03s
2025-03-02 20:45:15,790 - INFO - [TRAIN] Epoch: 2/30 | Batch: 360/903 (40.0%) | Loss: 1.1965 | Batch time: 0.02s
2025-03-02 20:45:18,131 - INFO - [TRAIN] Epoch: 2/30 | Batch: 450/903 (49.9%) | Loss: 0.6325 | Batch time: 0.03s
2025-03-02 20:45:20,537 - INFO - [TRAIN] Epoch: 2/30 | Batch: 540/903 (59.9%) | Loss: 0.8965 | Batch time: 0.03s
2025-03-02 20:45:22,798 - INFO - [TRAIN] Epoch: 2/30 | Batch: 630/903 (69.9%) | Loss: 0.9415 | Batch time: 0.02s
2025-03-02 20:45:25,063 - INFO - [TRAIN] Epoch: 2/30 | Batch: 720/903 (79.8%) | Loss: 1.2391 | Batch time: 0.02s
2025-03-02 20:45:27,360 - INFO - [TRAIN] Epoch: 2/30 | Batch: 810/903 (89.8%) | Loss: 0.9643 | Batch time: 0.02s
2025-03-02 20:45:29,607 - INFO - [TRAIN] Epoch: 2/30 | Batch: 900/903 (99.8%) | Loss: 0.9809 | Batch time: 0.02s
2025-03-02 20:45:29,657 - INFO - [TRAIN] Epoch: 2/30 | Batch: 902/903 (100.0%) | Loss: 0.7145 | Batch time: 0.03s
2025-03-02 20:45:29,722 - INFO - [VAL] Epoch: 2/30 | Batch: 0/194 (0.5%) | Loss: 0.9093 | Batch time: 0.03s
2025-03-02 20:45:30,082 - INFO - [VAL] Epoch: 2/30 | Batch: 19/194 (10.3%) | Loss: 0.9282 | Batch time: 0.02s
2025-03-02 20:45:30,441 - INFO - [VAL] Epoch: 2/30 | Batch: 38/194 (20.1%) | Loss: 0.5503 | Batch time: 0.02s
2025-03-02 20:45:30,802 - INFO - [VAL] Epoch: 2/30 | Batch: 57/194 (29.9%) | Loss: 1.0747 | Batch time: 0.02s
2025-03-02 20:45:31,163 - INFO - [VAL] Epoch: 2/30 | Batch: 76/194 (39.7%) | Loss: 0.3159 | Batch time: 0.02s
2025-03-02 20:45:31,527 - INFO - [VAL] Epoch: 2/30 | Batch: 95/194 (49.5%) | Loss: 0.9222 | Batch time: 0.02s
2025-03-02 20:45:31,883 - INFO - [VAL] Epoch: 2/30 | Batch: 114/194 (59.3%) | Loss: 0.9354 | Batch time: 0.02s
2025-03-02 20:45:32,243 - INFO - [VAL] Epoch: 2/30 | Batch: 133/194 (69.1%) | Loss: 0.8796 | Batch time: 0.02s
2025-03-02 20:45:32,608 - INFO - [VAL] Epoch: 2/30 | Batch: 152/194 (78.9%) | Loss: 1.0344 | Batch time: 0.02s
2025-03-02 20:45:32,971 - INFO - [VAL] Epoch: 2/30 | Batch: 171/194 (88.7%) | Loss: 1.2743 | Batch time: 0.02s
2025-03-02 20:45:33,330 - INFO - [VAL] Epoch: 2/30 | Batch: 190/194 (98.5%) | Loss: 1.1359 | Batch time: 0.02s
2025-03-02 20:45:33,385 - INFO - [VAL] Epoch: 2/30 | Batch: 193/194 (100.0%) | Loss: 0.3116 | Batch time: 0.02s
2025-03-02 20:45:33,613 - INFO - Checkpoint saved: efficientnet_b3_v1_best.pth (Epoch 2)
2025-03-02 20:45:33,613 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:45:33,613 - INFO - Epoch 2/30 completed in 27.09s
2025-03-02 20:45:33,613 - INFO - Training   - Loss: 1.2113, Accuracy: 0.6182, F1: 0.6217
2025-03-02 20:45:33,613 - INFO - Validation - Loss: 0.8115, Accuracy: 0.7361, F1: 0.7380
2025-03-02 20:45:33,613 - INFO - Validation F1 improved from 0.6803 to 0.7380
2025-03-02 20:45:33,613 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:45:33,613 - INFO - Epoch 3/30
2025-03-02 20:45:33,613 - INFO - ----------------------------------------
2025-03-02 20:45:33,764 - INFO - [TRAIN] Epoch: 3/30 | Batch: 0/903 (0.1%) | Loss: 1.1972 | Batch time: 0.05s
2025-03-02 20:45:36,047 - INFO - [TRAIN] Epoch: 3/30 | Batch: 90/903 (10.1%) | Loss: 1.3230 | Batch time: 0.02s
2025-03-02 20:45:38,289 - INFO - [TRAIN] Epoch: 3/30 | Batch: 180/903 (20.0%) | Loss: 1.5387 | Batch time: 0.02s
2025-03-02 20:45:40,551 - INFO - [TRAIN] Epoch: 3/30 | Batch: 270/903 (30.0%) | Loss: 1.2669 | Batch time: 0.02s
2025-03-02 20:45:42,839 - INFO - [TRAIN] Epoch: 3/30 | Batch: 360/903 (40.0%) | Loss: 1.6618 | Batch time: 0.02s
2025-03-02 20:45:45,117 - INFO - [TRAIN] Epoch: 3/30 | Batch: 450/903 (49.9%) | Loss: 1.5566 | Batch time: 0.02s
2025-03-02 20:45:47,397 - INFO - [TRAIN] Epoch: 3/30 | Batch: 540/903 (59.9%) | Loss: 1.7680 | Batch time: 0.02s
2025-03-02 20:45:49,680 - INFO - [TRAIN] Epoch: 3/30 | Batch: 630/903 (69.9%) | Loss: 1.6617 | Batch time: 0.02s
2025-03-02 20:45:51,956 - INFO - [TRAIN] Epoch: 3/30 | Batch: 720/903 (79.8%) | Loss: 0.8179 | Batch time: 0.03s
2025-03-02 20:45:54,260 - INFO - [TRAIN] Epoch: 3/30 | Batch: 810/903 (89.8%) | Loss: 2.3527 | Batch time: 0.02s
2025-03-02 20:45:56,457 - INFO - [TRAIN] Epoch: 3/30 | Batch: 900/903 (99.8%) | Loss: 0.8847 | Batch time: 0.02s
2025-03-02 20:45:56,502 - INFO - [TRAIN] Epoch: 3/30 | Batch: 902/903 (100.0%) | Loss: 1.0285 | Batch time: 0.02s
2025-03-02 20:45:56,565 - INFO - [VAL] Epoch: 3/30 | Batch: 0/194 (0.5%) | Loss: 1.1863 | Batch time: 0.02s
2025-03-02 20:45:56,910 - INFO - [VAL] Epoch: 3/30 | Batch: 19/194 (10.3%) | Loss: 1.3840 | Batch time: 0.02s
2025-03-02 20:45:57,254 - INFO - [VAL] Epoch: 3/30 | Batch: 38/194 (20.1%) | Loss: 0.8221 | Batch time: 0.02s
2025-03-02 20:45:57,605 - INFO - [VAL] Epoch: 3/30 | Batch: 57/194 (29.9%) | Loss: 1.3519 | Batch time: 0.02s
2025-03-02 20:45:57,951 - INFO - [VAL] Epoch: 3/30 | Batch: 76/194 (39.7%) | Loss: 0.3795 | Batch time: 0.02s
2025-03-02 20:45:58,297 - INFO - [VAL] Epoch: 3/30 | Batch: 95/194 (49.5%) | Loss: 0.9583 | Batch time: 0.02s
2025-03-02 20:45:58,643 - INFO - [VAL] Epoch: 3/30 | Batch: 114/194 (59.3%) | Loss: 1.7477 | Batch time: 0.02s
2025-03-02 20:45:58,988 - INFO - [VAL] Epoch: 3/30 | Batch: 133/194 (69.1%) | Loss: 1.0708 | Batch time: 0.02s
2025-03-02 20:45:59,333 - INFO - [VAL] Epoch: 3/30 | Batch: 152/194 (78.9%) | Loss: 1.2029 | Batch time: 0.02s
2025-03-02 20:45:59,679 - INFO - [VAL] Epoch: 3/30 | Batch: 171/194 (88.7%) | Loss: 1.1893 | Batch time: 0.02s
2025-03-02 20:46:00,022 - INFO - [VAL] Epoch: 3/30 | Batch: 190/194 (98.5%) | Loss: 0.8268 | Batch time: 0.02s
2025-03-02 20:46:00,073 - INFO - [VAL] Epoch: 3/30 | Batch: 193/194 (100.0%) | Loss: 0.7663 | Batch time: 0.01s
2025-03-02 20:46:00,076 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:46:00,076 - INFO - Epoch 3/30 completed in 26.46s
2025-03-02 20:46:00,076 - INFO - Training   - Loss: 1.1653, Accuracy: 0.6227, F1: 0.6268
2025-03-02 20:46:00,076 - INFO - Validation - Loss: 2.4646, Accuracy: 0.7213, F1: 0.7185
2025-03-02 20:46:00,076 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:46:00,076 - INFO - Epoch 4/30
2025-03-02 20:46:00,076 - INFO - ----------------------------------------
2025-03-02 20:46:00,240 - INFO - [TRAIN] Epoch: 4/30 | Batch: 0/903 (0.1%) | Loss: 1.2280 | Batch time: 0.05s
2025-03-02 20:46:02,478 - INFO - [TRAIN] Epoch: 4/30 | Batch: 90/903 (10.1%) | Loss: 1.4038 | Batch time: 0.02s
2025-03-02 20:46:04,700 - INFO - [TRAIN] Epoch: 4/30 | Batch: 180/903 (20.0%) | Loss: 0.5860 | Batch time: 0.02s
2025-03-02 20:46:06,984 - INFO - [TRAIN] Epoch: 4/30 | Batch: 270/903 (30.0%) | Loss: 2.3738 | Batch time: 0.02s
2025-03-02 20:46:09,250 - INFO - [TRAIN] Epoch: 4/30 | Batch: 360/903 (40.0%) | Loss: 0.5098 | Batch time: 0.02s
2025-03-02 20:46:11,458 - INFO - [TRAIN] Epoch: 4/30 | Batch: 450/903 (49.9%) | Loss: 1.1844 | Batch time: 0.02s
2025-03-02 20:46:13,670 - INFO - [TRAIN] Epoch: 4/30 | Batch: 540/903 (59.9%) | Loss: 0.8110 | Batch time: 0.03s
2025-03-02 20:46:15,867 - INFO - [TRAIN] Epoch: 4/30 | Batch: 630/903 (69.9%) | Loss: 1.3055 | Batch time: 0.02s
2025-03-02 20:46:18,047 - INFO - [TRAIN] Epoch: 4/30 | Batch: 720/903 (79.8%) | Loss: 1.0423 | Batch time: 0.02s
2025-03-02 20:46:20,218 - INFO - [TRAIN] Epoch: 4/30 | Batch: 810/903 (89.8%) | Loss: 0.7228 | Batch time: 0.02s
2025-03-02 20:46:22,359 - INFO - [TRAIN] Epoch: 4/30 | Batch: 900/903 (99.8%) | Loss: 0.7642 | Batch time: 0.02s
2025-03-02 20:46:22,404 - INFO - [TRAIN] Epoch: 4/30 | Batch: 902/903 (100.0%) | Loss: 0.4783 | Batch time: 0.02s
2025-03-02 20:46:22,466 - INFO - [VAL] Epoch: 4/30 | Batch: 0/194 (0.5%) | Loss: 1.2392 | Batch time: 0.02s
2025-03-02 20:46:22,804 - INFO - [VAL] Epoch: 4/30 | Batch: 19/194 (10.3%) | Loss: 1.0941 | Batch time: 0.02s
2025-03-02 20:46:23,142 - INFO - [VAL] Epoch: 4/30 | Batch: 38/194 (20.1%) | Loss: 0.6620 | Batch time: 0.02s
2025-03-02 20:46:23,483 - INFO - [VAL] Epoch: 4/30 | Batch: 57/194 (29.9%) | Loss: 1.2756 | Batch time: 0.02s
2025-03-02 20:46:23,822 - INFO - [VAL] Epoch: 4/30 | Batch: 76/194 (39.7%) | Loss: 0.4726 | Batch time: 0.02s
2025-03-02 20:46:24,167 - INFO - [VAL] Epoch: 4/30 | Batch: 95/194 (49.5%) | Loss: 0.7566 | Batch time: 0.02s
2025-03-02 20:46:24,507 - INFO - [VAL] Epoch: 4/30 | Batch: 114/194 (59.3%) | Loss: 1.4720 | Batch time: 0.02s
2025-03-02 20:46:24,850 - INFO - [VAL] Epoch: 4/30 | Batch: 133/194 (69.1%) | Loss: 0.8374 | Batch time: 0.02s
2025-03-02 20:46:25,188 - INFO - [VAL] Epoch: 4/30 | Batch: 152/194 (78.9%) | Loss: 1.0734 | Batch time: 0.02s
2025-03-02 20:46:25,525 - INFO - [VAL] Epoch: 4/30 | Batch: 171/194 (88.7%) | Loss: 1.0210 | Batch time: 0.02s
2025-03-02 20:46:25,859 - INFO - [VAL] Epoch: 4/30 | Batch: 190/194 (98.5%) | Loss: 0.8196 | Batch time: 0.02s
2025-03-02 20:46:25,908 - INFO - [VAL] Epoch: 4/30 | Batch: 193/194 (100.0%) | Loss: 0.7587 | Batch time: 0.01s
2025-03-02 20:46:25,912 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:46:25,912 - INFO - Epoch 4/30 completed in 25.84s
2025-03-02 20:46:25,912 - INFO - Training   - Loss: 1.1251, Accuracy: 0.6370, F1: 0.6415
2025-03-02 20:46:25,912 - INFO - Validation - Loss: 1.1299, Accuracy: 0.7351, F1: 0.7365
2025-03-02 20:46:25,912 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:46:25,912 - INFO - Epoch 5/30
2025-03-02 20:46:25,912 - INFO - ----------------------------------------
2025-03-02 20:46:26,084 - INFO - [TRAIN] Epoch: 5/30 | Batch: 0/903 (0.1%) | Loss: 1.2944 | Batch time: 0.07s
2025-03-02 20:46:28,245 - INFO - [TRAIN] Epoch: 5/30 | Batch: 90/903 (10.1%) | Loss: 1.4042 | Batch time: 0.02s
2025-03-02 20:46:30,386 - INFO - [TRAIN] Epoch: 5/30 | Batch: 180/903 (20.0%) | Loss: 1.4327 | Batch time: 0.02s
2025-03-02 20:46:32,544 - INFO - [TRAIN] Epoch: 5/30 | Batch: 270/903 (30.0%) | Loss: 1.2531 | Batch time: 0.02s
2025-03-02 20:46:34,695 - INFO - [TRAIN] Epoch: 5/30 | Batch: 360/903 (40.0%) | Loss: 0.8298 | Batch time: 0.02s
2025-03-02 20:46:36,849 - INFO - [TRAIN] Epoch: 5/30 | Batch: 450/903 (49.9%) | Loss: 0.8822 | Batch time: 0.02s
2025-03-02 20:46:38,987 - INFO - [TRAIN] Epoch: 5/30 | Batch: 540/903 (59.9%) | Loss: 1.7119 | Batch time: 0.02s
2025-03-02 20:46:41,145 - INFO - [TRAIN] Epoch: 5/30 | Batch: 630/903 (69.9%) | Loss: 1.4364 | Batch time: 0.02s
2025-03-02 20:46:43,306 - INFO - [TRAIN] Epoch: 5/30 | Batch: 720/903 (79.8%) | Loss: 0.8332 | Batch time: 0.02s
2025-03-02 20:46:45,458 - INFO - [TRAIN] Epoch: 5/30 | Batch: 810/903 (89.8%) | Loss: 0.7050 | Batch time: 0.02s
2025-03-02 20:46:47,595 - INFO - [TRAIN] Epoch: 5/30 | Batch: 900/903 (99.8%) | Loss: 1.4955 | Batch time: 0.02s
2025-03-02 20:46:47,639 - INFO - [TRAIN] Epoch: 5/30 | Batch: 902/903 (100.0%) | Loss: 1.5903 | Batch time: 0.02s
2025-03-02 20:46:47,698 - INFO - [VAL] Epoch: 5/30 | Batch: 0/194 (0.5%) | Loss: 0.9769 | Batch time: 0.02s
2025-03-02 20:46:48,034 - INFO - [VAL] Epoch: 5/30 | Batch: 19/194 (10.3%) | Loss: 0.8279 | Batch time: 0.02s
2025-03-02 20:46:48,372 - INFO - [VAL] Epoch: 5/30 | Batch: 38/194 (20.1%) | Loss: 0.8871 | Batch time: 0.02s
2025-03-02 20:46:48,718 - INFO - [VAL] Epoch: 5/30 | Batch: 57/194 (29.9%) | Loss: 1.1647 | Batch time: 0.02s
2025-03-02 20:46:49,059 - INFO - [VAL] Epoch: 5/30 | Batch: 76/194 (39.7%) | Loss: 0.3224 | Batch time: 0.02s
2025-03-02 20:46:49,399 - INFO - [VAL] Epoch: 5/30 | Batch: 95/194 (49.5%) | Loss: 1.0920 | Batch time: 0.02s
2025-03-02 20:46:49,737 - INFO - [VAL] Epoch: 5/30 | Batch: 114/194 (59.3%) | Loss: 1.3205 | Batch time: 0.02s
2025-03-02 20:46:50,075 - INFO - [VAL] Epoch: 5/30 | Batch: 133/194 (69.1%) | Loss: 0.8522 | Batch time: 0.02s
2025-03-02 20:46:50,413 - INFO - [VAL] Epoch: 5/30 | Batch: 152/194 (78.9%) | Loss: 0.6879 | Batch time: 0.02s
2025-03-02 20:46:50,749 - INFO - [VAL] Epoch: 5/30 | Batch: 171/194 (88.7%) | Loss: 1.1100 | Batch time: 0.02s
2025-03-02 20:46:51,082 - INFO - [VAL] Epoch: 5/30 | Batch: 190/194 (98.5%) | Loss: 0.9006 | Batch time: 0.02s
2025-03-02 20:46:51,131 - INFO - [VAL] Epoch: 5/30 | Batch: 193/194 (100.0%) | Loss: 0.5019 | Batch time: 0.01s
2025-03-02 20:46:51,369 - INFO - Checkpoint saved: efficientnet_b3_v1_best.pth (Epoch 5)
2025-03-02 20:46:51,369 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:46:51,369 - INFO - Epoch 5/30 completed in 25.46s
2025-03-02 20:46:51,369 - INFO - Training   - Loss: 1.1226, Accuracy: 0.6379, F1: 0.6428
2025-03-02 20:46:51,369 - INFO - Validation - Loss: 0.7925, Accuracy: 0.7548, F1: 0.7544
2025-03-02 20:46:51,369 - INFO - Validation F1 improved from 0.7380 to 0.7544
2025-03-02 20:46:51,369 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:46:51,514 - INFO - Checkpoint saved: checkpoint_epoch_5.pth (Epoch 5)
2025-03-02 20:46:51,514 - INFO - Epoch 6/30
2025-03-02 20:46:51,514 - INFO - ----------------------------------------
2025-03-02 20:46:51,655 - INFO - [TRAIN] Epoch: 6/30 | Batch: 0/903 (0.1%) | Loss: 1.7649 | Batch time: 0.06s
2025-03-02 20:46:53,844 - INFO - [TRAIN] Epoch: 6/30 | Batch: 90/903 (10.1%) | Loss: 0.9819 | Batch time: 0.02s
2025-03-02 20:46:56,009 - INFO - [TRAIN] Epoch: 6/30 | Batch: 180/903 (20.0%) | Loss: 1.1921 | Batch time: 0.02s
2025-03-02 20:46:58,183 - INFO - [TRAIN] Epoch: 6/30 | Batch: 270/903 (30.0%) | Loss: 1.2131 | Batch time: 0.02s
2025-03-02 20:47:00,425 - INFO - [TRAIN] Epoch: 6/30 | Batch: 360/903 (40.0%) | Loss: 1.0932 | Batch time: 0.02s
2025-03-02 20:47:02,609 - INFO - [TRAIN] Epoch: 6/30 | Batch: 450/903 (49.9%) | Loss: 0.6474 | Batch time: 0.02s
2025-03-02 20:47:04,872 - INFO - [TRAIN] Epoch: 6/30 | Batch: 540/903 (59.9%) | Loss: 1.2316 | Batch time: 0.02s
2025-03-02 20:47:07,040 - INFO - [TRAIN] Epoch: 6/30 | Batch: 630/903 (69.9%) | Loss: 0.9719 | Batch time: 0.02s
2025-03-02 20:47:09,225 - INFO - [TRAIN] Epoch: 6/30 | Batch: 720/903 (79.8%) | Loss: 1.4212 | Batch time: 0.02s
2025-03-02 20:47:11,384 - INFO - [TRAIN] Epoch: 6/30 | Batch: 810/903 (89.8%) | Loss: 1.0874 | Batch time: 0.02s
2025-03-02 20:47:13,501 - INFO - [TRAIN] Epoch: 6/30 | Batch: 900/903 (99.8%) | Loss: 0.9747 | Batch time: 0.02s
2025-03-02 20:47:13,545 - INFO - [TRAIN] Epoch: 6/30 | Batch: 902/903 (100.0%) | Loss: 1.0543 | Batch time: 0.02s
2025-03-02 20:47:13,606 - INFO - [VAL] Epoch: 6/30 | Batch: 0/194 (0.5%) | Loss: 0.8685 | Batch time: 0.02s
2025-03-02 20:47:13,942 - INFO - [VAL] Epoch: 6/30 | Batch: 19/194 (10.3%) | Loss: 1.0050 | Batch time: 0.02s
2025-03-02 20:47:14,279 - INFO - [VAL] Epoch: 6/30 | Batch: 38/194 (20.1%) | Loss: 0.5885 | Batch time: 0.02s
2025-03-02 20:47:14,620 - INFO - [VAL] Epoch: 6/30 | Batch: 57/194 (29.9%) | Loss: 1.0672 | Batch time: 0.02s
2025-03-02 20:47:14,966 - INFO - [VAL] Epoch: 6/30 | Batch: 76/194 (39.7%) | Loss: 0.3403 | Batch time: 0.02s
2025-03-02 20:47:15,303 - INFO - [VAL] Epoch: 6/30 | Batch: 95/194 (49.5%) | Loss: 0.6427 | Batch time: 0.02s
2025-03-02 20:47:15,640 - INFO - [VAL] Epoch: 6/30 | Batch: 114/194 (59.3%) | Loss: 0.8265 | Batch time: 0.02s
2025-03-02 20:47:15,976 - INFO - [VAL] Epoch: 6/30 | Batch: 133/194 (69.1%) | Loss: 0.7269 | Batch time: 0.02s
2025-03-02 20:47:16,310 - INFO - [VAL] Epoch: 6/30 | Batch: 152/194 (78.9%) | Loss: 0.6530 | Batch time: 0.02s
2025-03-02 20:47:16,647 - INFO - [VAL] Epoch: 6/30 | Batch: 171/194 (88.7%) | Loss: 1.1199 | Batch time: 0.02s
2025-03-02 20:47:16,982 - INFO - [VAL] Epoch: 6/30 | Batch: 190/194 (98.5%) | Loss: 0.9470 | Batch time: 0.02s
2025-03-02 20:47:17,032 - INFO - [VAL] Epoch: 6/30 | Batch: 193/194 (100.0%) | Loss: 0.5745 | Batch time: 0.01s
2025-03-02 20:47:17,268 - INFO - Checkpoint saved: efficientnet_b3_v1_best.pth (Epoch 6)
2025-03-02 20:47:17,268 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:47:17,268 - INFO - Epoch 6/30 completed in 25.75s
2025-03-02 20:47:17,268 - INFO - Training   - Loss: 1.0815, Accuracy: 0.6474, F1: 0.6525
2025-03-02 20:47:17,268 - INFO - Validation - Loss: 0.6786, Accuracy: 0.7729, F1: 0.7743
2025-03-02 20:47:17,268 - INFO - Validation F1 improved from 0.7544 to 0.7743
2025-03-02 20:47:17,268 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:47:17,268 - INFO - Epoch 7/30
2025-03-02 20:47:17,268 - INFO - ----------------------------------------
2025-03-02 20:47:17,407 - INFO - [TRAIN] Epoch: 7/30 | Batch: 0/903 (0.1%) | Loss: 1.0039 | Batch time: 0.05s
2025-03-02 20:47:19,615 - INFO - [TRAIN] Epoch: 7/30 | Batch: 90/903 (10.1%) | Loss: 1.0006 | Batch time: 0.02s
2025-03-02 20:47:21,789 - INFO - [TRAIN] Epoch: 7/30 | Batch: 180/903 (20.0%) | Loss: 0.5729 | Batch time: 0.02s
2025-03-02 20:47:23,997 - INFO - [TRAIN] Epoch: 7/30 | Batch: 270/903 (30.0%) | Loss: 1.9440 | Batch time: 0.03s
2025-03-02 20:47:26,200 - INFO - [TRAIN] Epoch: 7/30 | Batch: 360/903 (40.0%) | Loss: 0.5837 | Batch time: 0.02s
2025-03-02 20:47:28,429 - INFO - [TRAIN] Epoch: 7/30 | Batch: 450/903 (49.9%) | Loss: 0.4796 | Batch time: 0.02s
2025-03-02 20:47:30,669 - INFO - [TRAIN] Epoch: 7/30 | Batch: 540/903 (59.9%) | Loss: 1.0996 | Batch time: 0.02s
2025-03-02 20:47:32,913 - INFO - [TRAIN] Epoch: 7/30 | Batch: 630/903 (69.9%) | Loss: 1.4233 | Batch time: 0.02s
2025-03-02 20:47:35,146 - INFO - [TRAIN] Epoch: 7/30 | Batch: 720/903 (79.8%) | Loss: 1.0811 | Batch time: 0.02s
2025-03-02 20:47:37,380 - INFO - [TRAIN] Epoch: 7/30 | Batch: 810/903 (89.8%) | Loss: 0.9495 | Batch time: 0.02s
2025-03-02 20:47:39,563 - INFO - [TRAIN] Epoch: 7/30 | Batch: 900/903 (99.8%) | Loss: 1.5383 | Batch time: 0.02s
2025-03-02 20:47:39,609 - INFO - [TRAIN] Epoch: 7/30 | Batch: 902/903 (100.0%) | Loss: 1.4751 | Batch time: 0.02s
2025-03-02 20:47:39,666 - INFO - [VAL] Epoch: 7/30 | Batch: 0/194 (0.5%) | Loss: 0.9830 | Batch time: 0.02s
2025-03-02 20:47:40,012 - INFO - [VAL] Epoch: 7/30 | Batch: 19/194 (10.3%) | Loss: 1.1294 | Batch time: 0.02s
2025-03-02 20:47:40,358 - INFO - [VAL] Epoch: 7/30 | Batch: 38/194 (20.1%) | Loss: 0.8422 | Batch time: 0.02s
2025-03-02 20:47:40,705 - INFO - [VAL] Epoch: 7/30 | Batch: 57/194 (29.9%) | Loss: 1.0073 | Batch time: 0.02s
2025-03-02 20:47:41,053 - INFO - [VAL] Epoch: 7/30 | Batch: 76/194 (39.7%) | Loss: 0.2749 | Batch time: 0.02s
2025-03-02 20:47:41,398 - INFO - [VAL] Epoch: 7/30 | Batch: 95/194 (49.5%) | Loss: 0.9874 | Batch time: 0.02s
2025-03-02 20:47:41,742 - INFO - [VAL] Epoch: 7/30 | Batch: 114/194 (59.3%) | Loss: 1.5245 | Batch time: 0.02s
2025-03-02 20:47:42,087 - INFO - [VAL] Epoch: 7/30 | Batch: 133/194 (69.1%) | Loss: 1.0125 | Batch time: 0.02s
2025-03-02 20:47:42,429 - INFO - [VAL] Epoch: 7/30 | Batch: 152/194 (78.9%) | Loss: 0.8787 | Batch time: 0.02s
2025-03-02 20:47:42,769 - INFO - [VAL] Epoch: 7/30 | Batch: 171/194 (88.7%) | Loss: 1.1455 | Batch time: 0.02s
2025-03-02 20:47:43,109 - INFO - [VAL] Epoch: 7/30 | Batch: 190/194 (98.5%) | Loss: 0.9900 | Batch time: 0.02s
2025-03-02 20:47:43,159 - INFO - [VAL] Epoch: 7/30 | Batch: 193/194 (100.0%) | Loss: 0.4486 | Batch time: 0.01s
2025-03-02 20:47:43,162 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:47:43,163 - INFO - Epoch 7/30 completed in 25.89s
2025-03-02 20:47:43,163 - INFO - Training   - Loss: 1.0888, Accuracy: 0.6553, F1: 0.6592
2025-03-02 20:47:43,163 - INFO - Validation - Loss: 0.7520, Accuracy: 0.7645, F1: 0.7649
2025-03-02 20:47:43,163 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:47:43,163 - INFO - Epoch 8/30
2025-03-02 20:47:43,163 - INFO - ----------------------------------------
2025-03-02 20:47:43,308 - INFO - [TRAIN] Epoch: 8/30 | Batch: 0/903 (0.1%) | Loss: 1.0216 | Batch time: 0.05s
2025-03-02 20:47:45,559 - INFO - [TRAIN] Epoch: 8/30 | Batch: 90/903 (10.1%) | Loss: 0.9581 | Batch time: 0.03s
2025-03-02 20:47:47,777 - INFO - [TRAIN] Epoch: 8/30 | Batch: 180/903 (20.0%) | Loss: 0.3809 | Batch time: 0.02s
2025-03-02 20:47:49,969 - INFO - [TRAIN] Epoch: 8/30 | Batch: 270/903 (30.0%) | Loss: 1.3128 | Batch time: 0.02s
2025-03-02 20:47:52,177 - INFO - [TRAIN] Epoch: 8/30 | Batch: 360/903 (40.0%) | Loss: 1.3800 | Batch time: 0.02s
2025-03-02 20:47:54,400 - INFO - [TRAIN] Epoch: 8/30 | Batch: 450/903 (49.9%) | Loss: 1.4611 | Batch time: 0.02s
2025-03-02 20:47:56,616 - INFO - [TRAIN] Epoch: 8/30 | Batch: 540/903 (59.9%) | Loss: 1.0431 | Batch time: 0.03s
2025-03-02 20:47:58,828 - INFO - [TRAIN] Epoch: 8/30 | Batch: 630/903 (69.9%) | Loss: 1.0103 | Batch time: 0.02s
2025-03-02 20:48:01,081 - INFO - [TRAIN] Epoch: 8/30 | Batch: 720/903 (79.8%) | Loss: 0.4954 | Batch time: 0.03s
2025-03-02 20:48:03,290 - INFO - [TRAIN] Epoch: 8/30 | Batch: 810/903 (89.8%) | Loss: 0.8244 | Batch time: 0.02s
2025-03-02 20:48:05,511 - INFO - [TRAIN] Epoch: 8/30 | Batch: 900/903 (99.8%) | Loss: 1.0872 | Batch time: 0.02s
2025-03-02 20:48:05,558 - INFO - [TRAIN] Epoch: 8/30 | Batch: 902/903 (100.0%) | Loss: 1.7364 | Batch time: 0.02s
2025-03-02 20:48:05,610 - INFO - [VAL] Epoch: 8/30 | Batch: 0/194 (0.5%) | Loss: 1.0120 | Batch time: 0.02s
2025-03-02 20:48:05,962 - INFO - [VAL] Epoch: 8/30 | Batch: 19/194 (10.3%) | Loss: 1.0806 | Batch time: 0.02s
2025-03-02 20:48:06,312 - INFO - [VAL] Epoch: 8/30 | Batch: 38/194 (20.1%) | Loss: 0.9382 | Batch time: 0.02s
2025-03-02 20:48:06,667 - INFO - [VAL] Epoch: 8/30 | Batch: 57/194 (29.9%) | Loss: 0.9383 | Batch time: 0.02s
2025-03-02 20:48:07,017 - INFO - [VAL] Epoch: 8/30 | Batch: 76/194 (39.7%) | Loss: 0.1622 | Batch time: 0.02s
2025-03-02 20:48:07,369 - INFO - [VAL] Epoch: 8/30 | Batch: 95/194 (49.5%) | Loss: 0.8931 | Batch time: 0.02s
2025-03-02 20:48:07,720 - INFO - [VAL] Epoch: 8/30 | Batch: 114/194 (59.3%) | Loss: 1.7025 | Batch time: 0.02s
2025-03-02 20:48:08,064 - INFO - [VAL] Epoch: 8/30 | Batch: 133/194 (69.1%) | Loss: 0.9327 | Batch time: 0.02s
2025-03-02 20:48:08,408 - INFO - [VAL] Epoch: 8/30 | Batch: 152/194 (78.9%) | Loss: 0.9302 | Batch time: 0.02s
2025-03-02 20:48:08,756 - INFO - [VAL] Epoch: 8/30 | Batch: 171/194 (88.7%) | Loss: 1.0084 | Batch time: 0.02s
2025-03-02 20:48:09,105 - INFO - [VAL] Epoch: 8/30 | Batch: 190/194 (98.5%) | Loss: 0.9793 | Batch time: 0.02s
2025-03-02 20:48:09,157 - INFO - [VAL] Epoch: 8/30 | Batch: 193/194 (100.0%) | Loss: 0.3953 | Batch time: 0.01s
2025-03-02 20:48:09,160 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:48:09,160 - INFO - Epoch 8/30 completed in 26.00s
2025-03-02 20:48:09,160 - INFO - Training   - Loss: 1.0181, Accuracy: 0.6705, F1: 0.6750
2025-03-02 20:48:09,160 - INFO - Validation - Loss: 0.7489, Accuracy: 0.7500, F1: 0.7516
2025-03-02 20:48:09,160 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:48:09,160 - INFO - Epoch 9/30
2025-03-02 20:48:09,160 - INFO - ----------------------------------------
2025-03-02 20:48:09,313 - INFO - [TRAIN] Epoch: 9/30 | Batch: 0/903 (0.1%) | Loss: 0.6837 | Batch time: 0.05s
2025-03-02 20:48:11,523 - INFO - [TRAIN] Epoch: 9/30 | Batch: 90/903 (10.1%) | Loss: 0.5325 | Batch time: 0.02s
2025-03-02 20:48:13,700 - INFO - [TRAIN] Epoch: 9/30 | Batch: 180/903 (20.0%) | Loss: 0.7395 | Batch time: 0.02s
2025-03-02 20:48:15,898 - INFO - [TRAIN] Epoch: 9/30 | Batch: 270/903 (30.0%) | Loss: 0.8041 | Batch time: 0.02s
2025-03-02 20:48:18,103 - INFO - [TRAIN] Epoch: 9/30 | Batch: 360/903 (40.0%) | Loss: 1.0607 | Batch time: 0.02s
2025-03-02 20:48:20,327 - INFO - [TRAIN] Epoch: 9/30 | Batch: 450/903 (49.9%) | Loss: 0.3323 | Batch time: 0.02s
2025-03-02 20:48:22,549 - INFO - [TRAIN] Epoch: 9/30 | Batch: 540/903 (59.9%) | Loss: 0.9455 | Batch time: 0.02s
2025-03-02 20:48:24,779 - INFO - [TRAIN] Epoch: 9/30 | Batch: 630/903 (69.9%) | Loss: 1.0473 | Batch time: 0.02s
2025-03-02 20:48:27,014 - INFO - [TRAIN] Epoch: 9/30 | Batch: 720/903 (79.8%) | Loss: 0.6434 | Batch time: 0.02s
2025-03-02 20:48:29,265 - INFO - [TRAIN] Epoch: 9/30 | Batch: 810/903 (89.8%) | Loss: 1.2901 | Batch time: 0.02s
2025-03-02 20:48:31,498 - INFO - [TRAIN] Epoch: 9/30 | Batch: 900/903 (99.8%) | Loss: 1.2936 | Batch time: 0.02s
2025-03-02 20:48:31,546 - INFO - [TRAIN] Epoch: 9/30 | Batch: 902/903 (100.0%) | Loss: 1.2768 | Batch time: 0.02s
2025-03-02 20:48:31,606 - INFO - [VAL] Epoch: 9/30 | Batch: 0/194 (0.5%) | Loss: 0.7774 | Batch time: 0.02s
2025-03-02 20:48:31,959 - INFO - [VAL] Epoch: 9/30 | Batch: 19/194 (10.3%) | Loss: 6.8980 | Batch time: 0.02s
2025-03-02 20:48:32,315 - INFO - [VAL] Epoch: 9/30 | Batch: 38/194 (20.1%) | Loss: 14.7395 | Batch time: 0.02s
2025-03-02 20:48:32,671 - INFO - [VAL] Epoch: 9/30 | Batch: 57/194 (29.9%) | Loss: 5.8277 | Batch time: 0.02s
2025-03-02 20:48:33,023 - INFO - [VAL] Epoch: 9/30 | Batch: 76/194 (39.7%) | Loss: 0.2677 | Batch time: 0.02s
2025-03-02 20:48:33,374 - INFO - [VAL] Epoch: 9/30 | Batch: 95/194 (49.5%) | Loss: 0.7875 | Batch time: 0.02s
2025-03-02 20:48:33,722 - INFO - [VAL] Epoch: 9/30 | Batch: 114/194 (59.3%) | Loss: 2.0735 | Batch time: 0.02s
2025-03-02 20:48:34,069 - INFO - [VAL] Epoch: 9/30 | Batch: 133/194 (69.1%) | Loss: 1.3783 | Batch time: 0.02s
2025-03-02 20:48:34,416 - INFO - [VAL] Epoch: 9/30 | Batch: 152/194 (78.9%) | Loss: 1.1314 | Batch time: 0.02s
2025-03-02 20:48:34,764 - INFO - [VAL] Epoch: 9/30 | Batch: 171/194 (88.7%) | Loss: 1.0459 | Batch time: 0.02s
2025-03-02 20:48:35,113 - INFO - [VAL] Epoch: 9/30 | Batch: 190/194 (98.5%) | Loss: 32.8727 | Batch time: 0.02s
2025-03-02 20:48:35,164 - INFO - [VAL] Epoch: 9/30 | Batch: 193/194 (100.0%) | Loss: 0.3655 | Batch time: 0.01s
2025-03-02 20:48:35,168 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:48:35,168 - INFO - Epoch 9/30 completed in 26.01s
2025-03-02 20:48:35,168 - INFO - Training   - Loss: 0.9848, Accuracy: 0.6788, F1: 0.6822
2025-03-02 20:48:35,168 - INFO - Validation - Loss: 32.3407, Accuracy: 0.7261, F1: 0.7272
2025-03-02 20:48:35,168 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:48:35,168 - INFO - Epoch 10/30
2025-03-02 20:48:35,168 - INFO - ----------------------------------------
2025-03-02 20:48:35,311 - INFO - [TRAIN] Epoch: 10/30 | Batch: 0/903 (0.1%) | Loss: 0.6480 | Batch time: 0.05s
2025-03-02 20:48:37,503 - INFO - [TRAIN] Epoch: 10/30 | Batch: 90/903 (10.1%) | Loss: 0.6245 | Batch time: 0.02s
2025-03-02 20:48:39,728 - INFO - [TRAIN] Epoch: 10/30 | Batch: 180/903 (20.0%) | Loss: 0.9273 | Batch time: 0.02s
2025-03-02 20:48:41,980 - INFO - [TRAIN] Epoch: 10/30 | Batch: 270/903 (30.0%) | Loss: 0.9954 | Batch time: 0.02s
2025-03-02 20:48:44,221 - INFO - [TRAIN] Epoch: 10/30 | Batch: 360/903 (40.0%) | Loss: 0.5040 | Batch time: 0.02s
2025-03-02 20:48:46,464 - INFO - [TRAIN] Epoch: 10/30 | Batch: 450/903 (49.9%) | Loss: 0.7081 | Batch time: 0.02s
2025-03-02 20:48:48,711 - INFO - [TRAIN] Epoch: 10/30 | Batch: 540/903 (59.9%) | Loss: 0.3597 | Batch time: 0.03s
2025-03-02 20:48:50,952 - INFO - [TRAIN] Epoch: 10/30 | Batch: 630/903 (69.9%) | Loss: 1.0057 | Batch time: 0.02s
2025-03-02 20:48:53,193 - INFO - [TRAIN] Epoch: 10/30 | Batch: 720/903 (79.8%) | Loss: 1.1234 | Batch time: 0.02s
2025-03-02 20:48:55,463 - INFO - [TRAIN] Epoch: 10/30 | Batch: 810/903 (89.8%) | Loss: 0.7485 | Batch time: 0.03s
2025-03-02 20:48:57,697 - INFO - [TRAIN] Epoch: 10/30 | Batch: 900/903 (99.8%) | Loss: 0.9127 | Batch time: 0.02s
2025-03-02 20:48:57,743 - INFO - [TRAIN] Epoch: 10/30 | Batch: 902/903 (100.0%) | Loss: 1.3508 | Batch time: 0.02s
2025-03-02 20:48:57,801 - INFO - [VAL] Epoch: 10/30 | Batch: 0/194 (0.5%) | Loss: 1.3091 | Batch time: 0.02s
2025-03-02 20:48:58,150 - INFO - [VAL] Epoch: 10/30 | Batch: 19/194 (10.3%) | Loss: 1.5047 | Batch time: 0.02s
2025-03-02 20:48:58,499 - INFO - [VAL] Epoch: 10/30 | Batch: 38/194 (20.1%) | Loss: 0.4740 | Batch time: 0.02s
2025-03-02 20:48:58,852 - INFO - [VAL] Epoch: 10/30 | Batch: 57/194 (29.9%) | Loss: 0.8782 | Batch time: 0.02s
2025-03-02 20:48:59,204 - INFO - [VAL] Epoch: 10/30 | Batch: 76/194 (39.7%) | Loss: 0.3675 | Batch time: 0.02s
2025-03-02 20:48:59,558 - INFO - [VAL] Epoch: 10/30 | Batch: 95/194 (49.5%) | Loss: 0.9172 | Batch time: 0.02s
2025-03-02 20:48:59,908 - INFO - [VAL] Epoch: 10/30 | Batch: 114/194 (59.3%) | Loss: 1.8097 | Batch time: 0.02s
2025-03-02 20:49:00,257 - INFO - [VAL] Epoch: 10/30 | Batch: 133/194 (69.1%) | Loss: 0.8161 | Batch time: 0.02s
2025-03-02 20:49:00,605 - INFO - [VAL] Epoch: 10/30 | Batch: 152/194 (78.9%) | Loss: 1.1650 | Batch time: 0.02s
2025-03-02 20:49:00,954 - INFO - [VAL] Epoch: 10/30 | Batch: 171/194 (88.7%) | Loss: 0.8414 | Batch time: 0.02s
2025-03-02 20:49:01,302 - INFO - [VAL] Epoch: 10/30 | Batch: 190/194 (98.5%) | Loss: 11.0157 | Batch time: 0.02s
2025-03-02 20:49:01,354 - INFO - [VAL] Epoch: 10/30 | Batch: 193/194 (100.0%) | Loss: 0.4105 | Batch time: 0.01s
2025-03-02 20:49:01,357 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:49:01,357 - INFO - Epoch 10/30 completed in 26.19s
2025-03-02 20:49:01,358 - INFO - Training   - Loss: 0.9708, Accuracy: 0.6861, F1: 0.6893
2025-03-02 20:49:01,358 - INFO - Validation - Loss: 2.0708, Accuracy: 0.7474, F1: 0.7493
2025-03-02 20:49:01,358 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:49:01,562 - INFO - Checkpoint saved: checkpoint_epoch_10.pth (Epoch 10)
2025-03-02 20:49:01,562 - INFO - Epoch 11/30
2025-03-02 20:49:01,562 - INFO - ----------------------------------------
2025-03-02 20:49:01,716 - INFO - [TRAIN] Epoch: 11/30 | Batch: 0/903 (0.1%) | Loss: 0.4977 | Batch time: 0.06s
2025-03-02 20:49:03,971 - INFO - [TRAIN] Epoch: 11/30 | Batch: 90/903 (10.1%) | Loss: 1.4359 | Batch time: 0.02s
2025-03-02 20:49:06,271 - INFO - [TRAIN] Epoch: 11/30 | Batch: 180/903 (20.0%) | Loss: 0.7651 | Batch time: 0.03s
2025-03-02 20:49:08,647 - INFO - [TRAIN] Epoch: 11/30 | Batch: 270/903 (30.0%) | Loss: 0.7878 | Batch time: 0.03s
2025-03-02 20:49:11,024 - INFO - [TRAIN] Epoch: 11/30 | Batch: 360/903 (40.0%) | Loss: 0.9988 | Batch time: 0.03s
2025-03-02 20:49:13,753 - INFO - [TRAIN] Epoch: 11/30 | Batch: 450/903 (49.9%) | Loss: 1.3361 | Batch time: 0.03s
2025-03-02 20:49:16,186 - INFO - [TRAIN] Epoch: 11/30 | Batch: 540/903 (59.9%) | Loss: 0.6607 | Batch time: 0.03s
2025-03-02 20:49:18,571 - INFO - [TRAIN] Epoch: 11/30 | Batch: 630/903 (69.9%) | Loss: 0.3657 | Batch time: 0.03s
2025-03-02 20:49:20,962 - INFO - [TRAIN] Epoch: 11/30 | Batch: 720/903 (79.8%) | Loss: 0.8542 | Batch time: 0.03s
2025-03-02 20:49:23,359 - INFO - [TRAIN] Epoch: 11/30 | Batch: 810/903 (89.8%) | Loss: 0.5651 | Batch time: 0.03s
2025-03-02 20:49:25,673 - INFO - [TRAIN] Epoch: 11/30 | Batch: 900/903 (99.8%) | Loss: 0.6802 | Batch time: 0.02s
2025-03-02 20:49:25,719 - INFO - [TRAIN] Epoch: 11/30 | Batch: 902/903 (100.0%) | Loss: 1.2689 | Batch time: 0.02s
2025-03-02 20:49:25,780 - INFO - [VAL] Epoch: 11/30 | Batch: 0/194 (0.5%) | Loss: 1.2604 | Batch time: 0.02s
2025-03-02 20:49:26,135 - INFO - [VAL] Epoch: 11/30 | Batch: 19/194 (10.3%) | Loss: 1.1644 | Batch time: 0.03s
2025-03-02 20:49:26,482 - INFO - [VAL] Epoch: 11/30 | Batch: 38/194 (20.1%) | Loss: 0.7392 | Batch time: 0.02s
2025-03-02 20:49:26,838 - INFO - [VAL] Epoch: 11/30 | Batch: 57/194 (29.9%) | Loss: 1.0226 | Batch time: 0.02s
2025-03-02 20:49:27,192 - INFO - [VAL] Epoch: 11/30 | Batch: 76/194 (39.7%) | Loss: 0.1429 | Batch time: 0.02s
2025-03-02 20:49:27,549 - INFO - [VAL] Epoch: 11/30 | Batch: 95/194 (49.5%) | Loss: 0.7023 | Batch time: 0.02s
2025-03-02 20:49:27,905 - INFO - [VAL] Epoch: 11/30 | Batch: 114/194 (59.3%) | Loss: 1.7650 | Batch time: 0.02s
2025-03-02 20:49:28,255 - INFO - [VAL] Epoch: 11/30 | Batch: 133/194 (69.1%) | Loss: 0.9003 | Batch time: 0.02s
2025-03-02 20:49:28,606 - INFO - [VAL] Epoch: 11/30 | Batch: 152/194 (78.9%) | Loss: 0.8883 | Batch time: 0.02s
2025-03-02 20:49:28,957 - INFO - [VAL] Epoch: 11/30 | Batch: 171/194 (88.7%) | Loss: 1.0977 | Batch time: 0.02s
2025-03-02 20:49:29,308 - INFO - [VAL] Epoch: 11/30 | Batch: 190/194 (98.5%) | Loss: 1.1227 | Batch time: 0.02s
2025-03-02 20:49:29,359 - INFO - [VAL] Epoch: 11/30 | Batch: 193/194 (100.0%) | Loss: 0.3362 | Batch time: 0.01s
2025-03-02 20:49:29,362 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:49:29,362 - INFO - Epoch 11/30 completed in 27.80s
2025-03-02 20:49:29,362 - INFO - Training   - Loss: 0.9694, Accuracy: 0.6880, F1: 0.6915
2025-03-02 20:49:29,362 - INFO - Validation - Loss: 0.7548, Accuracy: 0.7523, F1: 0.7528
2025-03-02 20:49:29,362 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:49:29,362 - INFO - Epoch 12/30
2025-03-02 20:49:29,362 - INFO - ----------------------------------------
2025-03-02 20:49:29,508 - INFO - [TRAIN] Epoch: 12/30 | Batch: 0/903 (0.1%) | Loss: 0.8032 | Batch time: 0.05s
2025-03-02 20:49:31,981 - INFO - [TRAIN] Epoch: 12/30 | Batch: 90/903 (10.1%) | Loss: 1.2962 | Batch time: 0.03s
2025-03-02 20:49:34,466 - INFO - [TRAIN] Epoch: 12/30 | Batch: 180/903 (20.0%) | Loss: 1.4612 | Batch time: 0.03s
2025-03-02 20:49:36,940 - INFO - [TRAIN] Epoch: 12/30 | Batch: 270/903 (30.0%) | Loss: 0.6347 | Batch time: 0.03s
2025-03-02 20:49:39,450 - INFO - [TRAIN] Epoch: 12/30 | Batch: 360/903 (40.0%) | Loss: 0.6619 | Batch time: 0.03s
2025-03-02 20:49:41,948 - INFO - [TRAIN] Epoch: 12/30 | Batch: 450/903 (49.9%) | Loss: 1.0479 | Batch time: 0.03s
2025-03-02 20:49:44,457 - INFO - [TRAIN] Epoch: 12/30 | Batch: 540/903 (59.9%) | Loss: 1.1602 | Batch time: 0.03s
2025-03-02 20:49:46,968 - INFO - [TRAIN] Epoch: 12/30 | Batch: 630/903 (69.9%) | Loss: 0.9115 | Batch time: 0.03s
2025-03-02 20:49:49,500 - INFO - [TRAIN] Epoch: 12/30 | Batch: 720/903 (79.8%) | Loss: 1.5407 | Batch time: 0.03s
2025-03-02 20:49:52,040 - INFO - [TRAIN] Epoch: 12/30 | Batch: 810/903 (89.8%) | Loss: 1.4422 | Batch time: 0.03s
2025-03-02 20:49:54,483 - INFO - [TRAIN] Epoch: 12/30 | Batch: 900/903 (99.8%) | Loss: 1.4259 | Batch time: 0.02s
2025-03-02 20:49:54,528 - INFO - [TRAIN] Epoch: 12/30 | Batch: 902/903 (100.0%) | Loss: 0.9823 | Batch time: 0.02s
2025-03-02 20:49:54,590 - INFO - [VAL] Epoch: 12/30 | Batch: 0/194 (0.5%) | Loss: 0.8259 | Batch time: 0.02s
2025-03-02 20:49:54,935 - INFO - [VAL] Epoch: 12/30 | Batch: 19/194 (10.3%) | Loss: 1.0015 | Batch time: 0.02s
2025-03-02 20:49:55,289 - INFO - [VAL] Epoch: 12/30 | Batch: 38/194 (20.1%) | Loss: 0.7427 | Batch time: 0.02s
2025-03-02 20:49:55,644 - INFO - [VAL] Epoch: 12/30 | Batch: 57/194 (29.9%) | Loss: 1.0640 | Batch time: 0.02s
2025-03-02 20:49:56,000 - INFO - [VAL] Epoch: 12/30 | Batch: 76/194 (39.7%) | Loss: 0.1588 | Batch time: 0.02s
2025-03-02 20:49:56,360 - INFO - [VAL] Epoch: 12/30 | Batch: 95/194 (49.5%) | Loss: 0.8461 | Batch time: 0.02s
2025-03-02 20:49:56,716 - INFO - [VAL] Epoch: 12/30 | Batch: 114/194 (59.3%) | Loss: 1.0642 | Batch time: 0.02s
2025-03-02 20:49:57,069 - INFO - [VAL] Epoch: 12/30 | Batch: 133/194 (69.1%) | Loss: 0.8868 | Batch time: 0.02s
2025-03-02 20:49:57,423 - INFO - [VAL] Epoch: 12/30 | Batch: 152/194 (78.9%) | Loss: 0.6893 | Batch time: 0.02s
2025-03-02 20:49:57,776 - INFO - [VAL] Epoch: 12/30 | Batch: 171/194 (88.7%) | Loss: 1.1075 | Batch time: 0.02s
2025-03-02 20:49:58,129 - INFO - [VAL] Epoch: 12/30 | Batch: 190/194 (98.5%) | Loss: 0.9561 | Batch time: 0.02s
2025-03-02 20:49:58,180 - INFO - [VAL] Epoch: 12/30 | Batch: 193/194 (100.0%) | Loss: 0.3887 | Batch time: 0.01s
2025-03-02 20:49:58,184 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:49:58,184 - INFO - Epoch 12/30 completed in 28.82s
2025-03-02 20:49:58,184 - INFO - Training   - Loss: 0.9705, Accuracy: 0.6802, F1: 0.6836
2025-03-02 20:49:58,184 - INFO - Validation - Loss: 0.6943, Accuracy: 0.7703, F1: 0.7722
2025-03-02 20:49:58,184 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:49:58,184 - INFO - Epoch 13/30
2025-03-02 20:49:58,184 - INFO - ----------------------------------------
2025-03-02 20:49:58,343 - INFO - [TRAIN] Epoch: 13/30 | Batch: 0/903 (0.1%) | Loss: 0.8442 | Batch time: 0.05s
2025-03-02 20:50:00,897 - INFO - [TRAIN] Epoch: 13/30 | Batch: 90/903 (10.1%) | Loss: 1.2880 | Batch time: 0.03s
2025-03-02 20:50:03,459 - INFO - [TRAIN] Epoch: 13/30 | Batch: 180/903 (20.0%) | Loss: 1.2957 | Batch time: 0.03s
2025-03-02 20:50:05,980 - INFO - [TRAIN] Epoch: 13/30 | Batch: 270/903 (30.0%) | Loss: 0.7362 | Batch time: 0.03s
2025-03-02 20:50:08,564 - INFO - [TRAIN] Epoch: 13/30 | Batch: 360/903 (40.0%) | Loss: 0.9550 | Batch time: 0.03s
2025-03-02 20:50:11,121 - INFO - [TRAIN] Epoch: 13/30 | Batch: 450/903 (49.9%) | Loss: 0.7907 | Batch time: 0.03s
2025-03-02 20:50:13,691 - INFO - [TRAIN] Epoch: 13/30 | Batch: 540/903 (59.9%) | Loss: 1.2253 | Batch time: 0.03s
2025-03-02 20:50:16,324 - INFO - [TRAIN] Epoch: 13/30 | Batch: 630/903 (69.9%) | Loss: 1.1227 | Batch time: 0.03s
2025-03-02 20:50:18,874 - INFO - [TRAIN] Epoch: 13/30 | Batch: 720/903 (79.8%) | Loss: 1.0169 | Batch time: 0.03s
2025-03-02 20:50:21,397 - INFO - [TRAIN] Epoch: 13/30 | Batch: 810/903 (89.8%) | Loss: 0.4573 | Batch time: 0.03s
2025-03-02 20:50:23,796 - INFO - [TRAIN] Epoch: 13/30 | Batch: 900/903 (99.8%) | Loss: 0.7513 | Batch time: 0.02s
2025-03-02 20:50:23,841 - INFO - [TRAIN] Epoch: 13/30 | Batch: 902/903 (100.0%) | Loss: 0.8620 | Batch time: 0.02s
2025-03-02 20:50:23,899 - INFO - [VAL] Epoch: 13/30 | Batch: 0/194 (0.5%) | Loss: 0.8970 | Batch time: 0.02s
2025-03-02 20:50:24,248 - INFO - [VAL] Epoch: 13/30 | Batch: 19/194 (10.3%) | Loss: 0.7861 | Batch time: 0.02s
2025-03-02 20:50:24,594 - INFO - [VAL] Epoch: 13/30 | Batch: 38/194 (20.1%) | Loss: 0.7540 | Batch time: 0.02s
2025-03-02 20:50:24,948 - INFO - [VAL] Epoch: 13/30 | Batch: 57/194 (29.9%) | Loss: 0.9775 | Batch time: 0.02s
2025-03-02 20:50:25,306 - INFO - [VAL] Epoch: 13/30 | Batch: 76/194 (39.7%) | Loss: 0.1840 | Batch time: 0.02s
2025-03-02 20:50:25,663 - INFO - [VAL] Epoch: 13/30 | Batch: 95/194 (49.5%) | Loss: 0.6619 | Batch time: 0.02s
2025-03-02 20:50:26,020 - INFO - [VAL] Epoch: 13/30 | Batch: 114/194 (59.3%) | Loss: 1.3685 | Batch time: 0.02s
2025-03-02 20:50:26,376 - INFO - [VAL] Epoch: 13/30 | Batch: 133/194 (69.1%) | Loss: 0.7486 | Batch time: 0.02s
2025-03-02 20:50:26,734 - INFO - [VAL] Epoch: 13/30 | Batch: 152/194 (78.9%) | Loss: 0.6410 | Batch time: 0.02s
2025-03-02 20:50:27,095 - INFO - [VAL] Epoch: 13/30 | Batch: 171/194 (88.7%) | Loss: 1.4416 | Batch time: 0.02s
2025-03-02 20:50:27,450 - INFO - [VAL] Epoch: 13/30 | Batch: 190/194 (98.5%) | Loss: 0.8338 | Batch time: 0.02s
2025-03-02 20:50:27,502 - INFO - [VAL] Epoch: 13/30 | Batch: 193/194 (100.0%) | Loss: 0.4036 | Batch time: 0.01s
2025-03-02 20:50:27,719 - INFO - Checkpoint saved: efficientnet_b3_v1_best.pth (Epoch 13)
2025-03-02 20:50:27,719 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:50:27,719 - INFO - Epoch 13/30 completed in 29.54s
2025-03-02 20:50:27,719 - INFO - Training   - Loss: 0.9634, Accuracy: 0.6881, F1: 0.6912
2025-03-02 20:50:27,719 - INFO - Validation - Loss: 0.6397, Accuracy: 0.7943, F1: 0.7947
2025-03-02 20:50:27,719 - INFO - Validation F1 improved from 0.7743 to 0.7947
2025-03-02 20:50:27,719 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:50:27,719 - INFO - Epoch 14/30
2025-03-02 20:50:27,719 - INFO - ----------------------------------------
2025-03-02 20:50:27,857 - INFO - [TRAIN] Epoch: 14/30 | Batch: 0/903 (0.1%) | Loss: 0.6019 | Batch time: 0.04s
2025-03-02 20:50:30,376 - INFO - [TRAIN] Epoch: 14/30 | Batch: 90/903 (10.1%) | Loss: 1.0784 | Batch time: 0.03s
2025-03-02 20:50:32,837 - INFO - [TRAIN] Epoch: 14/30 | Batch: 180/903 (20.0%) | Loss: 0.9167 | Batch time: 0.03s
2025-03-02 20:50:35,334 - INFO - [TRAIN] Epoch: 14/30 | Batch: 270/903 (30.0%) | Loss: 1.1763 | Batch time: 0.03s
2025-03-02 20:50:37,946 - INFO - [TRAIN] Epoch: 14/30 | Batch: 360/903 (40.0%) | Loss: 0.6270 | Batch time: 0.03s
2025-03-02 20:50:40,472 - INFO - [TRAIN] Epoch: 14/30 | Batch: 450/903 (49.9%) | Loss: 0.7882 | Batch time: 0.03s
2025-03-02 20:50:43,023 - INFO - [TRAIN] Epoch: 14/30 | Batch: 540/903 (59.9%) | Loss: 1.1239 | Batch time: 0.03s
2025-03-02 20:50:45,553 - INFO - [TRAIN] Epoch: 14/30 | Batch: 630/903 (69.9%) | Loss: 1.4366 | Batch time: 0.03s
2025-03-02 20:50:48,070 - INFO - [TRAIN] Epoch: 14/30 | Batch: 720/903 (79.8%) | Loss: 1.4531 | Batch time: 0.03s
2025-03-02 20:50:50,616 - INFO - [TRAIN] Epoch: 14/30 | Batch: 810/903 (89.8%) | Loss: 1.2738 | Batch time: 0.03s
2025-03-02 20:50:53,026 - INFO - [TRAIN] Epoch: 14/30 | Batch: 900/903 (99.8%) | Loss: 0.7902 | Batch time: 0.02s
2025-03-02 20:50:53,071 - INFO - [TRAIN] Epoch: 14/30 | Batch: 902/903 (100.0%) | Loss: 0.4457 | Batch time: 0.02s
2025-03-02 20:50:53,132 - INFO - [VAL] Epoch: 14/30 | Batch: 0/194 (0.5%) | Loss: 0.8251 | Batch time: 0.02s
2025-03-02 20:50:53,476 - INFO - [VAL] Epoch: 14/30 | Batch: 19/194 (10.3%) | Loss: 1.0399 | Batch time: 0.02s
2025-03-02 20:50:53,822 - INFO - [VAL] Epoch: 14/30 | Batch: 38/194 (20.1%) | Loss: 0.5546 | Batch time: 0.02s
2025-03-02 20:50:54,175 - INFO - [VAL] Epoch: 14/30 | Batch: 57/194 (29.9%) | Loss: 0.8931 | Batch time: 0.02s
2025-03-02 20:50:54,537 - INFO - [VAL] Epoch: 14/30 | Batch: 76/194 (39.7%) | Loss: 0.2001 | Batch time: 0.02s
2025-03-02 20:50:54,894 - INFO - [VAL] Epoch: 14/30 | Batch: 95/194 (49.5%) | Loss: 0.8450 | Batch time: 0.02s
2025-03-02 20:50:55,251 - INFO - [VAL] Epoch: 14/30 | Batch: 114/194 (59.3%) | Loss: 1.4843 | Batch time: 0.02s
2025-03-02 20:50:55,607 - INFO - [VAL] Epoch: 14/30 | Batch: 133/194 (69.1%) | Loss: 0.6974 | Batch time: 0.02s
2025-03-02 20:50:55,963 - INFO - [VAL] Epoch: 14/30 | Batch: 152/194 (78.9%) | Loss: 0.8398 | Batch time: 0.02s
2025-03-02 20:50:56,318 - INFO - [VAL] Epoch: 14/30 | Batch: 171/194 (88.7%) | Loss: 1.0993 | Batch time: 0.02s
2025-03-02 20:50:56,671 - INFO - [VAL] Epoch: 14/30 | Batch: 190/194 (98.5%) | Loss: 1.0128 | Batch time: 0.02s
2025-03-02 20:50:56,722 - INFO - [VAL] Epoch: 14/30 | Batch: 193/194 (100.0%) | Loss: 0.4295 | Batch time: 0.01s
2025-03-02 20:50:56,725 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:50:56,725 - INFO - Epoch 14/30 completed in 29.01s
2025-03-02 20:50:56,725 - INFO - Training   - Loss: 0.9682, Accuracy: 0.6904, F1: 0.6936
2025-03-02 20:50:56,725 - INFO - Validation - Loss: 0.7136, Accuracy: 0.7826, F1: 0.7829
2025-03-02 20:50:56,725 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:50:56,725 - INFO - Epoch 15/30
2025-03-02 20:50:56,725 - INFO - ----------------------------------------
2025-03-02 20:50:56,881 - INFO - [TRAIN] Epoch: 15/30 | Batch: 0/903 (0.1%) | Loss: 0.5174 | Batch time: 0.05s
2025-03-02 20:50:59,452 - INFO - [TRAIN] Epoch: 15/30 | Batch: 90/903 (10.1%) | Loss: 0.3891 | Batch time: 0.03s
2025-03-02 20:51:02,020 - INFO - [TRAIN] Epoch: 15/30 | Batch: 180/903 (20.0%) | Loss: 0.4078 | Batch time: 0.03s
2025-03-02 20:51:04,588 - INFO - [TRAIN] Epoch: 15/30 | Batch: 270/903 (30.0%) | Loss: 0.8406 | Batch time: 0.03s
2025-03-02 20:51:07,187 - INFO - [TRAIN] Epoch: 15/30 | Batch: 360/903 (40.0%) | Loss: 1.0224 | Batch time: 0.03s
2025-03-02 20:51:09,722 - INFO - [TRAIN] Epoch: 15/30 | Batch: 450/903 (49.9%) | Loss: 0.6056 | Batch time: 0.03s
2025-03-02 20:51:12,309 - INFO - [TRAIN] Epoch: 15/30 | Batch: 540/903 (59.9%) | Loss: 0.8193 | Batch time: 0.03s
2025-03-02 20:51:14,961 - INFO - [TRAIN] Epoch: 15/30 | Batch: 630/903 (69.9%) | Loss: 1.6274 | Batch time: 0.03s
2025-03-02 20:51:17,465 - INFO - [TRAIN] Epoch: 15/30 | Batch: 720/903 (79.8%) | Loss: 1.1409 | Batch time: 0.03s
2025-03-02 20:51:20,046 - INFO - [TRAIN] Epoch: 15/30 | Batch: 810/903 (89.8%) | Loss: 0.4327 | Batch time: 0.03s
2025-03-02 20:51:22,466 - INFO - [TRAIN] Epoch: 15/30 | Batch: 900/903 (99.8%) | Loss: 1.1585 | Batch time: 0.02s
2025-03-02 20:51:22,510 - INFO - [TRAIN] Epoch: 15/30 | Batch: 902/903 (100.0%) | Loss: 0.8476 | Batch time: 0.02s
2025-03-02 20:51:22,575 - INFO - [VAL] Epoch: 15/30 | Batch: 0/194 (0.5%) | Loss: 0.5367 | Batch time: 0.02s
2025-03-02 20:51:22,919 - INFO - [VAL] Epoch: 15/30 | Batch: 19/194 (10.3%) | Loss: 0.8168 | Batch time: 0.02s
2025-03-02 20:51:23,259 - INFO - [VAL] Epoch: 15/30 | Batch: 38/194 (20.1%) | Loss: 0.3980 | Batch time: 0.02s
2025-03-02 20:51:23,609 - INFO - [VAL] Epoch: 15/30 | Batch: 57/194 (29.9%) | Loss: 0.6488 | Batch time: 0.02s
2025-03-02 20:51:23,963 - INFO - [VAL] Epoch: 15/30 | Batch: 76/194 (39.7%) | Loss: 0.1392 | Batch time: 0.02s
2025-03-02 20:51:24,318 - INFO - [VAL] Epoch: 15/30 | Batch: 95/194 (49.5%) | Loss: 0.5849 | Batch time: 0.02s
2025-03-02 20:51:24,679 - INFO - [VAL] Epoch: 15/30 | Batch: 114/194 (59.3%) | Loss: 0.8895 | Batch time: 0.02s
2025-03-02 20:51:25,032 - INFO - [VAL] Epoch: 15/30 | Batch: 133/194 (69.1%) | Loss: 0.6546 | Batch time: 0.02s
2025-03-02 20:51:25,391 - INFO - [VAL] Epoch: 15/30 | Batch: 152/194 (78.9%) | Loss: 0.6872 | Batch time: 0.02s
2025-03-02 20:51:25,741 - INFO - [VAL] Epoch: 15/30 | Batch: 171/194 (88.7%) | Loss: 1.1119 | Batch time: 0.02s
2025-03-02 20:51:26,090 - INFO - [VAL] Epoch: 15/30 | Batch: 190/194 (98.5%) | Loss: 0.7163 | Batch time: 0.02s
2025-03-02 20:51:26,152 - INFO - [VAL] Epoch: 15/30 | Batch: 193/194 (100.0%) | Loss: 0.2882 | Batch time: 0.02s
2025-03-02 20:51:26,372 - INFO - Checkpoint saved: efficientnet_b3_v1_best.pth (Epoch 15)
2025-03-02 20:51:26,372 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:51:26,372 - INFO - Epoch 15/30 completed in 29.65s
2025-03-02 20:51:26,372 - INFO - Training   - Loss: 0.9524, Accuracy: 0.6900, F1: 0.6929
2025-03-02 20:51:26,372 - INFO - Validation - Loss: 0.5786, Accuracy: 0.8149, F1: 0.8146
2025-03-02 20:51:26,372 - INFO - Validation F1 improved from 0.7947 to 0.8146
2025-03-02 20:51:26,372 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:51:26,501 - INFO - Checkpoint saved: checkpoint_epoch_15.pth (Epoch 15)
2025-03-02 20:51:26,501 - INFO - Epoch 16/30
2025-03-02 20:51:26,501 - INFO - ----------------------------------------
2025-03-02 20:51:26,639 - INFO - [TRAIN] Epoch: 16/30 | Batch: 0/903 (0.1%) | Loss: 0.7725 | Batch time: 0.04s
2025-03-02 20:51:29,121 - INFO - [TRAIN] Epoch: 16/30 | Batch: 90/903 (10.1%) | Loss: 0.5628 | Batch time: 0.03s
2025-03-02 20:51:31,555 - INFO - [TRAIN] Epoch: 16/30 | Batch: 180/903 (20.0%) | Loss: 0.6569 | Batch time: 0.03s
2025-03-02 20:51:34,036 - INFO - [TRAIN] Epoch: 16/30 | Batch: 270/903 (30.0%) | Loss: 1.6235 | Batch time: 0.03s
2025-03-02 20:51:36,505 - INFO - [TRAIN] Epoch: 16/30 | Batch: 360/903 (40.0%) | Loss: 0.4798 | Batch time: 0.03s
2025-03-02 20:51:39,272 - INFO - [TRAIN] Epoch: 16/30 | Batch: 450/903 (49.9%) | Loss: 0.2444 | Batch time: 0.03s
2025-03-02 20:51:41,697 - INFO - [TRAIN] Epoch: 16/30 | Batch: 540/903 (59.9%) | Loss: 0.8903 | Batch time: 0.03s
2025-03-02 20:51:44,133 - INFO - [TRAIN] Epoch: 16/30 | Batch: 630/903 (69.9%) | Loss: 0.8913 | Batch time: 0.03s
2025-03-02 20:51:46,602 - INFO - [TRAIN] Epoch: 16/30 | Batch: 720/903 (79.8%) | Loss: 0.7553 | Batch time: 0.03s
2025-03-02 20:51:49,069 - INFO - [TRAIN] Epoch: 16/30 | Batch: 810/903 (89.8%) | Loss: 0.3964 | Batch time: 0.03s
2025-03-02 20:51:51,420 - INFO - [TRAIN] Epoch: 16/30 | Batch: 900/903 (99.8%) | Loss: 1.0587 | Batch time: 0.02s
2025-03-02 20:51:51,466 - INFO - [TRAIN] Epoch: 16/30 | Batch: 902/903 (100.0%) | Loss: 1.2419 | Batch time: 0.02s
2025-03-02 20:51:51,530 - INFO - [VAL] Epoch: 16/30 | Batch: 0/194 (0.5%) | Loss: 1.2682 | Batch time: 0.02s
2025-03-02 20:51:51,877 - INFO - [VAL] Epoch: 16/30 | Batch: 19/194 (10.3%) | Loss: 0.8846 | Batch time: 0.02s
2025-03-02 20:51:52,225 - INFO - [VAL] Epoch: 16/30 | Batch: 38/194 (20.1%) | Loss: 0.6078 | Batch time: 0.02s
2025-03-02 20:51:52,578 - INFO - [VAL] Epoch: 16/30 | Batch: 57/194 (29.9%) | Loss: 0.7691 | Batch time: 0.02s
2025-03-02 20:51:52,933 - INFO - [VAL] Epoch: 16/30 | Batch: 76/194 (39.7%) | Loss: 0.1863 | Batch time: 0.02s
2025-03-02 20:51:53,288 - INFO - [VAL] Epoch: 16/30 | Batch: 95/194 (49.5%) | Loss: 0.6304 | Batch time: 0.02s
2025-03-02 20:51:53,643 - INFO - [VAL] Epoch: 16/30 | Batch: 114/194 (59.3%) | Loss: 1.1799 | Batch time: 0.02s
2025-03-02 20:51:53,998 - INFO - [VAL] Epoch: 16/30 | Batch: 133/194 (69.1%) | Loss: 0.7685 | Batch time: 0.02s
2025-03-02 20:51:54,353 - INFO - [VAL] Epoch: 16/30 | Batch: 152/194 (78.9%) | Loss: 0.7873 | Batch time: 0.02s
2025-03-02 20:51:54,708 - INFO - [VAL] Epoch: 16/30 | Batch: 171/194 (88.7%) | Loss: 1.3171 | Batch time: 0.02s
2025-03-02 20:51:55,062 - INFO - [VAL] Epoch: 16/30 | Batch: 190/194 (98.5%) | Loss: 0.8930 | Batch time: 0.02s
2025-03-02 20:51:55,113 - INFO - [VAL] Epoch: 16/30 | Batch: 193/194 (100.0%) | Loss: 0.3582 | Batch time: 0.01s
2025-03-02 20:51:55,116 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:51:55,116 - INFO - Epoch 16/30 completed in 28.61s
2025-03-02 20:51:55,116 - INFO - Training   - Loss: 0.9541, Accuracy: 0.6909, F1: 0.6938
2025-03-02 20:51:55,116 - INFO - Validation - Loss: 0.6402, Accuracy: 0.7810, F1: 0.7822
2025-03-02 20:51:55,116 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:51:55,116 - INFO - Epoch 17/30
2025-03-02 20:51:55,116 - INFO - ----------------------------------------
2025-03-02 20:51:55,255 - INFO - [TRAIN] Epoch: 17/30 | Batch: 0/903 (0.1%) | Loss: 0.6758 | Batch time: 0.04s
2025-03-02 20:51:57,785 - INFO - [TRAIN] Epoch: 17/30 | Batch: 90/903 (10.1%) | Loss: 1.8081 | Batch time: 0.03s
2025-03-02 20:52:00,295 - INFO - [TRAIN] Epoch: 17/30 | Batch: 180/903 (20.0%) | Loss: 0.7152 | Batch time: 0.03s
2025-03-02 20:52:02,796 - INFO - [TRAIN] Epoch: 17/30 | Batch: 270/903 (30.0%) | Loss: 0.9890 | Batch time: 0.03s
2025-03-02 20:52:05,318 - INFO - [TRAIN] Epoch: 17/30 | Batch: 360/903 (40.0%) | Loss: 0.3524 | Batch time: 0.03s
2025-03-02 20:52:07,837 - INFO - [TRAIN] Epoch: 17/30 | Batch: 450/903 (49.9%) | Loss: 1.4069 | Batch time: 0.03s
2025-03-02 20:52:10,400 - INFO - [TRAIN] Epoch: 17/30 | Batch: 540/903 (59.9%) | Loss: 0.9141 | Batch time: 0.03s
2025-03-02 20:52:12,908 - INFO - [TRAIN] Epoch: 17/30 | Batch: 630/903 (69.9%) | Loss: 0.7559 | Batch time: 0.03s
2025-03-02 20:52:15,556 - INFO - [TRAIN] Epoch: 17/30 | Batch: 720/903 (79.8%) | Loss: 0.7276 | Batch time: 0.03s
2025-03-02 20:52:18,036 - INFO - [TRAIN] Epoch: 17/30 | Batch: 810/903 (89.8%) | Loss: 0.8032 | Batch time: 0.03s
2025-03-02 20:52:20,405 - INFO - [TRAIN] Epoch: 17/30 | Batch: 900/903 (99.8%) | Loss: 0.9607 | Batch time: 0.02s
2025-03-02 20:52:20,450 - INFO - [TRAIN] Epoch: 17/30 | Batch: 902/903 (100.0%) | Loss: 1.9274 | Batch time: 0.02s
2025-03-02 20:52:20,514 - INFO - [VAL] Epoch: 17/30 | Batch: 0/194 (0.5%) | Loss: 1.1499 | Batch time: 0.02s
2025-03-02 20:52:20,859 - INFO - [VAL] Epoch: 17/30 | Batch: 19/194 (10.3%) | Loss: 1.4212 | Batch time: 0.02s
2025-03-02 20:52:21,203 - INFO - [VAL] Epoch: 17/30 | Batch: 38/194 (20.1%) | Loss: 0.4706 | Batch time: 0.02s
2025-03-02 20:52:21,552 - INFO - [VAL] Epoch: 17/30 | Batch: 57/194 (29.9%) | Loss: 0.9932 | Batch time: 0.02s
2025-03-02 20:52:21,904 - INFO - [VAL] Epoch: 17/30 | Batch: 76/194 (39.7%) | Loss: 0.2505 | Batch time: 0.02s
2025-03-02 20:52:22,257 - INFO - [VAL] Epoch: 17/30 | Batch: 95/194 (49.5%) | Loss: 0.6767 | Batch time: 0.02s
2025-03-02 20:52:22,609 - INFO - [VAL] Epoch: 17/30 | Batch: 114/194 (59.3%) | Loss: 1.8896 | Batch time: 0.02s
2025-03-02 20:52:22,961 - INFO - [VAL] Epoch: 17/30 | Batch: 133/194 (69.1%) | Loss: 0.7908 | Batch time: 0.02s
2025-03-02 20:52:23,315 - INFO - [VAL] Epoch: 17/30 | Batch: 152/194 (78.9%) | Loss: 0.9052 | Batch time: 0.02s
2025-03-02 20:52:23,671 - INFO - [VAL] Epoch: 17/30 | Batch: 171/194 (88.7%) | Loss: 0.8796 | Batch time: 0.02s
2025-03-02 20:52:24,019 - INFO - [VAL] Epoch: 17/30 | Batch: 190/194 (98.5%) | Loss: 0.9156 | Batch time: 0.02s
2025-03-02 20:52:24,070 - INFO - [VAL] Epoch: 17/30 | Batch: 193/194 (100.0%) | Loss: 0.5417 | Batch time: 0.01s
2025-03-02 20:52:24,073 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:52:24,073 - INFO - Epoch 17/30 completed in 28.96s
2025-03-02 20:52:24,073 - INFO - Training   - Loss: 0.9343, Accuracy: 0.6989, F1: 0.7018
2025-03-02 20:52:24,073 - INFO - Validation - Loss: 2.3774, Accuracy: 0.7610, F1: 0.7627
2025-03-02 20:52:24,073 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:52:24,073 - INFO - Epoch 18/30
2025-03-02 20:52:24,073 - INFO - ----------------------------------------
2025-03-02 20:52:24,207 - INFO - [TRAIN] Epoch: 18/30 | Batch: 0/903 (0.1%) | Loss: 1.7611 | Batch time: 0.04s
2025-03-02 20:52:26,736 - INFO - [TRAIN] Epoch: 18/30 | Batch: 90/903 (10.1%) | Loss: 0.5249 | Batch time: 0.03s
2025-03-02 20:52:29,230 - INFO - [TRAIN] Epoch: 18/30 | Batch: 180/903 (20.0%) | Loss: 1.0538 | Batch time: 0.03s
2025-03-02 20:52:31,743 - INFO - [TRAIN] Epoch: 18/30 | Batch: 270/903 (30.0%) | Loss: 1.1127 | Batch time: 0.03s
2025-03-02 20:52:34,271 - INFO - [TRAIN] Epoch: 18/30 | Batch: 360/903 (40.0%) | Loss: 0.6678 | Batch time: 0.03s
2025-03-02 20:52:36,759 - INFO - [TRAIN] Epoch: 18/30 | Batch: 450/903 (49.9%) | Loss: 0.3428 | Batch time: 0.03s
2025-03-02 20:52:39,285 - INFO - [TRAIN] Epoch: 18/30 | Batch: 540/903 (59.9%) | Loss: 0.7168 | Batch time: 0.03s
2025-03-02 20:52:41,782 - INFO - [TRAIN] Epoch: 18/30 | Batch: 630/903 (69.9%) | Loss: 1.3814 | Batch time: 0.03s
2025-03-02 20:52:44,308 - INFO - [TRAIN] Epoch: 18/30 | Batch: 720/903 (79.8%) | Loss: 0.9754 | Batch time: 0.03s
2025-03-02 20:52:46,823 - INFO - [TRAIN] Epoch: 18/30 | Batch: 810/903 (89.8%) | Loss: 1.0275 | Batch time: 0.03s
2025-03-02 20:52:49,240 - INFO - [TRAIN] Epoch: 18/30 | Batch: 900/903 (99.8%) | Loss: 1.3479 | Batch time: 0.02s
2025-03-02 20:52:49,284 - INFO - [TRAIN] Epoch: 18/30 | Batch: 902/903 (100.0%) | Loss: 1.0021 | Batch time: 0.02s
2025-03-02 20:52:49,345 - INFO - [VAL] Epoch: 18/30 | Batch: 0/194 (0.5%) | Loss: 0.7962 | Batch time: 0.02s
2025-03-02 20:52:49,686 - INFO - [VAL] Epoch: 18/30 | Batch: 19/194 (10.3%) | Loss: 1.2533 | Batch time: 0.02s
2025-03-02 20:52:50,029 - INFO - [VAL] Epoch: 18/30 | Batch: 38/194 (20.1%) | Loss: 0.3686 | Batch time: 0.02s
2025-03-02 20:52:50,377 - INFO - [VAL] Epoch: 18/30 | Batch: 57/194 (29.9%) | Loss: 1.0251 | Batch time: 0.02s
2025-03-02 20:52:50,735 - INFO - [VAL] Epoch: 18/30 | Batch: 76/194 (39.7%) | Loss: 0.2608 | Batch time: 0.02s
2025-03-02 20:52:51,085 - INFO - [VAL] Epoch: 18/30 | Batch: 95/194 (49.5%) | Loss: 0.7278 | Batch time: 0.02s
2025-03-02 20:52:51,436 - INFO - [VAL] Epoch: 18/30 | Batch: 114/194 (59.3%) | Loss: 1.7521 | Batch time: 0.02s
2025-03-02 20:52:51,786 - INFO - [VAL] Epoch: 18/30 | Batch: 133/194 (69.1%) | Loss: 0.8694 | Batch time: 0.02s
2025-03-02 20:52:52,136 - INFO - [VAL] Epoch: 18/30 | Batch: 152/194 (78.9%) | Loss: 0.7869 | Batch time: 0.02s
2025-03-02 20:52:52,485 - INFO - [VAL] Epoch: 18/30 | Batch: 171/194 (88.7%) | Loss: 0.8963 | Batch time: 0.02s
2025-03-02 20:52:52,835 - INFO - [VAL] Epoch: 18/30 | Batch: 190/194 (98.5%) | Loss: 1.0702 | Batch time: 0.02s
2025-03-02 20:52:52,886 - INFO - [VAL] Epoch: 18/30 | Batch: 193/194 (100.0%) | Loss: 0.4219 | Batch time: 0.01s
2025-03-02 20:52:52,889 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:52:52,889 - INFO - Epoch 18/30 completed in 28.82s
2025-03-02 20:52:52,889 - INFO - Training   - Loss: 0.9268, Accuracy: 0.6983, F1: 0.7007
2025-03-02 20:52:52,889 - INFO - Validation - Loss: 0.8184, Accuracy: 0.7687, F1: 0.7685
2025-03-02 20:52:52,889 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:52:52,889 - INFO - Epoch 19/30
2025-03-02 20:52:52,889 - INFO - ----------------------------------------
2025-03-02 20:52:53,048 - INFO - [TRAIN] Epoch: 19/30 | Batch: 0/903 (0.1%) | Loss: 0.8740 | Batch time: 0.06s
2025-03-02 20:52:55,528 - INFO - [TRAIN] Epoch: 19/30 | Batch: 90/903 (10.1%) | Loss: 1.0411 | Batch time: 0.03s
2025-03-02 20:52:58,004 - INFO - [TRAIN] Epoch: 19/30 | Batch: 180/903 (20.0%) | Loss: 0.4371 | Batch time: 0.03s
2025-03-02 20:53:00,461 - INFO - [TRAIN] Epoch: 19/30 | Batch: 270/903 (30.0%) | Loss: 0.5814 | Batch time: 0.03s
2025-03-02 20:53:02,931 - INFO - [TRAIN] Epoch: 19/30 | Batch: 360/903 (40.0%) | Loss: 1.8067 | Batch time: 0.03s
2025-03-02 20:53:05,401 - INFO - [TRAIN] Epoch: 19/30 | Batch: 450/903 (49.9%) | Loss: 1.0839 | Batch time: 0.03s
2025-03-02 20:53:07,894 - INFO - [TRAIN] Epoch: 19/30 | Batch: 540/903 (59.9%) | Loss: 1.4287 | Batch time: 0.03s
2025-03-02 20:53:10,319 - INFO - [TRAIN] Epoch: 19/30 | Batch: 630/903 (69.9%) | Loss: 0.7876 | Batch time: 0.03s
2025-03-02 20:53:12,766 - INFO - [TRAIN] Epoch: 19/30 | Batch: 720/903 (79.8%) | Loss: 1.3302 | Batch time: 0.03s
2025-03-02 20:53:15,293 - INFO - [TRAIN] Epoch: 19/30 | Batch: 810/903 (89.8%) | Loss: 1.0462 | Batch time: 0.03s
2025-03-02 20:53:17,673 - INFO - [TRAIN] Epoch: 19/30 | Batch: 900/903 (99.8%) | Loss: 0.7044 | Batch time: 0.02s
2025-03-02 20:53:17,720 - INFO - [TRAIN] Epoch: 19/30 | Batch: 902/903 (100.0%) | Loss: 0.6096 | Batch time: 0.02s
2025-03-02 20:53:17,779 - INFO - [VAL] Epoch: 19/30 | Batch: 0/194 (0.5%) | Loss: 0.7201 | Batch time: 0.02s
2025-03-02 20:53:18,124 - INFO - [VAL] Epoch: 19/30 | Batch: 19/194 (10.3%) | Loss: 0.8058 | Batch time: 0.02s
2025-03-02 20:53:18,472 - INFO - [VAL] Epoch: 19/30 | Batch: 38/194 (20.1%) | Loss: 0.5151 | Batch time: 0.02s
2025-03-02 20:53:18,824 - INFO - [VAL] Epoch: 19/30 | Batch: 57/194 (29.9%) | Loss: 0.7825 | Batch time: 0.02s
2025-03-02 20:53:19,178 - INFO - [VAL] Epoch: 19/30 | Batch: 76/194 (39.7%) | Loss: 0.1522 | Batch time: 0.02s
2025-03-02 20:53:19,531 - INFO - [VAL] Epoch: 19/30 | Batch: 95/194 (49.5%) | Loss: 0.5961 | Batch time: 0.02s
2025-03-02 20:53:19,888 - INFO - [VAL] Epoch: 19/30 | Batch: 114/194 (59.3%) | Loss: 0.6590 | Batch time: 0.02s
2025-03-02 20:53:20,241 - INFO - [VAL] Epoch: 19/30 | Batch: 133/194 (69.1%) | Loss: 0.8300 | Batch time: 0.02s
2025-03-02 20:53:20,593 - INFO - [VAL] Epoch: 19/30 | Batch: 152/194 (78.9%) | Loss: 0.6778 | Batch time: 0.02s
2025-03-02 20:53:20,945 - INFO - [VAL] Epoch: 19/30 | Batch: 171/194 (88.7%) | Loss: 1.1951 | Batch time: 0.02s
2025-03-02 20:53:21,296 - INFO - [VAL] Epoch: 19/30 | Batch: 190/194 (98.5%) | Loss: 0.8305 | Batch time: 0.02s
2025-03-02 20:53:21,347 - INFO - [VAL] Epoch: 19/30 | Batch: 193/194 (100.0%) | Loss: 0.2959 | Batch time: 0.01s
2025-03-02 20:53:21,350 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:53:21,350 - INFO - Epoch 19/30 completed in 28.46s
2025-03-02 20:53:21,350 - INFO - Training   - Loss: 0.9555, Accuracy: 0.6908, F1: 0.6946
2025-03-02 20:53:21,350 - INFO - Validation - Loss: 0.5970, Accuracy: 0.8081, F1: 0.8068
2025-03-02 20:53:21,350 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:53:21,350 - INFO - Epoch 20/30
2025-03-02 20:53:21,350 - INFO - ----------------------------------------
2025-03-02 20:53:21,500 - INFO - [TRAIN] Epoch: 20/30 | Batch: 0/903 (0.1%) | Loss: 1.0110 | Batch time: 0.06s
2025-03-02 20:53:23,941 - INFO - [TRAIN] Epoch: 20/30 | Batch: 90/903 (10.1%) | Loss: 0.9205 | Batch time: 0.03s
2025-03-02 20:53:26,369 - INFO - [TRAIN] Epoch: 20/30 | Batch: 180/903 (20.0%) | Loss: 0.7015 | Batch time: 0.03s
2025-03-02 20:53:28,831 - INFO - [TRAIN] Epoch: 20/30 | Batch: 270/903 (30.0%) | Loss: 0.7661 | Batch time: 0.03s
2025-03-02 20:53:31,247 - INFO - [TRAIN] Epoch: 20/30 | Batch: 360/903 (40.0%) | Loss: 0.5735 | Batch time: 0.03s
2025-03-02 20:53:33,709 - INFO - [TRAIN] Epoch: 20/30 | Batch: 450/903 (49.9%) | Loss: 0.6447 | Batch time: 0.03s
2025-03-02 20:53:36,268 - INFO - [TRAIN] Epoch: 20/30 | Batch: 540/903 (59.9%) | Loss: 1.2317 | Batch time: 0.03s
2025-03-02 20:53:38,790 - INFO - [TRAIN] Epoch: 20/30 | Batch: 630/903 (69.9%) | Loss: 0.9739 | Batch time: 0.03s
2025-03-02 20:53:41,242 - INFO - [TRAIN] Epoch: 20/30 | Batch: 720/903 (79.8%) | Loss: 0.3203 | Batch time: 0.03s
2025-03-02 20:53:43,679 - INFO - [TRAIN] Epoch: 20/30 | Batch: 810/903 (89.8%) | Loss: 1.5379 | Batch time: 0.03s
2025-03-02 20:53:46,044 - INFO - [TRAIN] Epoch: 20/30 | Batch: 900/903 (99.8%) | Loss: 1.1835 | Batch time: 0.02s
2025-03-02 20:53:46,088 - INFO - [TRAIN] Epoch: 20/30 | Batch: 902/903 (100.0%) | Loss: 0.8500 | Batch time: 0.02s
2025-03-02 20:53:46,146 - INFO - [VAL] Epoch: 20/30 | Batch: 0/194 (0.5%) | Loss: 1.1341 | Batch time: 0.02s
2025-03-02 20:53:46,489 - INFO - [VAL] Epoch: 20/30 | Batch: 19/194 (10.3%) | Loss: 1.0744 | Batch time: 0.02s
2025-03-02 20:53:46,834 - INFO - [VAL] Epoch: 20/30 | Batch: 38/194 (20.1%) | Loss: 0.4134 | Batch time: 0.02s
2025-03-02 20:53:47,183 - INFO - [VAL] Epoch: 20/30 | Batch: 57/194 (29.9%) | Loss: 0.9315 | Batch time: 0.02s
2025-03-02 20:53:47,533 - INFO - [VAL] Epoch: 20/30 | Batch: 76/194 (39.7%) | Loss: 0.2124 | Batch time: 0.02s
2025-03-02 20:53:47,888 - INFO - [VAL] Epoch: 20/30 | Batch: 95/194 (49.5%) | Loss: 0.7756 | Batch time: 0.02s
2025-03-02 20:53:48,238 - INFO - [VAL] Epoch: 20/30 | Batch: 114/194 (59.3%) | Loss: 1.8052 | Batch time: 0.02s
2025-03-02 20:53:48,588 - INFO - [VAL] Epoch: 20/30 | Batch: 133/194 (69.1%) | Loss: 0.8106 | Batch time: 0.02s
2025-03-02 20:53:48,938 - INFO - [VAL] Epoch: 20/30 | Batch: 152/194 (78.9%) | Loss: 0.9525 | Batch time: 0.02s
2025-03-02 20:53:49,286 - INFO - [VAL] Epoch: 20/30 | Batch: 171/194 (88.7%) | Loss: 1.0072 | Batch time: 0.02s
2025-03-02 20:53:49,632 - INFO - [VAL] Epoch: 20/30 | Batch: 190/194 (98.5%) | Loss: 0.8751 | Batch time: 0.02s
2025-03-02 20:53:49,683 - INFO - [VAL] Epoch: 20/30 | Batch: 193/194 (100.0%) | Loss: 0.4439 | Batch time: 0.01s
2025-03-02 20:53:49,686 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:53:49,686 - INFO - Epoch 20/30 completed in 28.34s
2025-03-02 20:53:49,686 - INFO - Training   - Loss: 0.9379, Accuracy: 0.6968, F1: 0.7003
2025-03-02 20:53:49,686 - INFO - Validation - Loss: 0.7995, Accuracy: 0.7668, F1: 0.7681
2025-03-02 20:53:49,686 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:53:49,831 - INFO - Checkpoint saved: checkpoint_epoch_20.pth (Epoch 20)
2025-03-02 20:53:49,831 - INFO - Epoch 21/30
2025-03-02 20:53:49,831 - INFO - ----------------------------------------
2025-03-02 20:53:49,988 - INFO - [TRAIN] Epoch: 21/30 | Batch: 0/903 (0.1%) | Loss: 0.3360 | Batch time: 0.06s
2025-03-02 20:53:52,365 - INFO - [TRAIN] Epoch: 21/30 | Batch: 90/903 (10.1%) | Loss: 0.7853 | Batch time: 0.03s
2025-03-02 20:53:54,757 - INFO - [TRAIN] Epoch: 21/30 | Batch: 180/903 (20.0%) | Loss: 1.1621 | Batch time: 0.03s
2025-03-02 20:53:57,188 - INFO - [TRAIN] Epoch: 21/30 | Batch: 270/903 (30.0%) | Loss: 1.0159 | Batch time: 0.03s
2025-03-02 20:53:59,767 - INFO - [TRAIN] Epoch: 21/30 | Batch: 360/903 (40.0%) | Loss: 0.8199 | Batch time: 0.03s
2025-03-02 20:54:02,468 - INFO - [TRAIN] Epoch: 21/30 | Batch: 450/903 (49.9%) | Loss: 0.8231 | Batch time: 0.03s
2025-03-02 20:54:04,866 - INFO - [TRAIN] Epoch: 21/30 | Batch: 540/903 (59.9%) | Loss: 2.0615 | Batch time: 0.03s
2025-03-02 20:54:07,252 - INFO - [TRAIN] Epoch: 21/30 | Batch: 630/903 (69.9%) | Loss: 0.6610 | Batch time: 0.03s
2025-03-02 20:54:09,659 - INFO - [TRAIN] Epoch: 21/30 | Batch: 720/903 (79.8%) | Loss: 0.6597 | Batch time: 0.03s
2025-03-02 20:54:12,076 - INFO - [TRAIN] Epoch: 21/30 | Batch: 810/903 (89.8%) | Loss: 0.7652 | Batch time: 0.03s
2025-03-02 20:54:14,420 - INFO - [TRAIN] Epoch: 21/30 | Batch: 900/903 (99.8%) | Loss: 1.1506 | Batch time: 0.02s
2025-03-02 20:54:14,470 - INFO - [TRAIN] Epoch: 21/30 | Batch: 902/903 (100.0%) | Loss: 0.9810 | Batch time: 0.02s
2025-03-02 20:54:14,536 - INFO - [VAL] Epoch: 21/30 | Batch: 0/194 (0.5%) | Loss: 0.7338 | Batch time: 0.03s
2025-03-02 20:54:14,882 - INFO - [VAL] Epoch: 21/30 | Batch: 19/194 (10.3%) | Loss: 0.7647 | Batch time: 0.02s
2025-03-02 20:54:15,229 - INFO - [VAL] Epoch: 21/30 | Batch: 38/194 (20.1%) | Loss: 0.6081 | Batch time: 0.02s
2025-03-02 20:54:15,581 - INFO - [VAL] Epoch: 21/30 | Batch: 57/194 (29.9%) | Loss: 0.9331 | Batch time: 0.02s
2025-03-02 20:54:15,941 - INFO - [VAL] Epoch: 21/30 | Batch: 76/194 (39.7%) | Loss: 0.1537 | Batch time: 0.02s
2025-03-02 20:54:16,294 - INFO - [VAL] Epoch: 21/30 | Batch: 95/194 (49.5%) | Loss: 0.7485 | Batch time: 0.02s
2025-03-02 20:54:16,648 - INFO - [VAL] Epoch: 21/30 | Batch: 114/194 (59.3%) | Loss: 0.7672 | Batch time: 0.02s
2025-03-02 20:54:17,002 - INFO - [VAL] Epoch: 21/30 | Batch: 133/194 (69.1%) | Loss: 0.7869 | Batch time: 0.02s
2025-03-02 20:54:17,356 - INFO - [VAL] Epoch: 21/30 | Batch: 152/194 (78.9%) | Loss: 0.5685 | Batch time: 0.02s
2025-03-02 20:54:17,707 - INFO - [VAL] Epoch: 21/30 | Batch: 171/194 (88.7%) | Loss: 1.2935 | Batch time: 0.02s
2025-03-02 20:54:18,054 - INFO - [VAL] Epoch: 21/30 | Batch: 190/194 (98.5%) | Loss: 0.7963 | Batch time: 0.02s
2025-03-02 20:54:18,105 - INFO - [VAL] Epoch: 21/30 | Batch: 193/194 (100.0%) | Loss: 0.3212 | Batch time: 0.01s
2025-03-02 20:54:18,108 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:54:18,108 - INFO - Epoch 21/30 completed in 28.28s
2025-03-02 20:54:18,108 - INFO - Training   - Loss: 0.9367, Accuracy: 0.6968, F1: 0.7003
2025-03-02 20:54:18,108 - INFO - Validation - Loss: 0.6328, Accuracy: 0.7884, F1: 0.7901
2025-03-02 20:54:18,108 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:54:18,108 - INFO - Epoch 22/30
2025-03-02 20:54:18,108 - INFO - ----------------------------------------
2025-03-02 20:54:18,247 - INFO - [TRAIN] Epoch: 22/30 | Batch: 0/903 (0.1%) | Loss: 0.6064 | Batch time: 0.04s
2025-03-02 20:54:20,788 - INFO - [TRAIN] Epoch: 22/30 | Batch: 90/903 (10.1%) | Loss: 1.1567 | Batch time: 0.03s
2025-03-02 20:54:23,261 - INFO - [TRAIN] Epoch: 22/30 | Batch: 180/903 (20.0%) | Loss: 0.4809 | Batch time: 0.03s
2025-03-02 20:54:25,731 - INFO - [TRAIN] Epoch: 22/30 | Batch: 270/903 (30.0%) | Loss: 0.9498 | Batch time: 0.03s
2025-03-02 20:54:28,271 - INFO - [TRAIN] Epoch: 22/30 | Batch: 360/903 (40.0%) | Loss: 1.0048 | Batch time: 0.03s
2025-03-02 20:54:30,749 - INFO - [TRAIN] Epoch: 22/30 | Batch: 450/903 (49.9%) | Loss: 0.4998 | Batch time: 0.03s
2025-03-02 20:54:33,242 - INFO - [TRAIN] Epoch: 22/30 | Batch: 540/903 (59.9%) | Loss: 0.7930 | Batch time: 0.03s
2025-03-02 20:54:35,747 - INFO - [TRAIN] Epoch: 22/30 | Batch: 630/903 (69.9%) | Loss: 0.5614 | Batch time: 0.03s
2025-03-02 20:54:38,244 - INFO - [TRAIN] Epoch: 22/30 | Batch: 720/903 (79.8%) | Loss: 1.8770 | Batch time: 0.03s
2025-03-02 20:54:40,758 - INFO - [TRAIN] Epoch: 22/30 | Batch: 810/903 (89.8%) | Loss: 1.0522 | Batch time: 0.03s
2025-03-02 20:54:43,154 - INFO - [TRAIN] Epoch: 22/30 | Batch: 900/903 (99.8%) | Loss: 1.0751 | Batch time: 0.02s
2025-03-02 20:54:43,198 - INFO - [TRAIN] Epoch: 22/30 | Batch: 902/903 (100.0%) | Loss: 1.5313 | Batch time: 0.02s
2025-03-02 20:54:43,261 - INFO - [VAL] Epoch: 22/30 | Batch: 0/194 (0.5%) | Loss: 31.0555 | Batch time: 0.02s
2025-03-02 20:54:43,605 - INFO - [VAL] Epoch: 22/30 | Batch: 19/194 (10.3%) | Loss: 274.6970 | Batch time: 0.02s
2025-03-02 20:54:43,951 - INFO - [VAL] Epoch: 22/30 | Batch: 38/194 (20.1%) | Loss: 10.8591 | Batch time: 0.02s
2025-03-02 20:54:44,301 - INFO - [VAL] Epoch: 22/30 | Batch: 57/194 (29.9%) | Loss: 45.7269 | Batch time: 0.02s
2025-03-02 20:54:44,654 - INFO - [VAL] Epoch: 22/30 | Batch: 76/194 (39.7%) | Loss: 15.3688 | Batch time: 0.02s
2025-03-02 20:54:45,007 - INFO - [VAL] Epoch: 22/30 | Batch: 95/194 (49.5%) | Loss: 195.4066 | Batch time: 0.02s
2025-03-02 20:54:45,361 - INFO - [VAL] Epoch: 22/30 | Batch: 114/194 (59.3%) | Loss: 2.5221 | Batch time: 0.02s
2025-03-02 20:54:45,713 - INFO - [VAL] Epoch: 22/30 | Batch: 133/194 (69.1%) | Loss: 3.9847 | Batch time: 0.02s
2025-03-02 20:54:46,069 - INFO - [VAL] Epoch: 22/30 | Batch: 152/194 (78.9%) | Loss: 3.6640 | Batch time: 0.02s
2025-03-02 20:54:46,420 - INFO - [VAL] Epoch: 22/30 | Batch: 171/194 (88.7%) | Loss: 1.0342 | Batch time: 0.02s
2025-03-02 20:54:46,766 - INFO - [VAL] Epoch: 22/30 | Batch: 190/194 (98.5%) | Loss: 29.5793 | Batch time: 0.02s
2025-03-02 20:54:46,817 - INFO - [VAL] Epoch: 22/30 | Batch: 193/194 (100.0%) | Loss: 0.5085 | Batch time: 0.01s
2025-03-02 20:54:46,820 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:54:46,820 - INFO - Epoch 22/30 completed in 28.71s
2025-03-02 20:54:46,820 - INFO - Training   - Loss: 0.9293, Accuracy: 0.6985, F1: 0.7020
2025-03-02 20:54:46,820 - INFO - Validation - Loss: 63.3401, Accuracy: 0.6722, F1: 0.6739
2025-03-02 20:54:46,820 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:54:46,820 - INFO - Epoch 23/30
2025-03-02 20:54:46,820 - INFO - ----------------------------------------
2025-03-02 20:54:46,955 - INFO - [TRAIN] Epoch: 23/30 | Batch: 0/903 (0.1%) | Loss: 1.4063 | Batch time: 0.04s
2025-03-02 20:54:49,514 - INFO - [TRAIN] Epoch: 23/30 | Batch: 90/903 (10.1%) | Loss: 0.3820 | Batch time: 0.03s
2025-03-02 20:54:51,957 - INFO - [TRAIN] Epoch: 23/30 | Batch: 180/903 (20.0%) | Loss: 1.8608 | Batch time: 0.03s
2025-03-02 20:54:54,411 - INFO - [TRAIN] Epoch: 23/30 | Batch: 270/903 (30.0%) | Loss: 0.7768 | Batch time: 0.03s
2025-03-02 20:54:56,875 - INFO - [TRAIN] Epoch: 23/30 | Batch: 360/903 (40.0%) | Loss: 0.9915 | Batch time: 0.03s
2025-03-02 20:54:59,355 - INFO - [TRAIN] Epoch: 23/30 | Batch: 450/903 (49.9%) | Loss: 1.0725 | Batch time: 0.03s
2025-03-02 20:55:01,812 - INFO - [TRAIN] Epoch: 23/30 | Batch: 540/903 (59.9%) | Loss: 0.3162 | Batch time: 0.03s
2025-03-02 20:55:04,287 - INFO - [TRAIN] Epoch: 23/30 | Batch: 630/903 (69.9%) | Loss: 0.8470 | Batch time: 0.03s
2025-03-02 20:55:06,775 - INFO - [TRAIN] Epoch: 23/30 | Batch: 720/903 (79.8%) | Loss: 1.1622 | Batch time: 0.03s
2025-03-02 20:55:09,268 - INFO - [TRAIN] Epoch: 23/30 | Batch: 810/903 (89.8%) | Loss: 1.1008 | Batch time: 0.03s
2025-03-02 20:55:11,640 - INFO - [TRAIN] Epoch: 23/30 | Batch: 900/903 (99.8%) | Loss: 1.0840 | Batch time: 0.02s
2025-03-02 20:55:11,684 - INFO - [TRAIN] Epoch: 23/30 | Batch: 902/903 (100.0%) | Loss: 0.9750 | Batch time: 0.02s
2025-03-02 20:55:11,750 - INFO - [VAL] Epoch: 23/30 | Batch: 0/194 (0.5%) | Loss: 0.7849 | Batch time: 0.02s
2025-03-02 20:55:12,093 - INFO - [VAL] Epoch: 23/30 | Batch: 19/194 (10.3%) | Loss: 0.7768 | Batch time: 0.02s
2025-03-02 20:55:12,434 - INFO - [VAL] Epoch: 23/30 | Batch: 38/194 (20.1%) | Loss: 0.5640 | Batch time: 0.02s
2025-03-02 20:55:12,778 - INFO - [VAL] Epoch: 23/30 | Batch: 57/194 (29.9%) | Loss: 0.7664 | Batch time: 0.02s
2025-03-02 20:55:13,127 - INFO - [VAL] Epoch: 23/30 | Batch: 76/194 (39.7%) | Loss: 0.1526 | Batch time: 0.02s
2025-03-02 20:55:13,476 - INFO - [VAL] Epoch: 23/30 | Batch: 95/194 (49.5%) | Loss: 0.8673 | Batch time: 0.02s
2025-03-02 20:55:13,827 - INFO - [VAL] Epoch: 23/30 | Batch: 114/194 (59.3%) | Loss: 0.7189 | Batch time: 0.02s
2025-03-02 20:55:14,179 - INFO - [VAL] Epoch: 23/30 | Batch: 133/194 (69.1%) | Loss: 0.8476 | Batch time: 0.02s
2025-03-02 20:55:14,533 - INFO - [VAL] Epoch: 23/30 | Batch: 152/194 (78.9%) | Loss: 0.5953 | Batch time: 0.02s
2025-03-02 20:55:14,886 - INFO - [VAL] Epoch: 23/30 | Batch: 171/194 (88.7%) | Loss: 1.1698 | Batch time: 0.02s
2025-03-02 20:55:15,237 - INFO - [VAL] Epoch: 23/30 | Batch: 190/194 (98.5%) | Loss: 0.8147 | Batch time: 0.02s
2025-03-02 20:55:15,288 - INFO - [VAL] Epoch: 23/30 | Batch: 193/194 (100.0%) | Loss: 0.2753 | Batch time: 0.01s
2025-03-02 20:55:15,292 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:55:15,292 - INFO - Epoch 23/30 completed in 28.47s
2025-03-02 20:55:15,292 - INFO - Training   - Loss: 0.9499, Accuracy: 0.6958, F1: 0.6991
2025-03-02 20:55:15,292 - INFO - Validation - Loss: 0.6296, Accuracy: 0.7936, F1: 0.7931
2025-03-02 20:55:15,292 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:55:15,292 - INFO - Epoch 24/30
2025-03-02 20:55:15,292 - INFO - ----------------------------------------
2025-03-02 20:55:15,441 - INFO - [TRAIN] Epoch: 24/30 | Batch: 0/903 (0.1%) | Loss: 0.5578 | Batch time: 0.05s
2025-03-02 20:55:18,010 - INFO - [TRAIN] Epoch: 24/30 | Batch: 90/903 (10.1%) | Loss: 1.4222 | Batch time: 0.03s
2025-03-02 20:55:20,468 - INFO - [TRAIN] Epoch: 24/30 | Batch: 180/903 (20.0%) | Loss: 0.7879 | Batch time: 0.03s
2025-03-02 20:55:22,962 - INFO - [TRAIN] Epoch: 24/30 | Batch: 270/903 (30.0%) | Loss: 1.0527 | Batch time: 0.03s
2025-03-02 20:55:25,419 - INFO - [TRAIN] Epoch: 24/30 | Batch: 360/903 (40.0%) | Loss: 0.9046 | Batch time: 0.03s
2025-03-02 20:55:27,915 - INFO - [TRAIN] Epoch: 24/30 | Batch: 450/903 (49.9%) | Loss: 0.8391 | Batch time: 0.03s
2025-03-02 20:55:30,407 - INFO - [TRAIN] Epoch: 24/30 | Batch: 540/903 (59.9%) | Loss: 1.0066 | Batch time: 0.03s
2025-03-02 20:55:32,882 - INFO - [TRAIN] Epoch: 24/30 | Batch: 630/903 (69.9%) | Loss: 0.8792 | Batch time: 0.03s
2025-03-02 20:55:35,357 - INFO - [TRAIN] Epoch: 24/30 | Batch: 720/903 (79.8%) | Loss: 1.5418 | Batch time: 0.03s
2025-03-02 20:55:37,838 - INFO - [TRAIN] Epoch: 24/30 | Batch: 810/903 (89.8%) | Loss: 1.4306 | Batch time: 0.03s
2025-03-02 20:55:40,219 - INFO - [TRAIN] Epoch: 24/30 | Batch: 900/903 (99.8%) | Loss: 0.7694 | Batch time: 0.02s
2025-03-02 20:55:40,263 - INFO - [TRAIN] Epoch: 24/30 | Batch: 902/903 (100.0%) | Loss: 1.4073 | Batch time: 0.02s
2025-03-02 20:55:40,323 - INFO - [VAL] Epoch: 24/30 | Batch: 0/194 (0.5%) | Loss: 0.8795 | Batch time: 0.02s
2025-03-02 20:55:40,668 - INFO - [VAL] Epoch: 24/30 | Batch: 19/194 (10.3%) | Loss: 1.2898 | Batch time: 0.02s
2025-03-02 20:55:41,012 - INFO - [VAL] Epoch: 24/30 | Batch: 38/194 (20.1%) | Loss: 0.6621 | Batch time: 0.02s
2025-03-02 20:55:41,362 - INFO - [VAL] Epoch: 24/30 | Batch: 57/194 (29.9%) | Loss: 5.4361 | Batch time: 0.02s
2025-03-02 20:55:41,714 - INFO - [VAL] Epoch: 24/30 | Batch: 76/194 (39.7%) | Loss: 0.3212 | Batch time: 0.02s
2025-03-02 20:55:42,067 - INFO - [VAL] Epoch: 24/30 | Batch: 95/194 (49.5%) | Loss: 0.5805 | Batch time: 0.02s
2025-03-02 20:55:42,427 - INFO - [VAL] Epoch: 24/30 | Batch: 114/194 (59.3%) | Loss: 1.8815 | Batch time: 0.02s
2025-03-02 20:55:42,778 - INFO - [VAL] Epoch: 24/30 | Batch: 133/194 (69.1%) | Loss: 0.9164 | Batch time: 0.02s
2025-03-02 20:55:43,130 - INFO - [VAL] Epoch: 24/30 | Batch: 152/194 (78.9%) | Loss: 0.9586 | Batch time: 0.02s
2025-03-02 20:55:43,479 - INFO - [VAL] Epoch: 24/30 | Batch: 171/194 (88.7%) | Loss: 0.9801 | Batch time: 0.02s
2025-03-02 20:55:43,826 - INFO - [VAL] Epoch: 24/30 | Batch: 190/194 (98.5%) | Loss: 0.9103 | Batch time: 0.02s
2025-03-02 20:55:43,877 - INFO - [VAL] Epoch: 24/30 | Batch: 193/194 (100.0%) | Loss: 0.3388 | Batch time: 0.01s
2025-03-02 20:55:43,880 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:55:43,880 - INFO - Epoch 24/30 completed in 28.59s
2025-03-02 20:55:43,880 - INFO - Training   - Loss: 0.9483, Accuracy: 0.6887, F1: 0.6922
2025-03-02 20:55:43,880 - INFO - Validation - Loss: 1.8908, Accuracy: 0.7610, F1: 0.7621
2025-03-02 20:55:43,880 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:55:43,880 - INFO - Epoch 25/30
2025-03-02 20:55:43,880 - INFO - ----------------------------------------
2025-03-02 20:55:44,020 - INFO - [TRAIN] Epoch: 25/30 | Batch: 0/903 (0.1%) | Loss: 1.4572 | Batch time: 0.04s
2025-03-02 20:55:46,543 - INFO - [TRAIN] Epoch: 25/30 | Batch: 90/903 (10.1%) | Loss: 0.6635 | Batch time: 0.03s
2025-03-02 20:55:48,995 - INFO - [TRAIN] Epoch: 25/30 | Batch: 180/903 (20.0%) | Loss: 1.1845 | Batch time: 0.03s
2025-03-02 20:55:51,442 - INFO - [TRAIN] Epoch: 25/30 | Batch: 270/903 (30.0%) | Loss: 1.4375 | Batch time: 0.03s
2025-03-02 20:55:53,899 - INFO - [TRAIN] Epoch: 25/30 | Batch: 360/903 (40.0%) | Loss: 0.6260 | Batch time: 0.03s
2025-03-02 20:55:56,368 - INFO - [TRAIN] Epoch: 25/30 | Batch: 450/903 (49.9%) | Loss: 0.8468 | Batch time: 0.03s
2025-03-02 20:55:58,836 - INFO - [TRAIN] Epoch: 25/30 | Batch: 540/903 (59.9%) | Loss: 1.5049 | Batch time: 0.03s
2025-03-02 20:56:01,287 - INFO - [TRAIN] Epoch: 25/30 | Batch: 630/903 (69.9%) | Loss: 0.6072 | Batch time: 0.03s
2025-03-02 20:56:03,764 - INFO - [TRAIN] Epoch: 25/30 | Batch: 720/903 (79.8%) | Loss: 1.1046 | Batch time: 0.03s
2025-03-02 20:56:06,244 - INFO - [TRAIN] Epoch: 25/30 | Batch: 810/903 (89.8%) | Loss: 1.3123 | Batch time: 0.03s
2025-03-02 20:56:08,624 - INFO - [TRAIN] Epoch: 25/30 | Batch: 900/903 (99.8%) | Loss: 0.7672 | Batch time: 0.02s
2025-03-02 20:56:08,669 - INFO - [TRAIN] Epoch: 25/30 | Batch: 902/903 (100.0%) | Loss: 0.5193 | Batch time: 0.02s
2025-03-02 20:56:08,737 - INFO - [VAL] Epoch: 25/30 | Batch: 0/194 (0.5%) | Loss: 1.4003 | Batch time: 0.03s
2025-03-02 20:56:09,080 - INFO - [VAL] Epoch: 25/30 | Batch: 19/194 (10.3%) | Loss: 1.3016 | Batch time: 0.02s
2025-03-02 20:56:09,426 - INFO - [VAL] Epoch: 25/30 | Batch: 38/194 (20.1%) | Loss: 0.5739 | Batch time: 0.02s
2025-03-02 20:56:09,774 - INFO - [VAL] Epoch: 25/30 | Batch: 57/194 (29.9%) | Loss: 0.9741 | Batch time: 0.02s
2025-03-02 20:56:10,126 - INFO - [VAL] Epoch: 25/30 | Batch: 76/194 (39.7%) | Loss: 0.3265 | Batch time: 0.02s
2025-03-02 20:56:10,479 - INFO - [VAL] Epoch: 25/30 | Batch: 95/194 (49.5%) | Loss: 0.5868 | Batch time: 0.02s
2025-03-02 20:56:10,832 - INFO - [VAL] Epoch: 25/30 | Batch: 114/194 (59.3%) | Loss: 1.7992 | Batch time: 0.02s
2025-03-02 20:56:11,184 - INFO - [VAL] Epoch: 25/30 | Batch: 133/194 (69.1%) | Loss: 0.8289 | Batch time: 0.02s
2025-03-02 20:56:11,536 - INFO - [VAL] Epoch: 25/30 | Batch: 152/194 (78.9%) | Loss: 0.9952 | Batch time: 0.02s
2025-03-02 20:56:11,886 - INFO - [VAL] Epoch: 25/30 | Batch: 171/194 (88.7%) | Loss: 0.9459 | Batch time: 0.02s
2025-03-02 20:56:12,237 - INFO - [VAL] Epoch: 25/30 | Batch: 190/194 (98.5%) | Loss: 10.1620 | Batch time: 0.02s
2025-03-02 20:56:12,288 - INFO - [VAL] Epoch: 25/30 | Batch: 193/194 (100.0%) | Loss: 0.4167 | Batch time: 0.01s
2025-03-02 20:56:12,291 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:56:12,291 - INFO - Epoch 25/30 completed in 28.41s
2025-03-02 20:56:12,291 - INFO - Training   - Loss: 0.9371, Accuracy: 0.6919, F1: 0.6955
2025-03-02 20:56:12,291 - INFO - Validation - Loss: 1.4398, Accuracy: 0.7545, F1: 0.7578
2025-03-02 20:56:12,291 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:56:12,437 - INFO - Checkpoint saved: checkpoint_epoch_25.pth (Epoch 25)
2025-03-02 20:56:12,437 - INFO - Epoch 26/30
2025-03-02 20:56:12,437 - INFO - ----------------------------------------
2025-03-02 20:56:12,577 - INFO - [TRAIN] Epoch: 26/30 | Batch: 0/903 (0.1%) | Loss: 0.6545 | Batch time: 0.05s
2025-03-02 20:56:15,100 - INFO - [TRAIN] Epoch: 26/30 | Batch: 90/903 (10.1%) | Loss: 1.2756 | Batch time: 0.03s
2025-03-02 20:56:17,533 - INFO - [TRAIN] Epoch: 26/30 | Batch: 180/903 (20.0%) | Loss: 0.8664 | Batch time: 0.03s
2025-03-02 20:56:20,012 - INFO - [TRAIN] Epoch: 26/30 | Batch: 270/903 (30.0%) | Loss: 0.4769 | Batch time: 0.03s
2025-03-02 20:56:22,436 - INFO - [TRAIN] Epoch: 26/30 | Batch: 360/903 (40.0%) | Loss: 0.6925 | Batch time: 0.03s
2025-03-02 20:56:24,980 - INFO - [TRAIN] Epoch: 26/30 | Batch: 450/903 (49.9%) | Loss: 0.8013 | Batch time: 0.03s
2025-03-02 20:56:27,743 - INFO - [TRAIN] Epoch: 26/30 | Batch: 540/903 (59.9%) | Loss: 2.5393 | Batch time: 0.03s
2025-03-02 20:56:30,109 - INFO - [TRAIN] Epoch: 26/30 | Batch: 630/903 (69.9%) | Loss: 0.7294 | Batch time: 0.03s
2025-03-02 20:56:32,485 - INFO - [TRAIN] Epoch: 26/30 | Batch: 720/903 (79.8%) | Loss: 1.2893 | Batch time: 0.03s
2025-03-02 20:56:34,872 - INFO - [TRAIN] Epoch: 26/30 | Batch: 810/903 (89.8%) | Loss: 1.1938 | Batch time: 0.03s
2025-03-02 20:56:37,182 - INFO - [TRAIN] Epoch: 26/30 | Batch: 900/903 (99.8%) | Loss: 1.4580 | Batch time: 0.02s
2025-03-02 20:56:37,227 - INFO - [TRAIN] Epoch: 26/30 | Batch: 902/903 (100.0%) | Loss: 1.2211 | Batch time: 0.02s
2025-03-02 20:56:37,291 - INFO - [VAL] Epoch: 26/30 | Batch: 0/194 (0.5%) | Loss: 0.8630 | Batch time: 0.02s
2025-03-02 20:56:37,633 - INFO - [VAL] Epoch: 26/30 | Batch: 19/194 (10.3%) | Loss: 0.9854 | Batch time: 0.02s
2025-03-02 20:56:37,976 - INFO - [VAL] Epoch: 26/30 | Batch: 38/194 (20.1%) | Loss: 0.5234 | Batch time: 0.02s
2025-03-02 20:56:38,330 - INFO - [VAL] Epoch: 26/30 | Batch: 57/194 (29.9%) | Loss: 0.9128 | Batch time: 0.02s
2025-03-02 20:56:38,679 - INFO - [VAL] Epoch: 26/30 | Batch: 76/194 (39.7%) | Loss: 0.2815 | Batch time: 0.02s
2025-03-02 20:56:39,030 - INFO - [VAL] Epoch: 26/30 | Batch: 95/194 (49.5%) | Loss: 0.7696 | Batch time: 0.02s
2025-03-02 20:56:39,382 - INFO - [VAL] Epoch: 26/30 | Batch: 114/194 (59.3%) | Loss: 1.7075 | Batch time: 0.02s
2025-03-02 20:56:39,733 - INFO - [VAL] Epoch: 26/30 | Batch: 133/194 (69.1%) | Loss: 0.8222 | Batch time: 0.02s
2025-03-02 20:56:40,082 - INFO - [VAL] Epoch: 26/30 | Batch: 152/194 (78.9%) | Loss: 0.8056 | Batch time: 0.02s
2025-03-02 20:56:40,430 - INFO - [VAL] Epoch: 26/30 | Batch: 171/194 (88.7%) | Loss: 0.9078 | Batch time: 0.02s
2025-03-02 20:56:40,776 - INFO - [VAL] Epoch: 26/30 | Batch: 190/194 (98.5%) | Loss: 0.9508 | Batch time: 0.02s
2025-03-02 20:56:40,826 - INFO - [VAL] Epoch: 26/30 | Batch: 193/194 (100.0%) | Loss: 0.3721 | Batch time: 0.01s
2025-03-02 20:56:40,829 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:56:40,829 - INFO - Epoch 26/30 completed in 28.39s
2025-03-02 20:56:40,829 - INFO - Training   - Loss: 0.9456, Accuracy: 0.6922, F1: 0.6948
2025-03-02 20:56:40,829 - INFO - Validation - Loss: 0.8143, Accuracy: 0.7826, F1: 0.7840
2025-03-02 20:56:40,829 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:56:40,829 - INFO - Epoch 27/30
2025-03-02 20:56:40,829 - INFO - ----------------------------------------
2025-03-02 20:56:40,975 - INFO - [TRAIN] Epoch: 27/30 | Batch: 0/903 (0.1%) | Loss: 0.6348 | Batch time: 0.05s
2025-03-02 20:56:43,422 - INFO - [TRAIN] Epoch: 27/30 | Batch: 90/903 (10.1%) | Loss: 0.9076 | Batch time: 0.03s
2025-03-02 20:56:45,859 - INFO - [TRAIN] Epoch: 27/30 | Batch: 180/903 (20.0%) | Loss: 0.9587 | Batch time: 0.03s
2025-03-02 20:56:48,298 - INFO - [TRAIN] Epoch: 27/30 | Batch: 270/903 (30.0%) | Loss: 1.3873 | Batch time: 0.03s
2025-03-02 20:56:50,729 - INFO - [TRAIN] Epoch: 27/30 | Batch: 360/903 (40.0%) | Loss: 0.9688 | Batch time: 0.03s
2025-03-02 20:56:53,173 - INFO - [TRAIN] Epoch: 27/30 | Batch: 450/903 (49.9%) | Loss: 0.8039 | Batch time: 0.03s
2025-03-02 20:56:55,631 - INFO - [TRAIN] Epoch: 27/30 | Batch: 540/903 (59.9%) | Loss: 1.1872 | Batch time: 0.03s
2025-03-02 20:56:58,100 - INFO - [TRAIN] Epoch: 27/30 | Batch: 630/903 (69.9%) | Loss: 0.8296 | Batch time: 0.03s
2025-03-02 20:57:00,557 - INFO - [TRAIN] Epoch: 27/30 | Batch: 720/903 (79.8%) | Loss: 1.1796 | Batch time: 0.03s
2025-03-02 20:57:03,030 - INFO - [TRAIN] Epoch: 27/30 | Batch: 810/903 (89.8%) | Loss: 1.0564 | Batch time: 0.03s
2025-03-02 20:57:05,462 - INFO - [TRAIN] Epoch: 27/30 | Batch: 900/903 (99.8%) | Loss: 0.8757 | Batch time: 0.02s
2025-03-02 20:57:05,507 - INFO - [TRAIN] Epoch: 27/30 | Batch: 902/903 (100.0%) | Loss: 1.0704 | Batch time: 0.02s
2025-03-02 20:57:05,569 - INFO - [VAL] Epoch: 27/30 | Batch: 0/194 (0.5%) | Loss: 0.7507 | Batch time: 0.02s
2025-03-02 20:57:05,911 - INFO - [VAL] Epoch: 27/30 | Batch: 19/194 (10.3%) | Loss: 0.7619 | Batch time: 0.02s
2025-03-02 20:57:06,256 - INFO - [VAL] Epoch: 27/30 | Batch: 38/194 (20.1%) | Loss: 0.4034 | Batch time: 0.02s
2025-03-02 20:57:06,605 - INFO - [VAL] Epoch: 27/30 | Batch: 57/194 (29.9%) | Loss: 0.8922 | Batch time: 0.02s
2025-03-02 20:57:06,957 - INFO - [VAL] Epoch: 27/30 | Batch: 76/194 (39.7%) | Loss: 0.2313 | Batch time: 0.02s
2025-03-02 20:57:07,310 - INFO - [VAL] Epoch: 27/30 | Batch: 95/194 (49.5%) | Loss: 0.6032 | Batch time: 0.02s
2025-03-02 20:57:07,662 - INFO - [VAL] Epoch: 27/30 | Batch: 114/194 (59.3%) | Loss: 1.5487 | Batch time: 0.02s
2025-03-02 20:57:08,014 - INFO - [VAL] Epoch: 27/30 | Batch: 133/194 (69.1%) | Loss: 0.8342 | Batch time: 0.02s
2025-03-02 20:57:08,367 - INFO - [VAL] Epoch: 27/30 | Batch: 152/194 (78.9%) | Loss: 0.7513 | Batch time: 0.02s
2025-03-02 20:57:08,718 - INFO - [VAL] Epoch: 27/30 | Batch: 171/194 (88.7%) | Loss: 1.1658 | Batch time: 0.02s
2025-03-02 20:57:09,065 - INFO - [VAL] Epoch: 27/30 | Batch: 190/194 (98.5%) | Loss: 1.0352 | Batch time: 0.02s
2025-03-02 20:57:09,116 - INFO - [VAL] Epoch: 27/30 | Batch: 193/194 (100.0%) | Loss: 0.3574 | Batch time: 0.01s
2025-03-02 20:57:09,119 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:57:09,119 - INFO - Epoch 27/30 completed in 28.29s
2025-03-02 20:57:09,119 - INFO - Training   - Loss: 0.9211, Accuracy: 0.6957, F1: 0.6986
2025-03-02 20:57:09,119 - INFO - Validation - Loss: 0.9148, Accuracy: 0.7752, F1: 0.7773
2025-03-02 20:57:09,119 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:57:09,119 - INFO - Epoch 28/30
2025-03-02 20:57:09,119 - INFO - ----------------------------------------
2025-03-02 20:57:09,262 - INFO - [TRAIN] Epoch: 28/30 | Batch: 0/903 (0.1%) | Loss: 0.6105 | Batch time: 0.03s
2025-03-02 20:57:11,766 - INFO - [TRAIN] Epoch: 28/30 | Batch: 90/903 (10.1%) | Loss: 0.9437 | Batch time: 0.03s
2025-03-02 20:57:14,239 - INFO - [TRAIN] Epoch: 28/30 | Batch: 180/903 (20.0%) | Loss: 0.9734 | Batch time: 0.03s
2025-03-02 20:57:16,724 - INFO - [TRAIN] Epoch: 28/30 | Batch: 270/903 (30.0%) | Loss: 1.4591 | Batch time: 0.03s
2025-03-02 20:57:19,196 - INFO - [TRAIN] Epoch: 28/30 | Batch: 360/903 (40.0%) | Loss: 1.0207 | Batch time: 0.03s
2025-03-02 20:57:21,730 - INFO - [TRAIN] Epoch: 28/30 | Batch: 450/903 (49.9%) | Loss: 0.9519 | Batch time: 0.03s
2025-03-02 20:57:24,210 - INFO - [TRAIN] Epoch: 28/30 | Batch: 540/903 (59.9%) | Loss: 1.0644 | Batch time: 0.03s
2025-03-02 20:57:26,716 - INFO - [TRAIN] Epoch: 28/30 | Batch: 630/903 (69.9%) | Loss: 0.6974 | Batch time: 0.03s
2025-03-02 20:57:29,212 - INFO - [TRAIN] Epoch: 28/30 | Batch: 720/903 (79.8%) | Loss: 0.5937 | Batch time: 0.03s
2025-03-02 20:57:31,725 - INFO - [TRAIN] Epoch: 28/30 | Batch: 810/903 (89.8%) | Loss: 1.4910 | Batch time: 0.03s
2025-03-02 20:57:34,119 - INFO - [TRAIN] Epoch: 28/30 | Batch: 900/903 (99.8%) | Loss: 1.2461 | Batch time: 0.02s
2025-03-02 20:57:34,164 - INFO - [TRAIN] Epoch: 28/30 | Batch: 902/903 (100.0%) | Loss: 1.1275 | Batch time: 0.02s
2025-03-02 20:57:34,228 - INFO - [VAL] Epoch: 28/30 | Batch: 0/194 (0.5%) | Loss: 0.8395 | Batch time: 0.02s
2025-03-02 20:57:34,572 - INFO - [VAL] Epoch: 28/30 | Batch: 19/194 (10.3%) | Loss: 1.0744 | Batch time: 0.02s
2025-03-02 20:57:34,914 - INFO - [VAL] Epoch: 28/30 | Batch: 38/194 (20.1%) | Loss: 0.5225 | Batch time: 0.02s
2025-03-02 20:57:35,263 - INFO - [VAL] Epoch: 28/30 | Batch: 57/194 (29.9%) | Loss: 1.0140 | Batch time: 0.02s
2025-03-02 20:57:35,616 - INFO - [VAL] Epoch: 28/30 | Batch: 76/194 (39.7%) | Loss: 0.2731 | Batch time: 0.02s
2025-03-02 20:57:35,967 - INFO - [VAL] Epoch: 28/30 | Batch: 95/194 (49.5%) | Loss: 0.6235 | Batch time: 0.02s
2025-03-02 20:57:36,319 - INFO - [VAL] Epoch: 28/30 | Batch: 114/194 (59.3%) | Loss: 1.5966 | Batch time: 0.02s
2025-03-02 20:57:36,671 - INFO - [VAL] Epoch: 28/30 | Batch: 133/194 (69.1%) | Loss: 0.8483 | Batch time: 0.02s
2025-03-02 20:57:37,023 - INFO - [VAL] Epoch: 28/30 | Batch: 152/194 (78.9%) | Loss: 0.8886 | Batch time: 0.02s
2025-03-02 20:57:37,375 - INFO - [VAL] Epoch: 28/30 | Batch: 171/194 (88.7%) | Loss: 0.9234 | Batch time: 0.02s
2025-03-02 20:57:37,726 - INFO - [VAL] Epoch: 28/30 | Batch: 190/194 (98.5%) | Loss: 1.0470 | Batch time: 0.02s
2025-03-02 20:57:37,777 - INFO - [VAL] Epoch: 28/30 | Batch: 193/194 (100.0%) | Loss: 0.4638 | Batch time: 0.01s
2025-03-02 20:57:37,781 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:57:37,781 - INFO - Epoch 28/30 completed in 28.66s
2025-03-02 20:57:37,781 - INFO - Training   - Loss: 0.9433, Accuracy: 0.6950, F1: 0.6987
2025-03-02 20:57:37,781 - INFO - Validation - Loss: 0.8722, Accuracy: 0.7726, F1: 0.7742
2025-03-02 20:57:37,781 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:57:37,781 - INFO - Epoch 29/30
2025-03-02 20:57:37,781 - INFO - ----------------------------------------
2025-03-02 20:57:37,944 - INFO - [TRAIN] Epoch: 29/30 | Batch: 0/903 (0.1%) | Loss: 1.0229 | Batch time: 0.04s
2025-03-02 20:57:40,455 - INFO - [TRAIN] Epoch: 29/30 | Batch: 90/903 (10.1%) | Loss: 0.7887 | Batch time: 0.03s
2025-03-02 20:57:42,913 - INFO - [TRAIN] Epoch: 29/30 | Batch: 180/903 (20.0%) | Loss: 0.9694 | Batch time: 0.03s
2025-03-02 20:57:45,357 - INFO - [TRAIN] Epoch: 29/30 | Batch: 270/903 (30.0%) | Loss: 0.9151 | Batch time: 0.03s
2025-03-02 20:57:47,819 - INFO - [TRAIN] Epoch: 29/30 | Batch: 360/903 (40.0%) | Loss: 0.7021 | Batch time: 0.03s
2025-03-02 20:57:50,280 - INFO - [TRAIN] Epoch: 29/30 | Batch: 450/903 (49.9%) | Loss: 1.0142 | Batch time: 0.03s
2025-03-02 20:57:52,744 - INFO - [TRAIN] Epoch: 29/30 | Batch: 540/903 (59.9%) | Loss: 0.9217 | Batch time: 0.03s
2025-03-02 20:57:55,295 - INFO - [TRAIN] Epoch: 29/30 | Batch: 630/903 (69.9%) | Loss: 1.9336 | Batch time: 0.03s
2025-03-02 20:57:57,796 - INFO - [TRAIN] Epoch: 29/30 | Batch: 720/903 (79.8%) | Loss: 1.0703 | Batch time: 0.03s
2025-03-02 20:58:00,289 - INFO - [TRAIN] Epoch: 29/30 | Batch: 810/903 (89.8%) | Loss: 0.8913 | Batch time: 0.03s
2025-03-02 20:58:02,662 - INFO - [TRAIN] Epoch: 29/30 | Batch: 900/903 (99.8%) | Loss: 0.9130 | Batch time: 0.02s
2025-03-02 20:58:02,706 - INFO - [TRAIN] Epoch: 29/30 | Batch: 902/903 (100.0%) | Loss: 0.8102 | Batch time: 0.02s
2025-03-02 20:58:02,770 - INFO - [VAL] Epoch: 29/30 | Batch: 0/194 (0.5%) | Loss: 0.8259 | Batch time: 0.02s
2025-03-02 20:58:03,109 - INFO - [VAL] Epoch: 29/30 | Batch: 19/194 (10.3%) | Loss: 0.8824 | Batch time: 0.02s
2025-03-02 20:58:03,449 - INFO - [VAL] Epoch: 29/30 | Batch: 38/194 (20.1%) | Loss: 0.5347 | Batch time: 0.02s
2025-03-02 20:58:03,801 - INFO - [VAL] Epoch: 29/30 | Batch: 57/194 (29.9%) | Loss: 0.9797 | Batch time: 0.02s
2025-03-02 20:58:04,150 - INFO - [VAL] Epoch: 29/30 | Batch: 76/194 (39.7%) | Loss: 0.1886 | Batch time: 0.02s
2025-03-02 20:58:04,501 - INFO - [VAL] Epoch: 29/30 | Batch: 95/194 (49.5%) | Loss: 0.8332 | Batch time: 0.02s
2025-03-02 20:58:04,857 - INFO - [VAL] Epoch: 29/30 | Batch: 114/194 (59.3%) | Loss: 1.2099 | Batch time: 0.02s
2025-03-02 20:58:05,207 - INFO - [VAL] Epoch: 29/30 | Batch: 133/194 (69.1%) | Loss: 0.7211 | Batch time: 0.02s
2025-03-02 20:58:05,556 - INFO - [VAL] Epoch: 29/30 | Batch: 152/194 (78.9%) | Loss: 0.6110 | Batch time: 0.02s
2025-03-02 20:58:05,916 - INFO - [VAL] Epoch: 29/30 | Batch: 171/194 (88.7%) | Loss: 0.9519 | Batch time: 0.02s
2025-03-02 20:58:06,267 - INFO - [VAL] Epoch: 29/30 | Batch: 190/194 (98.5%) | Loss: 0.8975 | Batch time: 0.02s
2025-03-02 20:58:06,319 - INFO - [VAL] Epoch: 29/30 | Batch: 193/194 (100.0%) | Loss: 0.4104 | Batch time: 0.01s
2025-03-02 20:58:06,322 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:58:06,322 - INFO - Epoch 29/30 completed in 28.54s
2025-03-02 20:58:06,322 - INFO - Training   - Loss: 0.9597, Accuracy: 0.6939, F1: 0.6972
2025-03-02 20:58:06,322 - INFO - Validation - Loss: 0.6362, Accuracy: 0.7907, F1: 0.7918
2025-03-02 20:58:06,322 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:58:06,322 - INFO - Epoch 30/30
2025-03-02 20:58:06,322 - INFO - ----------------------------------------
2025-03-02 20:58:06,449 - INFO - [TRAIN] Epoch: 30/30 | Batch: 0/903 (0.1%) | Loss: 1.1652 | Batch time: 0.04s
2025-03-02 20:58:09,056 - INFO - [TRAIN] Epoch: 30/30 | Batch: 90/903 (10.1%) | Loss: 0.8866 | Batch time: 0.03s
2025-03-02 20:58:11,550 - INFO - [TRAIN] Epoch: 30/30 | Batch: 180/903 (20.0%) | Loss: 0.4809 | Batch time: 0.03s
2025-03-02 20:58:14,063 - INFO - [TRAIN] Epoch: 30/30 | Batch: 270/903 (30.0%) | Loss: 0.6762 | Batch time: 0.03s
2025-03-02 20:58:16,646 - INFO - [TRAIN] Epoch: 30/30 | Batch: 360/903 (40.0%) | Loss: 0.8573 | Batch time: 0.03s
2025-03-02 20:58:19,107 - INFO - [TRAIN] Epoch: 30/30 | Batch: 450/903 (49.9%) | Loss: 1.4243 | Batch time: 0.03s
2025-03-02 20:58:21,551 - INFO - [TRAIN] Epoch: 30/30 | Batch: 540/903 (59.9%) | Loss: 1.0762 | Batch time: 0.03s
2025-03-02 20:58:24,014 - INFO - [TRAIN] Epoch: 30/30 | Batch: 630/903 (69.9%) | Loss: 1.8995 | Batch time: 0.03s
2025-03-02 20:58:26,501 - INFO - [TRAIN] Epoch: 30/30 | Batch: 720/903 (79.8%) | Loss: 1.8238 | Batch time: 0.03s
2025-03-02 20:58:28,996 - INFO - [TRAIN] Epoch: 30/30 | Batch: 810/903 (89.8%) | Loss: 1.6124 | Batch time: 0.03s
2025-03-02 20:58:31,365 - INFO - [TRAIN] Epoch: 30/30 | Batch: 900/903 (99.8%) | Loss: 1.2245 | Batch time: 0.02s
2025-03-02 20:58:31,410 - INFO - [TRAIN] Epoch: 30/30 | Batch: 902/903 (100.0%) | Loss: 0.6691 | Batch time: 0.02s
2025-03-02 20:58:31,472 - INFO - [VAL] Epoch: 30/30 | Batch: 0/194 (0.5%) | Loss: 0.8258 | Batch time: 0.02s
2025-03-02 20:58:31,821 - INFO - [VAL] Epoch: 30/30 | Batch: 19/194 (10.3%) | Loss: 1.0896 | Batch time: 0.02s
2025-03-02 20:58:32,164 - INFO - [VAL] Epoch: 30/30 | Batch: 38/194 (20.1%) | Loss: 0.4837 | Batch time: 0.02s
2025-03-02 20:58:32,514 - INFO - [VAL] Epoch: 30/30 | Batch: 57/194 (29.9%) | Loss: 0.7049 | Batch time: 0.02s
2025-03-02 20:58:32,866 - INFO - [VAL] Epoch: 30/30 | Batch: 76/194 (39.7%) | Loss: 0.2570 | Batch time: 0.02s
2025-03-02 20:58:33,220 - INFO - [VAL] Epoch: 30/30 | Batch: 95/194 (49.5%) | Loss: 0.6402 | Batch time: 0.02s
2025-03-02 20:58:33,574 - INFO - [VAL] Epoch: 30/30 | Batch: 114/194 (59.3%) | Loss: 1.8404 | Batch time: 0.02s
2025-03-02 20:58:33,928 - INFO - [VAL] Epoch: 30/30 | Batch: 133/194 (69.1%) | Loss: 0.8617 | Batch time: 0.02s
2025-03-02 20:58:34,282 - INFO - [VAL] Epoch: 30/30 | Batch: 152/194 (78.9%) | Loss: 0.8687 | Batch time: 0.02s
2025-03-02 20:58:34,639 - INFO - [VAL] Epoch: 30/30 | Batch: 171/194 (88.7%) | Loss: 0.9755 | Batch time: 0.02s
2025-03-02 20:58:34,993 - INFO - [VAL] Epoch: 30/30 | Batch: 190/194 (98.5%) | Loss: 1.0451 | Batch time: 0.02s
2025-03-02 20:58:35,044 - INFO - [VAL] Epoch: 30/30 | Batch: 193/194 (100.0%) | Loss: 0.4807 | Batch time: 0.01s
2025-03-02 20:58:35,047 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:58:35,047 - INFO - Epoch 30/30 completed in 28.73s
2025-03-02 20:58:35,047 - INFO - Training   - Loss: 0.9212, Accuracy: 0.6989, F1: 0.7022
2025-03-02 20:58:35,047 - INFO - Validation - Loss: 0.8013, Accuracy: 0.7642, F1: 0.7663
2025-03-02 20:58:35,047 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:58:35,192 - INFO - Checkpoint saved: checkpoint_epoch_30.pth (Epoch 30)
2025-03-02 20:58:35,192 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:58:35,192 - INFO - Training completed in 0h 14m 52.66s
2025-03-02 20:58:35,192 - INFO - Best validation F1: 0.8146 (Epoch 15)
2025-03-02 20:58:35,192 - INFO - --------------------------------------------------------------------------------
2025-03-02 20:58:35,770 - INFO - Final model saved to models/efficientnet_b3_v1/models/efficientnet_b3_v1_final.pth

2025-03-05 00:57:42,883 - INFO - Starting experiment: resnet_attention
2025-03-05 00:57:42,883 - INFO - Command line arguments: Namespace(data_dir='data/raw', output_dir='models/resnet_attention_v2', model='resnet_attention', img_size=224, batch_size=32, num_workers=16, epochs=30, lr=0.001, weight_decay=0.0001, use_weights=True, freeze_backbone=False, no_cuda=False, no_mps=False, use_mps=True, use_amp=False, memory_efficient=True, cache_dataset=True, mps_graph=True, mps_fallback=False, pin_memory=False, optimize_for_m_series=True, patience=10, keep_top_k=3, version=None, find_lr=False, experiment_name='resnet_attention', resnet_version=50)
2025-03-05 00:57:42,883 - INFO - Processing dataset...
2025-03-05 00:57:43,195 - INFO - Class distribution:
2025-03-05 00:57:43,195 - INFO -   Strawberry___healthy: 1000 images
2025-03-05 00:57:43,195 - INFO -   Grape___Black_rot: 1180 images
2025-03-05 00:57:43,195 - INFO -   Potato___Early_blight: 1000 images
2025-03-05 00:57:43,195 - INFO -   Blueberry___healthy: 1502 images
2025-03-05 00:57:43,195 - INFO -   Cherry___Powdery_mildew: 1052 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Target_Spot: 1404 images
2025-03-05 00:57:43,195 - INFO -   Peach___healthy: 1000 images
2025-03-05 00:57:43,195 - INFO -   Potato___Late_blight: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Late_blight: 1909 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Tomato_mosaic_virus: 1000 images
2025-03-05 00:57:43,195 - INFO -   Pepper,_bell___healthy: 1478 images
2025-03-05 00:57:43,195 - INFO -   Orange___Haunglongbing_(Citrus_greening): 5507 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Leaf_Mold: 1000 images
2025-03-05 00:57:43,195 - INFO -   Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 1076 images
2025-03-05 00:57:43,195 - INFO -   Apple___Cedar_apple_rust: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Bacterial_spot: 2127 images
2025-03-05 00:57:43,195 - INFO -   Grape___healthy: 1000 images
2025-03-05 00:57:43,195 - INFO -   Corn___Cercospora_leaf_spot Gray_leaf_spot: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Early_blight: 1000 images
2025-03-05 00:57:43,195 - INFO -   Grape___Esca_(Black_Measles): 1383 images
2025-03-05 00:57:43,195 - INFO -   Raspberry___healthy: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___healthy: 1591 images
2025-03-05 00:57:43,195 - INFO -   Corn___Northern_Leaf_Blight: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Tomato_Yellow_Leaf_Curl_Virus: 5357 images
2025-03-05 00:57:43,195 - INFO -   Cherry___healthy: 1000 images
2025-03-05 00:57:43,195 - INFO -   Apple___Apple_scab: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Spider_mites Two-spotted_spider_mite: 1676 images
2025-03-05 00:57:43,195 - INFO -   Corn___Common_rust: 1192 images
2025-03-05 00:57:43,195 - INFO -   Background_without_leaves: 1143 images
2025-03-05 00:57:43,195 - INFO -   Peach___Bacterial_spot: 2297 images
2025-03-05 00:57:43,195 - INFO -   Pepper,_bell___Bacterial_spot: 1000 images
2025-03-05 00:57:43,195 - INFO -   Tomato___Septoria_leaf_spot: 1771 images
2025-03-05 00:57:43,195 - INFO -   Corn___healthy: 1162 images
2025-03-05 00:57:43,195 - INFO -   Squash___Powdery_mildew: 1835 images
2025-03-05 00:57:43,195 - INFO -   Apple___Black_rot: 1000 images
2025-03-05 00:57:43,195 - INFO -   Apple___healthy: 1645 images
2025-03-05 00:57:43,195 - INFO -   Strawberry___Leaf_scorch: 1109 images
2025-03-05 00:57:43,195 - INFO -   Potato___healthy: 1000 images
2025-03-05 00:57:43,195 - INFO -   Soybean___healthy: 5090 images
2025-03-05 00:57:43,195 - INFO - Creating model: resnet_attention with 39 classes
2025-03-05 00:57:43,501 - INFO - Model architecture:
ResNetWithAttention(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (5): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (6): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (7): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (attention): ResidualAttention(
    (avg_pool): AdaptiveAvgPool2d(output_size=1)
    (fc): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=False)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=128, out_features=2048, bias=False)
      (3): Sigmoid()
    )
  )
  (avg_pool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=2048, out_features=39, bias=True)
  )
)
2025-03-05 00:57:43,502 - INFO - Using class weights: [1.2614028  1.0689855  1.2614028  0.83981544 1.1990521  0.89843506
 1.2614028  1.2614028  0.66076624 1.2614028  0.8534525  0.22905444
 1.2614028  1.1723075  1.2614028  0.59304315 1.2614028  1.2614028
 1.2614028  0.91207725 1.2614028  0.7928365  1.2614028  0.23546813
 1.2614028  1.2614028  0.75262696 1.0582238  1.1035895  0.5491523
 1.2614028  0.7122546  1.0855446  0.687413   1.2614028  0.76681024
 1.1374236  1.2614028  0.24781981]
2025-03-05 00:57:43,502 - INFO - Training all parameters (full model)
2025-03-05 00:57:43,503 - INFO - Starting training for 30 epochs
2025-03-05 00:57:43,503 - INFO - Using Automatic Mixed Precision: False
2025-03-05 00:57:43,503 - INFO - Early stopping patience: 10
2025-03-05 00:57:43,503 - INFO - --------------------------------------------------------------------------------
2025-03-05 00:57:43,503 - INFO - Starting training: resnet_attention
2025-03-05 00:57:43,503 - INFO - Total epochs: 30
2025-03-05 00:57:43,503 - INFO - Training batches per epoch: 1345
2025-03-05 00:57:43,503 - INFO - Validation batches per epoch: 289
2025-03-05 00:57:43,503 - INFO - --------------------------------------------------------------------------------
2025-03-05 00:57:43,503 - INFO - Training model: resnet_attention_v1
2025-03-05 00:57:43,503 - INFO - Epoch 1/30
2025-03-05 00:57:43,503 - INFO - ----------------------------------------
2025-03-05 00:58:13,469 - INFO - [TRAIN] Epoch: 1/30 | Batch: 0/1345 (0.1%) | Loss: 3.7185 | Batch time: 1.13s
2025-03-05 00:58:34,494 - INFO - [TRAIN] Epoch: 1/30 | Batch: 134/1345 (10.0%) | Loss: 2.7427 | Batch time: 0.19s
2025-03-05 00:58:57,818 - INFO - [TRAIN] Epoch: 1/30 | Batch: 268/1345 (20.0%) | Loss: 2.7018 | Batch time: 0.16s
2025-03-05 00:59:20,586 - INFO - [TRAIN] Epoch: 1/30 | Batch: 402/1345 (30.0%) | Loss: 1.7137 | Batch time: 0.16s
2025-03-05 00:59:42,349 - INFO - [TRAIN] Epoch: 1/30 | Batch: 536/1345 (39.9%) | Loss: 2.1593 | Batch time: 0.16s
2025-03-05 01:00:04,284 - INFO - [TRAIN] Epoch: 1/30 | Batch: 670/1345 (49.9%) | Loss: 2.0093 | Batch time: 0.17s
2025-03-05 01:00:25,844 - INFO - [TRAIN] Epoch: 1/30 | Batch: 804/1345 (59.9%) | Loss: 1.5987 | Batch time: 0.16s
2025-03-05 01:00:48,447 - INFO - [TRAIN] Epoch: 1/30 | Batch: 938/1345 (69.8%) | Loss: 1.7556 | Batch time: 0.16s
2025-03-05 01:01:10,263 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1072/1345 (79.8%) | Loss: 1.5063 | Batch time: 0.17s
2025-03-05 01:01:32,842 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1206/1345 (89.7%) | Loss: 1.1375 | Batch time: 0.16s
2025-03-05 01:01:54,235 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1340/1345 (99.7%) | Loss: 0.6694 | Batch time: 0.16s
2025-03-05 01:01:54,866 - INFO - [TRAIN] Epoch: 1/30 | Batch: 1344/1345 (100.0%) | Loss: 1.1116 | Batch time: 0.16s
2025-03-05 01:02:24,409 - INFO - [VAL] Epoch: 1/30 | Batch: 0/289 (0.3%) | Loss: 0.8211 | Batch time: 0.15s
2025-03-05 01:02:25,578 - INFO - [VAL] Epoch: 1/30 | Batch: 28/289 (10.0%) | Loss: 0.6864 | Batch time: 0.04s
2025-03-05 01:02:26,736 - INFO - [VAL] Epoch: 1/30 | Batch: 56/289 (19.7%) | Loss: 1.2655 | Batch time: 0.04s
2025-03-05 01:02:27,904 - INFO - [VAL] Epoch: 1/30 | Batch: 84/289 (29.4%) | Loss: 0.6893 | Batch time: 0.04s
2025-03-05 01:02:29,065 - INFO - [VAL] Epoch: 1/30 | Batch: 112/289 (39.1%) | Loss: 0.4622 | Batch time: 0.04s
2025-03-05 01:02:30,219 - INFO - [VAL] Epoch: 1/30 | Batch: 140/289 (48.8%) | Loss: 0.4920 | Batch time: 0.04s
2025-03-05 01:02:31,371 - INFO - [VAL] Epoch: 1/30 | Batch: 168/289 (58.5%) | Loss: 0.7894 | Batch time: 0.04s
2025-03-05 01:02:32,528 - INFO - [VAL] Epoch: 1/30 | Batch: 196/289 (68.2%) | Loss: 0.6732 | Batch time: 0.04s
2025-03-05 01:02:33,692 - INFO - [VAL] Epoch: 1/30 | Batch: 224/289 (77.9%) | Loss: 0.7103 | Batch time: 0.04s
2025-03-05 01:02:34,862 - INFO - [VAL] Epoch: 1/30 | Batch: 252/289 (87.5%) | Loss: 1.0658 | Batch time: 0.04s
2025-03-05 01:02:36,030 - INFO - [VAL] Epoch: 1/30 | Batch: 280/289 (97.2%) | Loss: 0.5747 | Batch time: 0.04s
2025-03-05 01:02:36,678 - INFO - [VAL] Epoch: 1/30 | Batch: 288/289 (100.0%) | Loss: 0.3851 | Batch time: 0.35s
2025-03-05 01:02:38,190 - INFO - Checkpoint saved: resnet_attention_v1_best.pth (Epoch 1)
2025-03-05 01:02:38,190 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:02:38,190 - INFO - Epoch 1/30 completed in 294.69s
2025-03-05 01:02:38,190 - INFO - Training   - Loss: 1.9422, Accuracy: 0.4412, F1: 0.4476
2025-03-05 01:02:38,190 - INFO - Validation - Loss: 0.6780, Accuracy: 0.7967, F1: 0.7977
2025-03-05 01:02:38,190 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:02:38,191 - INFO - Epoch 2/30
2025-03-05 01:02:38,191 - INFO - ----------------------------------------
2025-03-05 01:02:38,826 - INFO - [TRAIN] Epoch: 2/30 | Batch: 0/1345 (0.1%) | Loss: 0.9882 | Batch time: 0.28s
2025-03-05 01:03:00,015 - INFO - [TRAIN] Epoch: 2/30 | Batch: 134/1345 (10.0%) | Loss: 1.0537 | Batch time: 0.16s
2025-03-05 01:03:21,614 - INFO - [TRAIN] Epoch: 2/30 | Batch: 268/1345 (20.0%) | Loss: 0.9076 | Batch time: 0.16s
2025-03-05 01:03:43,263 - INFO - [TRAIN] Epoch: 2/30 | Batch: 402/1345 (30.0%) | Loss: 1.1787 | Batch time: 0.16s
2025-03-05 01:04:05,691 - INFO - [TRAIN] Epoch: 2/30 | Batch: 536/1345 (39.9%) | Loss: 0.6233 | Batch time: 0.16s
2025-03-05 01:04:27,141 - INFO - [TRAIN] Epoch: 2/30 | Batch: 670/1345 (49.9%) | Loss: 1.2127 | Batch time: 0.16s
2025-03-05 01:04:48,312 - INFO - [TRAIN] Epoch: 2/30 | Batch: 804/1345 (59.9%) | Loss: 1.1915 | Batch time: 0.16s
2025-03-05 01:05:09,671 - INFO - [TRAIN] Epoch: 2/30 | Batch: 938/1345 (69.8%) | Loss: 1.2512 | Batch time: 0.16s
2025-03-05 01:05:31,311 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1072/1345 (79.8%) | Loss: 0.8449 | Batch time: 0.16s
2025-03-05 01:05:53,711 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8401 | Batch time: 0.16s
2025-03-05 01:06:15,049 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1340/1345 (99.7%) | Loss: 0.8085 | Batch time: 0.16s
2025-03-05 01:06:15,679 - INFO - [TRAIN] Epoch: 2/30 | Batch: 1344/1345 (100.0%) | Loss: 1.4873 | Batch time: 0.16s
2025-03-05 01:06:15,808 - INFO - [VAL] Epoch: 2/30 | Batch: 0/289 (0.3%) | Loss: 0.6342 | Batch time: 0.05s
2025-03-05 01:06:17,017 - INFO - [VAL] Epoch: 2/30 | Batch: 28/289 (10.0%) | Loss: 0.4549 | Batch time: 0.04s
2025-03-05 01:06:18,304 - INFO - [VAL] Epoch: 2/30 | Batch: 56/289 (19.7%) | Loss: 0.7903 | Batch time: 0.05s
2025-03-05 01:06:19,580 - INFO - [VAL] Epoch: 2/30 | Batch: 84/289 (29.4%) | Loss: 0.3903 | Batch time: 0.04s
2025-03-05 01:06:20,836 - INFO - [VAL] Epoch: 2/30 | Batch: 112/289 (39.1%) | Loss: 0.4599 | Batch time: 0.04s
2025-03-05 01:06:22,073 - INFO - [VAL] Epoch: 2/30 | Batch: 140/289 (48.8%) | Loss: 0.4033 | Batch time: 0.04s
2025-03-05 01:06:23,309 - INFO - [VAL] Epoch: 2/30 | Batch: 168/289 (58.5%) | Loss: 0.3000 | Batch time: 0.04s
2025-03-05 01:06:24,545 - INFO - [VAL] Epoch: 2/30 | Batch: 196/289 (68.2%) | Loss: 0.4474 | Batch time: 0.04s
2025-03-05 01:06:25,784 - INFO - [VAL] Epoch: 2/30 | Batch: 224/289 (77.9%) | Loss: 0.3956 | Batch time: 0.04s
2025-03-05 01:06:27,032 - INFO - [VAL] Epoch: 2/30 | Batch: 252/289 (87.5%) | Loss: 0.9588 | Batch time: 0.04s
2025-03-05 01:06:28,275 - INFO - [VAL] Epoch: 2/30 | Batch: 280/289 (97.2%) | Loss: 0.5054 | Batch time: 0.04s
2025-03-05 01:06:28,594 - INFO - [VAL] Epoch: 2/30 | Batch: 288/289 (100.0%) | Loss: 0.6124 | Batch time: 0.01s
2025-03-05 01:06:28,603 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:06:28,603 - INFO - Epoch 2/30 completed in 230.41s
2025-03-05 01:06:28,603 - INFO - Training   - Loss: 1.0249, Accuracy: 0.6912, F1: 0.6938
2025-03-05 01:06:28,603 - INFO - Validation - Loss: 0.5883, Accuracy: 0.7961, F1: 0.7918
2025-03-05 01:06:28,603 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:06:28,603 - INFO - Epoch 3/30
2025-03-05 01:06:28,603 - INFO - ----------------------------------------
2025-03-05 01:06:29,003 - INFO - [TRAIN] Epoch: 3/30 | Batch: 0/1345 (0.1%) | Loss: 1.0329 | Batch time: 0.18s
2025-03-05 01:06:50,452 - INFO - [TRAIN] Epoch: 3/30 | Batch: 134/1345 (10.0%) | Loss: 0.9003 | Batch time: 0.16s
2025-03-05 01:07:12,907 - INFO - [TRAIN] Epoch: 3/30 | Batch: 268/1345 (20.0%) | Loss: 0.9848 | Batch time: 0.16s
2025-03-05 01:07:34,294 - INFO - [TRAIN] Epoch: 3/30 | Batch: 402/1345 (30.0%) | Loss: 0.8054 | Batch time: 0.16s
2025-03-05 01:07:55,497 - INFO - [TRAIN] Epoch: 3/30 | Batch: 536/1345 (39.9%) | Loss: 0.7861 | Batch time: 0.16s
2025-03-05 01:08:16,874 - INFO - [TRAIN] Epoch: 3/30 | Batch: 670/1345 (49.9%) | Loss: 1.5945 | Batch time: 0.16s
2025-03-05 01:08:40,538 - INFO - [TRAIN] Epoch: 3/30 | Batch: 804/1345 (59.9%) | Loss: 0.6785 | Batch time: 0.18s
2025-03-05 01:09:05,710 - INFO - [TRAIN] Epoch: 3/30 | Batch: 938/1345 (69.8%) | Loss: 0.6483 | Batch time: 0.19s
2025-03-05 01:09:31,242 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1072/1345 (79.8%) | Loss: 0.1887 | Batch time: 0.19s
2025-03-05 01:09:56,833 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1206/1345 (89.7%) | Loss: 0.4604 | Batch time: 0.19s
2025-03-05 01:10:21,865 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1340/1345 (99.7%) | Loss: 0.4270 | Batch time: 0.18s
2025-03-05 01:10:22,586 - INFO - [TRAIN] Epoch: 3/30 | Batch: 1344/1345 (100.0%) | Loss: 0.3479 | Batch time: 0.18s
2025-03-05 01:10:22,727 - INFO - [VAL] Epoch: 3/30 | Batch: 0/289 (0.3%) | Loss: 0.1841 | Batch time: 0.06s
2025-03-05 01:10:24,134 - INFO - [VAL] Epoch: 3/30 | Batch: 28/289 (10.0%) | Loss: 0.1349 | Batch time: 0.05s
2025-03-05 01:10:25,641 - INFO - [VAL] Epoch: 3/30 | Batch: 56/289 (19.7%) | Loss: 0.7703 | Batch time: 0.05s
2025-03-05 01:10:27,131 - INFO - [VAL] Epoch: 3/30 | Batch: 84/289 (29.4%) | Loss: 0.1135 | Batch time: 0.05s
2025-03-05 01:10:28,636 - INFO - [VAL] Epoch: 3/30 | Batch: 112/289 (39.1%) | Loss: 0.0147 | Batch time: 0.05s
2025-03-05 01:10:30,133 - INFO - [VAL] Epoch: 3/30 | Batch: 140/289 (48.8%) | Loss: 0.1887 | Batch time: 0.05s
2025-03-05 01:10:31,641 - INFO - [VAL] Epoch: 3/30 | Batch: 168/289 (58.5%) | Loss: 0.0621 | Batch time: 0.05s
2025-03-05 01:10:33,148 - INFO - [VAL] Epoch: 3/30 | Batch: 196/289 (68.2%) | Loss: 0.2733 | Batch time: 0.05s
2025-03-05 01:10:34,691 - INFO - [VAL] Epoch: 3/30 | Batch: 224/289 (77.9%) | Loss: 0.3841 | Batch time: 0.05s
2025-03-05 01:10:36,216 - INFO - [VAL] Epoch: 3/30 | Batch: 252/289 (87.5%) | Loss: 0.4839 | Batch time: 0.05s
2025-03-05 01:10:37,707 - INFO - [VAL] Epoch: 3/30 | Batch: 280/289 (97.2%) | Loss: 0.5398 | Batch time: 0.05s
2025-03-05 01:10:38,082 - INFO - [VAL] Epoch: 3/30 | Batch: 288/289 (100.0%) | Loss: 0.6289 | Batch time: 0.01s
2025-03-05 01:10:38,844 - INFO - Checkpoint saved: resnet_attention_v1_best.pth (Epoch 3)
2025-03-05 01:10:38,844 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:10:38,844 - INFO - Epoch 3/30 completed in 250.24s
2025-03-05 01:10:38,844 - INFO - Training   - Loss: 0.7557, Accuracy: 0.7684, F1: 0.7701
2025-03-05 01:10:38,844 - INFO - Validation - Loss: 0.2947, Accuracy: 0.9134, F1: 0.9106
2025-03-05 01:10:38,844 - INFO - Validation F1 improved from 0.7977 to 0.9106
2025-03-05 01:10:38,844 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:10:38,844 - INFO - Epoch 4/30
2025-03-05 01:10:38,844 - INFO - ----------------------------------------
2025-03-05 01:10:39,242 - INFO - [TRAIN] Epoch: 4/30 | Batch: 0/1345 (0.1%) | Loss: 0.6434 | Batch time: 0.19s
2025-03-05 01:11:05,434 - INFO - [TRAIN] Epoch: 4/30 | Batch: 134/1345 (10.0%) | Loss: 0.2336 | Batch time: 0.19s
2025-03-05 01:11:32,143 - INFO - [TRAIN] Epoch: 4/30 | Batch: 268/1345 (20.0%) | Loss: 0.6469 | Batch time: 0.20s
2025-03-05 01:12:02,447 - INFO - [TRAIN] Epoch: 4/30 | Batch: 402/1345 (30.0%) | Loss: 0.5702 | Batch time: 0.17s
2025-03-05 01:12:32,219 - INFO - [TRAIN] Epoch: 4/30 | Batch: 536/1345 (39.9%) | Loss: 0.5340 | Batch time: 0.22s
2025-03-05 01:13:06,460 - INFO - [TRAIN] Epoch: 4/30 | Batch: 670/1345 (49.9%) | Loss: 0.7816 | Batch time: 0.32s
2025-03-05 01:13:44,718 - INFO - [TRAIN] Epoch: 4/30 | Batch: 804/1345 (59.9%) | Loss: 0.9178 | Batch time: 0.19s
2025-03-05 01:14:10,887 - INFO - [TRAIN] Epoch: 4/30 | Batch: 938/1345 (69.8%) | Loss: 0.6048 | Batch time: 0.20s
2025-03-05 01:14:37,476 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1072/1345 (79.8%) | Loss: 0.6187 | Batch time: 0.19s
2025-03-05 01:15:03,876 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1206/1345 (89.7%) | Loss: 0.9225 | Batch time: 0.20s
2025-03-05 01:15:30,344 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1340/1345 (99.7%) | Loss: 0.4516 | Batch time: 0.19s
2025-03-05 01:15:31,093 - INFO - [TRAIN] Epoch: 4/30 | Batch: 1344/1345 (100.0%) | Loss: 0.5924 | Batch time: 0.19s
2025-03-05 01:15:31,238 - INFO - [VAL] Epoch: 4/30 | Batch: 0/289 (0.3%) | Loss: 0.1206 | Batch time: 0.06s
2025-03-05 01:15:32,746 - INFO - [VAL] Epoch: 4/30 | Batch: 28/289 (10.0%) | Loss: 0.1055 | Batch time: 0.06s
2025-03-05 01:15:34,430 - INFO - [VAL] Epoch: 4/30 | Batch: 56/289 (19.7%) | Loss: 0.4091 | Batch time: 0.06s
2025-03-05 01:15:36,243 - INFO - [VAL] Epoch: 4/30 | Batch: 84/289 (29.4%) | Loss: 0.1912 | Batch time: 0.06s
2025-03-05 01:15:38,067 - INFO - [VAL] Epoch: 4/30 | Batch: 112/289 (39.1%) | Loss: 0.0787 | Batch time: 0.07s
2025-03-05 01:15:39,673 - INFO - [VAL] Epoch: 4/30 | Batch: 140/289 (48.8%) | Loss: 0.1961 | Batch time: 0.06s
2025-03-05 01:15:41,248 - INFO - [VAL] Epoch: 4/30 | Batch: 168/289 (58.5%) | Loss: 0.0155 | Batch time: 0.05s
2025-03-05 01:15:42,779 - INFO - [VAL] Epoch: 4/30 | Batch: 196/289 (68.2%) | Loss: 0.0671 | Batch time: 0.05s
2025-03-05 01:15:44,324 - INFO - [VAL] Epoch: 4/30 | Batch: 224/289 (77.9%) | Loss: 0.1793 | Batch time: 0.06s
2025-03-05 01:15:45,950 - INFO - [VAL] Epoch: 4/30 | Batch: 252/289 (87.5%) | Loss: 0.1535 | Batch time: 0.06s
2025-03-05 01:15:47,515 - INFO - [VAL] Epoch: 4/30 | Batch: 280/289 (97.2%) | Loss: 0.2537 | Batch time: 0.05s
2025-03-05 01:15:47,918 - INFO - [VAL] Epoch: 4/30 | Batch: 288/289 (100.0%) | Loss: 0.0842 | Batch time: 0.02s
2025-03-05 01:15:48,831 - INFO - Checkpoint saved: resnet_attention_v1_best.pth (Epoch 4)
2025-03-05 01:15:48,831 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:15:48,832 - INFO - Epoch 4/30 completed in 309.99s
2025-03-05 01:15:48,832 - INFO - Training   - Loss: 0.6354, Accuracy: 0.8044, F1: 0.8055
2025-03-05 01:15:48,832 - INFO - Validation - Loss: 0.1896, Accuracy: 0.9387, F1: 0.9395
2025-03-05 01:15:48,832 - INFO - Validation F1 improved from 0.9106 to 0.9395
2025-03-05 01:15:48,832 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:15:48,832 - INFO - Epoch 5/30
2025-03-05 01:15:48,832 - INFO - ----------------------------------------
2025-03-05 01:15:49,232 - INFO - [TRAIN] Epoch: 5/30 | Batch: 0/1345 (0.1%) | Loss: 0.5317 | Batch time: 0.20s
2025-03-05 01:16:15,998 - INFO - [TRAIN] Epoch: 5/30 | Batch: 134/1345 (10.0%) | Loss: 0.2277 | Batch time: 0.20s
2025-03-05 01:16:42,637 - INFO - [TRAIN] Epoch: 5/30 | Batch: 268/1345 (20.0%) | Loss: 0.4599 | Batch time: 0.20s
2025-03-05 01:17:09,020 - INFO - [TRAIN] Epoch: 5/30 | Batch: 402/1345 (30.0%) | Loss: 0.7917 | Batch time: 0.20s
2025-03-05 01:17:35,535 - INFO - [TRAIN] Epoch: 5/30 | Batch: 536/1345 (39.9%) | Loss: 0.3974 | Batch time: 0.20s
2025-03-05 01:18:01,748 - INFO - [TRAIN] Epoch: 5/30 | Batch: 670/1345 (49.9%) | Loss: 0.8879 | Batch time: 0.19s
2025-03-05 01:18:27,798 - INFO - [TRAIN] Epoch: 5/30 | Batch: 804/1345 (59.9%) | Loss: 0.3203 | Batch time: 0.19s
2025-03-05 01:18:54,925 - INFO - [TRAIN] Epoch: 5/30 | Batch: 938/1345 (69.8%) | Loss: 0.6393 | Batch time: 0.21s
2025-03-05 01:19:21,356 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1072/1345 (79.8%) | Loss: 0.2605 | Batch time: 0.20s
2025-03-05 01:19:47,463 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1206/1345 (89.7%) | Loss: 0.8228 | Batch time: 0.20s
2025-03-05 01:20:13,379 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1340/1345 (99.7%) | Loss: 0.1453 | Batch time: 0.18s
2025-03-05 01:20:14,125 - INFO - [TRAIN] Epoch: 5/30 | Batch: 1344/1345 (100.0%) | Loss: 0.4905 | Batch time: 0.19s
2025-03-05 01:20:14,258 - INFO - [VAL] Epoch: 5/30 | Batch: 0/289 (0.3%) | Loss: 0.2668 | Batch time: 0.05s
2025-03-05 01:20:15,751 - INFO - [VAL] Epoch: 5/30 | Batch: 28/289 (10.0%) | Loss: 0.3382 | Batch time: 0.06s
2025-03-05 01:20:17,342 - INFO - [VAL] Epoch: 5/30 | Batch: 56/289 (19.7%) | Loss: 0.3877 | Batch time: 0.06s
2025-03-05 01:20:18,876 - INFO - [VAL] Epoch: 5/30 | Batch: 84/289 (29.4%) | Loss: 0.1948 | Batch time: 0.05s
2025-03-05 01:20:20,382 - INFO - [VAL] Epoch: 5/30 | Batch: 112/289 (39.1%) | Loss: 0.0226 | Batch time: 0.05s
2025-03-05 01:20:21,898 - INFO - [VAL] Epoch: 5/30 | Batch: 140/289 (48.8%) | Loss: 0.3891 | Batch time: 0.05s
2025-03-05 01:20:23,450 - INFO - [VAL] Epoch: 5/30 | Batch: 168/289 (58.5%) | Loss: 0.0354 | Batch time: 0.05s
2025-03-05 01:20:24,988 - INFO - [VAL] Epoch: 5/30 | Batch: 196/289 (68.2%) | Loss: 0.1041 | Batch time: 0.05s
2025-03-05 01:20:26,523 - INFO - [VAL] Epoch: 5/30 | Batch: 224/289 (77.9%) | Loss: 0.1511 | Batch time: 0.05s
2025-03-05 01:20:28,071 - INFO - [VAL] Epoch: 5/30 | Batch: 252/289 (87.5%) | Loss: 0.2520 | Batch time: 0.05s
2025-03-05 01:20:29,589 - INFO - [VAL] Epoch: 5/30 | Batch: 280/289 (97.2%) | Loss: 0.1628 | Batch time: 0.05s
2025-03-05 01:20:29,975 - INFO - [VAL] Epoch: 5/30 | Batch: 288/289 (100.0%) | Loss: 1.2771 | Batch time: 0.01s
2025-03-05 01:20:29,982 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:20:29,982 - INFO - Epoch 5/30 completed in 281.15s
2025-03-05 01:20:29,982 - INFO - Training   - Loss: 0.5612, Accuracy: 0.8278, F1: 0.8288
2025-03-05 01:20:29,982 - INFO - Validation - Loss: 0.2128, Accuracy: 0.9338, F1: 0.9334
2025-03-05 01:20:29,982 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:20:30,504 - INFO - Checkpoint saved: checkpoint_epoch_5.pth (Epoch 5)
2025-03-05 01:20:30,504 - INFO - Epoch 6/30
2025-03-05 01:20:30,504 - INFO - ----------------------------------------
2025-03-05 01:20:30,897 - INFO - [TRAIN] Epoch: 6/30 | Batch: 0/1345 (0.1%) | Loss: 0.6334 | Batch time: 0.19s
2025-03-05 01:20:57,096 - INFO - [TRAIN] Epoch: 6/30 | Batch: 134/1345 (10.0%) | Loss: 0.2013 | Batch time: 0.19s
2025-03-05 01:21:23,091 - INFO - [TRAIN] Epoch: 6/30 | Batch: 268/1345 (20.0%) | Loss: 0.5601 | Batch time: 0.19s
2025-03-05 01:21:49,685 - INFO - [TRAIN] Epoch: 6/30 | Batch: 402/1345 (30.0%) | Loss: 0.5396 | Batch time: 0.19s
2025-03-05 01:22:16,412 - INFO - [TRAIN] Epoch: 6/30 | Batch: 536/1345 (39.9%) | Loss: 0.8461 | Batch time: 0.20s
2025-03-05 01:22:43,600 - INFO - [TRAIN] Epoch: 6/30 | Batch: 670/1345 (49.9%) | Loss: 0.7821 | Batch time: 0.20s
2025-03-05 01:23:09,798 - INFO - [TRAIN] Epoch: 6/30 | Batch: 804/1345 (59.9%) | Loss: 0.7461 | Batch time: 0.19s
2025-03-05 01:23:35,781 - INFO - [TRAIN] Epoch: 6/30 | Batch: 938/1345 (69.8%) | Loss: 0.2762 | Batch time: 0.20s
2025-03-05 01:24:02,439 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1072/1345 (79.8%) | Loss: 0.4973 | Batch time: 0.19s
2025-03-05 01:24:28,978 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1206/1345 (89.7%) | Loss: 0.3024 | Batch time: 0.19s
2025-03-05 01:24:55,084 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1340/1345 (99.7%) | Loss: 0.2599 | Batch time: 0.18s
2025-03-05 01:24:55,828 - INFO - [TRAIN] Epoch: 6/30 | Batch: 1344/1345 (100.0%) | Loss: 0.7122 | Batch time: 0.19s
2025-03-05 01:24:55,968 - INFO - [VAL] Epoch: 6/30 | Batch: 0/289 (0.3%) | Loss: 0.1779 | Batch time: 0.05s
2025-03-05 01:24:57,486 - INFO - [VAL] Epoch: 6/30 | Batch: 28/289 (10.0%) | Loss: 0.0556 | Batch time: 0.06s
2025-03-05 01:24:59,041 - INFO - [VAL] Epoch: 6/30 | Batch: 56/289 (19.7%) | Loss: 0.4152 | Batch time: 0.05s
2025-03-05 01:25:00,556 - INFO - [VAL] Epoch: 6/30 | Batch: 84/289 (29.4%) | Loss: 0.1597 | Batch time: 0.05s
2025-03-05 01:25:02,087 - INFO - [VAL] Epoch: 6/30 | Batch: 112/289 (39.1%) | Loss: 0.1512 | Batch time: 0.05s
2025-03-05 01:25:03,640 - INFO - [VAL] Epoch: 6/30 | Batch: 140/289 (48.8%) | Loss: 0.1085 | Batch time: 0.05s
2025-03-05 01:25:05,178 - INFO - [VAL] Epoch: 6/30 | Batch: 168/289 (58.5%) | Loss: 0.0461 | Batch time: 0.05s
2025-03-05 01:25:06,731 - INFO - [VAL] Epoch: 6/30 | Batch: 196/289 (68.2%) | Loss: 0.1339 | Batch time: 0.05s
2025-03-05 01:25:08,267 - INFO - [VAL] Epoch: 6/30 | Batch: 224/289 (77.9%) | Loss: 0.1289 | Batch time: 0.05s
2025-03-05 01:25:09,802 - INFO - [VAL] Epoch: 6/30 | Batch: 252/289 (87.5%) | Loss: 0.1630 | Batch time: 0.05s
2025-03-05 01:25:11,322 - INFO - [VAL] Epoch: 6/30 | Batch: 280/289 (97.2%) | Loss: 0.2984 | Batch time: 0.05s
2025-03-05 01:25:11,707 - INFO - [VAL] Epoch: 6/30 | Batch: 288/289 (100.0%) | Loss: 0.0424 | Batch time: 0.01s
2025-03-05 01:25:11,714 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:25:11,714 - INFO - Epoch 6/30 completed in 281.21s
2025-03-05 01:25:11,714 - INFO - Training   - Loss: 0.5210, Accuracy: 0.8411, F1: 0.8420
2025-03-05 01:25:11,714 - INFO - Validation - Loss: 0.2084, Accuracy: 0.9339, F1: 0.9334
2025-03-05 01:25:11,714 - INFO - --------------------------------------------------------------------------------
2025-03-05 01:25:11,714 - INFO - Epoch 7/30
2025-03-05 01:25:11,714 - INFO - ----------------------------------------
2025-03-05 01:25:12,128 - INFO - [TRAIN] Epoch: 7/30 | Batch: 0/1345 (0.1%) | Loss: 0.6127 | Batch time: 0.21s
2025-03-05 01:25:38,520 - INFO - [TRAIN] Epoch: 7/30 | Batch: 134/1345 (10.0%) | Loss: 0.5237 | Batch time: 0.19s
2025-03-05 01:26:04,734 - INFO - [TRAIN] Epoch: 7/30 | Batch: 268/1345 (20.0%) | Loss: 0.8571 | Batch time: 0.19s
2025-03-05 01:26:30,738 - INFO - [TRAIN] Epoch: 7/30 | Batch: 402/1345 (30.0%) | Loss: 0.4375 | Batch time: 0.20s
2025-03-05 01:26:57,095 - INFO - [TRAIN] Epoch: 7/30 | Batch: 536/1345 (39.9%) | Loss: 0.3736 | Batch time: 0.19s
2025-03-05 01:27:23,187 - INFO - [TRAIN] Epoch: 7/30 | Batch: 670/1345 (49.9%) | Loss: 1.0229 | Batch time: 0.19s
2025-03-05 01:27:49,388 - INFO - [TRAIN] Epoch: 7/30 | Batch: 804/1345 (59.9%) | Loss: 0.5450 | Batch time: 0.20s
2025-03-05 01:28:15,793 - INFO - [TRAIN] Epoch: 7/30 | Batch: 938/1345 (69.8%) | Loss: 0.4923 | Batch time: 0.20s
